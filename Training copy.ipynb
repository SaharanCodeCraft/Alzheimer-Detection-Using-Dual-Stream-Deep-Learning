{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd1bae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\ACSS\\AppData\\Local\\Temp\\ipykernel_19576\\2766870451.py:4: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  base_path = \"D:\\Alz\\FINAL_BALANCED_DATASET\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                path label     subject\n",
      "0  D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\\sa...    AD  002_S_0619\n",
      "1  D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\\sa...    AD  002_S_0619\n",
      "2  D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\\sa...    AD  002_S_0619\n",
      "3  D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\\sa...    AD  002_S_0619\n",
      "4  D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\\sa...    AD  002_S_0619\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_path = \"D:\\Alz\\FINAL_BALANCED_DATASET\"\n",
    "classes = ['AD', 'CN', 'LMCI']\n",
    "data = []\n",
    "\n",
    "for label in classes:\n",
    "    class_path = os.path.join(base_path, label)\n",
    "    for subject in os.listdir(class_path):\n",
    "        subject_path = os.path.join(class_path, subject, 'sagittal_slices')\n",
    "        if os.path.isdir(subject_path):\n",
    "            slices = sorted(os.listdir(subject_path))\n",
    "            for slice_file in slices:\n",
    "                full_path = os.path.join(subject_path, slice_file)\n",
    "                data.append({'path': full_path, 'label': label, 'subject': subject})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ca637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load models\n",
    "vgg = models.vgg16(pretrained=True).features.eval()\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # Remove classifier\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg.to(device)\n",
    "resnet.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd515171",
   "metadata": {},
   "source": [
    "Preprocessing Pipeline for CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1cbddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),                    # Resize to match CNN input\n",
    "    transforms.Grayscale(num_output_channels=3),      # Convert grayscale to RGB\n",
    "    transforms.ToTensor(),                            # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # ImageNet mean\n",
    "                         std=[0.229, 0.224, 0.225])    # ImageNet std\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b91f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "img_path = r\"D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\\sagittal_slices\\sagittal_000.png\"\n",
    "img = Image.open(img_path).convert('L')  # Load as grayscale\n",
    "tensor = transform(img)\n",
    "print(tensor.shape)  # Should be [3, 224, 224]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424db0de",
   "metadata": {},
   "source": [
    "Feature Extraction Function (VGG16 + ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039a8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img_path, vgg_model, resnet_model, transform, device):\n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        img = Image.open(img_path).convert('L')  # Grayscale\n",
    "        img = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            vgg_feat = vgg_model(img).view(-1)       # Flatten VGG output\n",
    "            resnet_feat = resnet_model(img).view(-1)  # Flatten ResNet output\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_feat = torch.cat((vgg_feat, resnet_feat)).cpu().numpy()\n",
    "        return combined_feat\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ffd55b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load VGG16 (feature extractor only)\n",
    "vgg16 = models.vgg16(pretrained=True).features\n",
    "vgg16.eval().to(device)\n",
    "\n",
    "# Load ResNet50 (remove final FC layer)\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "resnet50 = torch.nn.Sequential(*list(resnet50.children())[:-1])  # Remove classifier\n",
    "resnet50.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c9bc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27136,)\n"
     ]
    }
   ],
   "source": [
    "feat = extract_features(img_path, vgg16, resnet50, transform, device)\n",
    "print(feat.shape)  # Should be something like (51200,) depending on model output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4da549",
   "metadata": {},
   "source": [
    "Subject-Level Feature Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cca9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def process_subject(subject_path, vgg_model, resnet_model, transform, device):\n",
    "    # Check for nested 'sagittal_slices' folder\n",
    "    slice_dir = os.path.join(subject_path, 'sagittal_slices')\n",
    "    if os.path.exists(slice_dir):\n",
    "        slice_files = sorted(os.listdir(slice_dir))\n",
    "    else:\n",
    "        # Fallback: assume slices are directly in subject_path\n",
    "        slice_dir = subject_path\n",
    "        slice_files = sorted([f for f in os.listdir(slice_dir) if f.endswith('.png')])\n",
    "\n",
    "    if len(slice_files) == 0:\n",
    "        print(f\"No slices found in: {slice_dir}\")\n",
    "        return None\n",
    "\n",
    "    features = []\n",
    "    for slice_file in slice_files:\n",
    "        slice_path = os.path.join(slice_dir, slice_file)\n",
    "        feat = extract_features(slice_path, vgg_model, resnet_model, transform, device)\n",
    "        if feat is not None:\n",
    "            features.append(feat)\n",
    "\n",
    "    if features:\n",
    "        subject_vector = np.mean(features, axis=0)\n",
    "        return subject_vector\n",
    "    else:\n",
    "        print(f\"All slices failed for: {subject_path}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f926b4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27136,)\n"
     ]
    }
   ],
   "source": [
    "subject_path = r\"D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\"\n",
    "subject_feat = process_subject(subject_path, vgg16, resnet50, transform, device)\n",
    "print(subject_feat.shape)  # Should be (27136,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4a3f15",
   "metadata": {},
   "source": [
    "Dataset-Wide Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "432cb46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_feature_matrix(base_path, vgg_model, resnet_model, transform, device):\n",
    "    classes = ['AD', 'CN', 'LMCI']\n",
    "    data = []\n",
    "\n",
    "    for label in classes:\n",
    "        class_path = os.path.join(base_path, label)\n",
    "        subjects = sorted(os.listdir(class_path))\n",
    "\n",
    "        for subject in subjects:\n",
    "            subject_path = os.path.join(class_path, subject)\n",
    "            feat = process_subject(subject_path, vgg_model, resnet_model, transform, device)\n",
    "\n",
    "            if feat is not None:\n",
    "                data.append({'features': feat, 'label': label, 'subject_id': subject})\n",
    "            else:\n",
    "                print(f\"Skipped subject due to missing or invalid data: {subject_path}\")\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e196929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix_single_class(class_path, label, vgg_model, resnet_model, transform, device):\n",
    "    subjects = sorted(os.listdir(class_path))\n",
    "    data = []\n",
    "\n",
    "    for i, subject in enumerate(subjects):\n",
    "        subject_path = os.path.join(class_path, subject)\n",
    "        print(f\"[{label}] Processing subject {i+1}/{len(subjects)}: {subject}\")\n",
    "        feat = process_subject(subject_path, vgg_model, resnet_model, transform, device)\n",
    "\n",
    "        if feat is not None:\n",
    "            data.append({'features': feat, 'label': label, 'subject_id': subject})\n",
    "        else:\n",
    "            print(f\"Skipped subject due to missing or invalid data: {subject_path}\")\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a6800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AD] Processing subject 1/311: 002_S_0619\n",
      "[AD] Processing subject 2/311: 002_S_0619_aug1\n",
      "[AD] Processing subject 3/311: 002_S_0619_aug2\n",
      "[AD] Processing subject 4/311: 002_S_0816\n",
      "[AD] Processing subject 5/311: 002_S_0816_aug0\n",
      "[AD] Processing subject 6/311: 002_S_0816_aug1\n",
      "[AD] Processing subject 7/311: 002_S_0938\n",
      "[AD] Processing subject 8/311: 002_S_0938_aug0\n",
      "[AD] Processing subject 9/311: 002_S_0938_aug1\n",
      "[AD] Processing subject 10/311: 002_S_1018\n",
      "[AD] Processing subject 11/311: 002_S_1018_aug0\n",
      "[AD] Processing subject 12/311: 002_S_1018_aug1\n",
      "[AD] Processing subject 13/311: 005_S_0221\n",
      "[AD] Processing subject 14/311: 005_S_0221_aug0\n",
      "[AD] Processing subject 15/311: 005_S_0221_aug1\n",
      "[AD] Processing subject 16/311: 005_S_0221_aug2\n",
      "[AD] Processing subject 17/311: 005_S_0814\n",
      "[AD] Processing subject 18/311: 005_S_0814_aug0\n",
      "[AD] Processing subject 19/311: 005_S_0814_aug1\n",
      "[AD] Processing subject 20/311: 005_S_0814_aug2\n",
      "[AD] Processing subject 21/311: 005_S_1341\n",
      "[AD] Processing subject 22/311: 005_S_1341_aug0\n",
      "[AD] Processing subject 23/311: 005_S_1341_aug1\n",
      "[AD] Processing subject 24/311: 005_S_1341_aug2\n",
      "[AD] Processing subject 25/311: 006_S_0547\n",
      "[AD] Processing subject 26/311: 006_S_0547_aug0\n",
      "[AD] Processing subject 27/311: 006_S_0547_aug1\n",
      "[AD] Processing subject 28/311: 006_S_0547_aug2\n",
      "[AD] Processing subject 29/311: 007_S_0316\n",
      "[AD] Processing subject 30/311: 007_S_0316_aug0\n",
      "[AD] Processing subject 31/311: 007_S_0316_aug2\n",
      "[AD] Processing subject 32/311: 007_S_1339\n",
      "[AD] Processing subject 33/311: 007_S_1339_aug1\n",
      "[AD] Processing subject 34/311: 007_S_1339_aug2\n",
      "[AD] Processing subject 35/311: 010_S_0786\n",
      "[AD] Processing subject 36/311: 010_S_0786_aug0\n",
      "[AD] Processing subject 37/311: 010_S_0786_aug1\n",
      "[AD] Processing subject 38/311: 010_S_0786_aug2\n",
      "[AD] Processing subject 39/311: 010_S_0829\n",
      "[AD] Processing subject 40/311: 010_S_0829_aug1\n",
      "[AD] Processing subject 41/311: 011_S_0003\n",
      "[AD] Processing subject 42/311: 011_S_0003_aug1\n",
      "[AD] Processing subject 43/311: 011_S_0003_aug2\n",
      "[AD] Processing subject 44/311: 011_S_0010\n",
      "[AD] Processing subject 45/311: 011_S_0010_aug0\n",
      "[AD] Processing subject 46/311: 011_S_0010_aug1\n",
      "[AD] Processing subject 47/311: 011_S_0010_aug2\n",
      "[AD] Processing subject 48/311: 011_S_0053\n",
      "[AD] Processing subject 49/311: 011_S_0053_aug0\n",
      "[AD] Processing subject 50/311: 011_S_0053_aug1\n",
      "[AD] Processing subject 51/311: 011_S_0053_aug2\n",
      "[AD] Processing subject 52/311: 011_S_0183\n",
      "[AD] Processing subject 53/311: 011_S_0183_aug0\n",
      "[AD] Processing subject 54/311: 011_S_0183_aug1\n",
      "[AD] Processing subject 55/311: 011_S_0183_aug2\n",
      "[AD] Processing subject 56/311: 012_S_0689\n",
      "[AD] Processing subject 57/311: 012_S_0689_aug2\n",
      "[AD] Processing subject 58/311: 012_S_0712\n",
      "[AD] Processing subject 59/311: 012_S_0712_aug0\n",
      "[AD] Processing subject 60/311: 012_S_0720\n",
      "[AD] Processing subject 61/311: 012_S_0720_aug1\n",
      "[AD] Processing subject 62/311: 012_S_0803\n",
      "[AD] Processing subject 63/311: 012_S_0803_aug1\n",
      "[AD] Processing subject 64/311: 012_S_0803_aug2\n",
      "[AD] Processing subject 65/311: 013_S_1205\n",
      "[AD] Processing subject 66/311: 013_S_1205_aug1\n",
      "[AD] Processing subject 67/311: 014_S_0328\n",
      "[AD] Processing subject 68/311: 014_S_0328_aug0\n",
      "[AD] Processing subject 69/311: 014_S_0328_aug1\n",
      "[AD] Processing subject 70/311: 014_S_0328_aug2\n",
      "[AD] Processing subject 71/311: 014_S_1095\n",
      "[AD] Processing subject 72/311: 014_S_1095_aug0\n",
      "[AD] Processing subject 73/311: 014_S_1095_aug1\n",
      "[AD] Processing subject 74/311: 014_S_1095_aug2\n",
      "[AD] Processing subject 75/311: 016_S_0991\n",
      "[AD] Processing subject 76/311: 016_S_0991_aug0\n",
      "[AD] Processing subject 77/311: 016_S_0991_aug1\n",
      "[AD] Processing subject 78/311: 016_S_0991_aug2\n",
      "[AD] Processing subject 79/311: 018_S_0286\n",
      "[AD] Processing subject 80/311: 018_S_0286_aug1\n",
      "[AD] Processing subject 81/311: 018_S_0335\n",
      "[AD] Processing subject 82/311: 018_S_0335_aug0\n",
      "[AD] Processing subject 83/311: 018_S_0335_aug1\n",
      "[AD] Processing subject 84/311: 018_S_0335_aug2\n",
      "[AD] Processing subject 85/311: 018_S_0633\n",
      "[AD] Processing subject 86/311: 018_S_0633_aug0\n",
      "[AD] Processing subject 87/311: 018_S_0633_aug1\n",
      "[AD] Processing subject 88/311: 018_S_0682\n",
      "[AD] Processing subject 89/311: 018_S_0682_aug0\n",
      "[AD] Processing subject 90/311: 018_S_0682_aug1\n",
      "[AD] Processing subject 91/311: 018_S_0682_aug2\n",
      "[AD] Processing subject 92/311: 020_S_0213\n",
      "[AD] Processing subject 93/311: 020_S_0213_aug0\n",
      "[AD] Processing subject 94/311: 020_S_0213_aug2\n",
      "[AD] Processing subject 95/311: 021_S_0343\n",
      "[AD] Processing subject 96/311: 021_S_0343_aug1\n",
      "[AD] Processing subject 97/311: 021_S_0343_aug2\n",
      "[AD] Processing subject 98/311: 021_S_0642\n",
      "[AD] Processing subject 99/311: 021_S_0642_aug0\n",
      "[AD] Processing subject 100/311: 021_S_0642_aug1\n",
      "[AD] Processing subject 101/311: 021_S_0642_aug2\n",
      "[AD] Processing subject 102/311: 021_S_0753\n",
      "[AD] Processing subject 103/311: 021_S_0753_aug0\n",
      "[AD] Processing subject 104/311: 021_S_0753_aug1\n",
      "[AD] Processing subject 105/311: 021_S_0753_aug2\n",
      "[AD] Processing subject 106/311: 021_S_1109\n",
      "[AD] Processing subject 107/311: 021_S_1109_aug1\n",
      "[AD] Processing subject 108/311: 021_S_1109_aug2\n",
      "[AD] Processing subject 109/311: 022_S_0129\n",
      "[AD] Processing subject 110/311: 022_S_0129_aug1\n",
      "[AD] Processing subject 111/311: 022_S_0129_aug2\n",
      "[AD] Processing subject 112/311: 023_S_0083\n",
      "[AD] Processing subject 113/311: 023_S_0083_aug0\n",
      "[AD] Processing subject 114/311: 023_S_0083_aug2\n",
      "[AD] Processing subject 115/311: 023_S_0084\n",
      "[AD] Processing subject 116/311: 023_S_0084_aug0\n",
      "[AD] Processing subject 117/311: 023_S_0084_aug1\n",
      "[AD] Processing subject 118/311: 023_S_0139\n",
      "[AD] Processing subject 119/311: 023_S_0139_aug1\n",
      "[AD] Processing subject 120/311: 023_S_0916\n",
      "[AD] Processing subject 121/311: 023_S_0916_aug0\n",
      "[AD] Processing subject 122/311: 023_S_0916_aug1\n",
      "[AD] Processing subject 123/311: 023_S_0916_aug2\n",
      "[AD] Processing subject 124/311: 023_S_1262\n",
      "[AD] Processing subject 125/311: 024_S_1171\n",
      "[AD] Processing subject 126/311: 024_S_1171_aug0\n",
      "[AD] Processing subject 127/311: 024_S_1171_aug2\n",
      "[AD] Processing subject 128/311: 024_S_1307\n",
      "[AD] Processing subject 129/311: 027_S_0404\n",
      "[AD] Processing subject 130/311: 027_S_0404_aug1\n",
      "[AD] Processing subject 131/311: 027_S_0404_aug2\n",
      "[AD] Processing subject 132/311: 027_S_0850\n",
      "[AD] Processing subject 133/311: 027_S_0850_aug0\n",
      "[AD] Processing subject 134/311: 027_S_0850_aug1\n",
      "[AD] Processing subject 135/311: 027_S_0850_aug2\n",
      "[AD] Processing subject 136/311: 027_S_1081\n",
      "[AD] Processing subject 137/311: 027_S_1081_aug0\n",
      "[AD] Processing subject 138/311: 027_S_1082\n",
      "[AD] Processing subject 139/311: 027_S_1082_aug0\n",
      "[AD] Processing subject 140/311: 027_S_1082_aug2\n",
      "[AD] Processing subject 141/311: 027_S_1254\n",
      "[AD] Processing subject 142/311: 027_S_1254_aug0\n",
      "[AD] Processing subject 143/311: 027_S_1254_aug1\n",
      "[AD] Processing subject 144/311: 027_S_1254_aug2\n",
      "[AD] Processing subject 145/311: 027_S_1385\n",
      "[AD] Processing subject 146/311: 027_S_1385_aug0\n",
      "[AD] Processing subject 147/311: 027_S_1385_aug1\n",
      "[AD] Processing subject 148/311: 027_S_1385_aug2\n",
      "[AD] Processing subject 149/311: 029_S_0836\n",
      "[AD] Processing subject 150/311: 029_S_0836_aug0\n",
      "[AD] Processing subject 151/311: 029_S_0836_aug1\n",
      "[AD] Processing subject 152/311: 029_S_0836_aug2\n",
      "[AD] Processing subject 153/311: 029_S_0999\n",
      "[AD] Processing subject 154/311: 029_S_0999_aug0\n",
      "[AD] Processing subject 155/311: 029_S_0999_aug1\n",
      "[AD] Processing subject 156/311: 029_S_0999_aug2\n",
      "[AD] Processing subject 157/311: 029_S_1056\n",
      "[AD] Processing subject 158/311: 029_S_1056_aug0\n",
      "[AD] Processing subject 159/311: 031_S_0321\n",
      "[AD] Processing subject 160/311: 031_S_0321_aug0\n",
      "[AD] Processing subject 161/311: 031_S_0321_aug2\n",
      "[AD] Processing subject 162/311: 031_S_0554\n",
      "[AD] Processing subject 163/311: 031_S_0554_aug1\n",
      "[AD] Processing subject 164/311: 031_S_0554_aug2\n",
      "[AD] Processing subject 165/311: 031_S_1209\n",
      "[AD] Processing subject 166/311: 031_S_1209_aug0\n",
      "[AD] Processing subject 167/311: 031_S_1209_aug1\n",
      "[AD] Processing subject 168/311: 031_S_1209_aug2\n",
      "[AD] Processing subject 169/311: 032_S_0147\n",
      "[AD] Processing subject 170/311: 032_S_0147_aug0\n",
      "[AD] Processing subject 171/311: 032_S_0147_aug1\n",
      "[AD] Processing subject 172/311: 032_S_0147_aug2\n",
      "[AD] Processing subject 173/311: 032_S_0400\n",
      "[AD] Processing subject 174/311: 032_S_0400_aug0\n",
      "[AD] Processing subject 175/311: 032_S_0400_aug1\n",
      "[AD] Processing subject 176/311: 032_S_0400_aug2\n",
      "[AD] Processing subject 177/311: 032_S_1101\n",
      "[AD] Processing subject 178/311: 032_S_1101_aug0\n",
      "[AD] Processing subject 179/311: 032_S_1101_aug1\n",
      "[AD] Processing subject 180/311: 032_S_1101_aug2\n",
      "[AD] Processing subject 181/311: 033_S_0724\n",
      "[AD] Processing subject 182/311: 033_S_0724_aug0\n",
      "[AD] Processing subject 183/311: 033_S_0724_aug1\n",
      "[AD] Processing subject 184/311: 033_S_0724_aug2\n",
      "[AD] Processing subject 185/311: 033_S_0733\n",
      "[AD] Processing subject 186/311: 033_S_0733_aug0\n",
      "[AD] Processing subject 187/311: 033_S_0733_aug1\n",
      "[AD] Processing subject 188/311: 033_S_0739\n",
      "[AD] Processing subject 189/311: 033_S_0739_aug0\n",
      "[AD] Processing subject 190/311: 033_S_0739_aug1\n",
      "[AD] Processing subject 191/311: 033_S_0739_aug2\n",
      "[AD] Processing subject 192/311: 033_S_0889\n",
      "[AD] Processing subject 193/311: 033_S_0889_aug0\n",
      "[AD] Processing subject 194/311: 033_S_0889_aug2\n",
      "[AD] Processing subject 195/311: 033_S_1281\n",
      "[AD] Processing subject 196/311: 033_S_1281_aug1\n",
      "[AD] Processing subject 197/311: 033_S_1281_aug2\n",
      "[AD] Processing subject 198/311: 033_S_1283\n",
      "[AD] Processing subject 199/311: 033_S_1283_aug1\n",
      "[AD] Processing subject 200/311: 033_S_1283_aug2\n",
      "[AD] Processing subject 201/311: 033_S_1285\n",
      "[AD] Processing subject 202/311: 033_S_1285_aug0\n",
      "[AD] Processing subject 203/311: 033_S_1285_aug1\n",
      "[AD] Processing subject 204/311: 033_S_1285_aug2\n",
      "[AD] Processing subject 205/311: 033_S_1308\n",
      "[AD] Processing subject 206/311: 033_S_1308_aug1\n",
      "[AD] Processing subject 207/311: 033_S_1308_aug2\n",
      "[AD] Processing subject 208/311: 035_S_0341\n",
      "[AD] Processing subject 209/311: 035_S_0341_aug0\n",
      "[AD] Processing subject 210/311: 035_S_0341_aug1\n",
      "[AD] Processing subject 211/311: 035_S_0341_aug2\n",
      "[AD] Processing subject 212/311: 036_S_0577\n",
      "[AD] Processing subject 213/311: 036_S_0577_aug0\n",
      "[AD] Processing subject 214/311: 036_S_0577_aug2\n",
      "[AD] Processing subject 215/311: 036_S_0759\n",
      "[AD] Processing subject 216/311: 036_S_0759_aug0\n",
      "[AD] Processing subject 217/311: 036_S_0759_aug1\n",
      "[AD] Processing subject 218/311: 036_S_0760\n",
      "[AD] Processing subject 219/311: 036_S_0760_aug0\n",
      "[AD] Processing subject 220/311: 036_S_0760_aug2\n",
      "[AD] Processing subject 221/311: 036_S_1001\n",
      "[AD] Processing subject 222/311: 036_S_1001_aug2\n",
      "[AD] Processing subject 223/311: 037_S_0627\n",
      "[AD] Processing subject 224/311: 037_S_0627_aug0\n",
      "[AD] Processing subject 225/311: 037_S_0627_aug1\n",
      "[AD] Processing subject 226/311: 037_S_0627_aug2\n",
      "[AD] Processing subject 227/311: 041_S_1368\n",
      "[AD] Processing subject 228/311: 041_S_1368_aug0\n",
      "[AD] Processing subject 229/311: 041_S_1368_aug1\n",
      "[AD] Processing subject 230/311: 051_S_1296\n",
      "[AD] Processing subject 231/311: 051_S_1296_aug0\n",
      "[AD] Processing subject 232/311: 051_S_1296_aug1\n",
      "[AD] Processing subject 233/311: 051_S_1296_aug2\n",
      "[AD] Processing subject 234/311: 053_S_1044\n",
      "[AD] Processing subject 235/311: 053_S_1044_aug0\n",
      "[AD] Processing subject 236/311: 053_S_1044_aug1\n",
      "[AD] Processing subject 237/311: 053_S_1044_aug2\n",
      "[AD] Processing subject 238/311: 057_S_0474\n",
      "[AD] Processing subject 239/311: 057_S_0474_aug0\n",
      "[AD] Processing subject 240/311: 057_S_0474_aug1\n",
      "[AD] Processing subject 241/311: 057_S_1371\n",
      "[AD] Processing subject 242/311: 057_S_1371_aug0\n",
      "[AD] Processing subject 243/311: 057_S_1371_aug1\n",
      "[AD] Processing subject 244/311: 057_S_1371_aug2\n",
      "[AD] Processing subject 245/311: 057_S_1373\n",
      "[AD] Processing subject 246/311: 057_S_1373_aug0\n",
      "[AD] Processing subject 247/311: 057_S_1373_aug1\n",
      "[AD] Processing subject 248/311: 057_S_1373_aug2\n",
      "[AD] Processing subject 249/311: 057_S_1379\n",
      "[AD] Processing subject 250/311: 057_S_1379_aug0\n",
      "[AD] Processing subject 251/311: 057_S_1379_aug1\n",
      "[AD] Processing subject 252/311: 057_S_1379_aug2\n",
      "[AD] Processing subject 253/311: 062_S_0535\n",
      "[AD] Processing subject 254/311: 062_S_0535_aug0\n",
      "[AD] Processing subject 255/311: 062_S_0535_aug1\n",
      "[AD] Processing subject 256/311: 062_S_0535_aug2\n",
      "[AD] Processing subject 257/311: 062_S_0690\n",
      "[AD] Processing subject 258/311: 062_S_0730\n",
      "[AD] Processing subject 259/311: 062_S_0793\n",
      "[AD] Processing subject 260/311: 067_S_0029\n",
      "[AD] Processing subject 261/311: 067_S_0076\n",
      "[AD] Processing subject 262/311: 068_S_0109\n",
      "[AD] Processing subject 263/311: 073_S_0565\n",
      "[AD] Processing subject 264/311: 082_S_1377\n",
      "[AD] Processing subject 265/311: 094_S_1027\n",
      "[AD] Processing subject 266/311: 094_S_1090\n",
      "[AD] Processing subject 267/311: 094_S_1164\n",
      "[AD] Processing subject 268/311: 094_S_1397\n",
      "[AD] Processing subject 269/311: 094_S_1402\n",
      "[AD] Processing subject 270/311: 098_S_0149\n",
      "[AD] Processing subject 271/311: 099_S_0372\n",
      "[AD] Processing subject 272/311: 099_S_0470\n",
      "[AD] Processing subject 273/311: 099_S_1144\n",
      "[AD] Processing subject 274/311: 109_S_1157\n",
      "[AD] Processing subject 275/311: 114_S_0374\n",
      "[AD] Processing subject 276/311: 114_S_0979\n",
      "[AD] Processing subject 277/311: 116_S_0370\n",
      "[AD] Processing subject 278/311: 116_S_0392\n",
      "[AD] Processing subject 279/311: 116_S_0487\n",
      "[AD] Processing subject 280/311: 123_S_0088\n",
      "[AD] Processing subject 281/311: 123_S_0091\n",
      "[AD] Processing subject 282/311: 123_S_0094\n",
      "[AD] Processing subject 283/311: 123_S_0162\n",
      "[AD] Processing subject 284/311: 126_S_0606\n",
      "[AD] Processing subject 285/311: 126_S_0784\n",
      "[AD] Processing subject 286/311: 126_S_0891\n",
      "[AD] Processing subject 287/311: 126_S_1221\n",
      "[AD] Processing subject 288/311: 127_S_0431\n",
      "[AD] Processing subject 289/311: 127_S_0754\n",
      "[AD] Processing subject 290/311: 127_S_0844\n",
      "[AD] Processing subject 291/311: 127_S_1382\n",
      "[AD] Processing subject 292/311: 130_S_0956\n",
      "[AD] Processing subject 293/311: 130_S_1201\n",
      "[AD] Processing subject 294/311: 130_S_1290\n",
      "[AD] Processing subject 295/311: 130_S_1337\n",
      "[AD] Processing subject 296/311: 131_S_0457\n",
      "[AD] Processing subject 297/311: 131_S_0497\n",
      "[AD] Processing subject 298/311: 133_S_1170\n",
      "[AD] Processing subject 299/311: 136_S_0194\n",
      "[AD] Processing subject 300/311: 136_S_0299\n",
      "[AD] Processing subject 301/311: 136_S_0300\n",
      "[AD] Processing subject 302/311: 136_S_0426\n",
      "[AD] Processing subject 303/311: 137_S_0366\n",
      "[AD] Processing subject 304/311: 137_S_0796\n",
      "[AD] Processing subject 305/311: 137_S_1041\n",
      "[AD] Processing subject 306/311: 141_S_0696\n",
      "[AD] Processing subject 307/311: 141_S_0790\n",
      "[AD] Processing subject 308/311: 141_S_0852\n",
      "[AD] Processing subject 309/311: 141_S_0853\n",
      "[AD] Processing subject 310/311: 141_S_1137\n",
      "[AD] Processing subject 311/311: 141_S_1152\n",
      "[CN] Processing subject 1/311: 002_S_0295\n",
      "[CN] Processing subject 2/311: 002_S_0295_aug0\n",
      "[CN] Processing subject 3/311: 002_S_0295_aug1\n",
      "[CN] Processing subject 4/311: 002_S_0413\n",
      "[CN] Processing subject 5/311: 002_S_0413_aug1\n",
      "[CN] Processing subject 6/311: 002_S_0685\n",
      "[CN] Processing subject 7/311: 002_S_0685_aug0\n",
      "[CN] Processing subject 8/311: 002_S_0685_aug1\n",
      "[CN] Processing subject 9/311: 002_S_1261\n",
      "[CN] Processing subject 10/311: 002_S_1261_aug0\n",
      "[CN] Processing subject 11/311: 002_S_1261_aug1\n",
      "[CN] Processing subject 12/311: 002_S_1280\n",
      "[CN] Processing subject 13/311: 002_S_1280_aug0\n",
      "[CN] Processing subject 14/311: 002_S_1280_aug1\n",
      "[CN] Processing subject 15/311: 003_S_0907\n",
      "[CN] Processing subject 16/311: 003_S_0907_aug0\n",
      "[CN] Processing subject 17/311: 003_S_0907_aug1\n",
      "[CN] Processing subject 18/311: 003_S_0981\n",
      "[CN] Processing subject 19/311: 003_S_0981_aug1\n",
      "[CN] Processing subject 20/311: 005_S_0223\n",
      "[CN] Processing subject 21/311: 005_S_0223_aug0\n",
      "[CN] Processing subject 22/311: 005_S_0223_aug1\n",
      "[CN] Processing subject 23/311: 005_S_0553\n",
      "[CN] Processing subject 24/311: 005_S_0553_aug0\n",
      "[CN] Processing subject 25/311: 005_S_0602\n",
      "[CN] Processing subject 26/311: 005_S_0602_aug1\n",
      "[CN] Processing subject 27/311: 005_S_0610\n",
      "[CN] Processing subject 28/311: 005_S_0610_aug0\n",
      "[CN] Processing subject 29/311: 005_S_0610_aug1\n",
      "[CN] Processing subject 30/311: 006_S_0498\n",
      "[CN] Processing subject 31/311: 006_S_0498_aug0\n",
      "[CN] Processing subject 32/311: 006_S_0498_aug1\n",
      "[CN] Processing subject 33/311: 006_S_0681\n",
      "[CN] Processing subject 34/311: 006_S_0681_aug0\n",
      "[CN] Processing subject 35/311: 006_S_0681_aug1\n",
      "[CN] Processing subject 36/311: 006_S_0731\n",
      "[CN] Processing subject 37/311: 006_S_0731_aug0\n",
      "[CN] Processing subject 38/311: 006_S_0731_aug1\n",
      "[CN] Processing subject 39/311: 007_S_0068\n",
      "[CN] Processing subject 40/311: 007_S_0068_aug0\n",
      "[CN] Processing subject 41/311: 007_S_0068_aug1\n",
      "[CN] Processing subject 42/311: 007_S_0070\n",
      "[CN] Processing subject 43/311: 007_S_0070_aug0\n",
      "[CN] Processing subject 44/311: 007_S_0070_aug1\n",
      "[CN] Processing subject 45/311: 007_S_1206\n",
      "[CN] Processing subject 46/311: 007_S_1206_aug0\n",
      "[CN] Processing subject 47/311: 007_S_1206_aug1\n",
      "[CN] Processing subject 48/311: 007_S_1222\n",
      "[CN] Processing subject 49/311: 007_S_1222_aug0\n",
      "[CN] Processing subject 50/311: 007_S_1222_aug1\n",
      "[CN] Processing subject 51/311: 009_S_0751\n",
      "[CN] Processing subject 52/311: 009_S_0751_aug0\n",
      "[CN] Processing subject 53/311: 009_S_0842\n",
      "[CN] Processing subject 54/311: 009_S_0842_aug0\n",
      "[CN] Processing subject 55/311: 009_S_0862\n",
      "[CN] Processing subject 56/311: 009_S_0862_aug0\n",
      "[CN] Processing subject 57/311: 009_S_0862_aug1\n",
      "[CN] Processing subject 58/311: 010_S_0067\n",
      "[CN] Processing subject 59/311: 010_S_0067_aug0\n",
      "[CN] Processing subject 60/311: 010_S_0067_aug1\n",
      "[CN] Processing subject 61/311: 010_S_0419\n",
      "[CN] Processing subject 62/311: 010_S_0419_aug0\n",
      "[CN] Processing subject 63/311: 010_S_0419_aug1\n",
      "[CN] Processing subject 64/311: 010_S_0472\n",
      "[CN] Processing subject 65/311: 010_S_0472_aug0\n",
      "[CN] Processing subject 66/311: 010_S_0472_aug1\n",
      "[CN] Processing subject 67/311: 011_S_0005\n",
      "[CN] Processing subject 68/311: 011_S_0005_aug0\n",
      "[CN] Processing subject 69/311: 011_S_0005_aug1\n",
      "[CN] Processing subject 70/311: 011_S_0016\n",
      "[CN] Processing subject 71/311: 011_S_0016_aug0\n",
      "[CN] Processing subject 72/311: 011_S_0021\n",
      "[CN] Processing subject 73/311: 011_S_0021_aug0\n",
      "[CN] Processing subject 74/311: 011_S_0022\n",
      "[CN] Processing subject 75/311: 011_S_0022_aug0\n",
      "[CN] Processing subject 76/311: 011_S_0023\n",
      "[CN] Processing subject 77/311: 011_S_0023_aug0\n",
      "[CN] Processing subject 78/311: 011_S_0023_aug1\n",
      "[CN] Processing subject 79/311: 012_S_0637\n",
      "[CN] Processing subject 80/311: 012_S_0637_aug0\n",
      "[CN] Processing subject 81/311: 012_S_0637_aug1\n",
      "[CN] Processing subject 82/311: 012_S_1009\n",
      "[CN] Processing subject 83/311: 012_S_1009_aug0\n",
      "[CN] Processing subject 84/311: 012_S_1009_aug1\n",
      "[CN] Processing subject 85/311: 012_S_1133\n",
      "[CN] Processing subject 86/311: 012_S_1133_aug0\n",
      "[CN] Processing subject 87/311: 012_S_1133_aug1\n",
      "[CN] Processing subject 88/311: 013_S_0502\n",
      "[CN] Processing subject 89/311: 013_S_0502_aug0\n",
      "[CN] Processing subject 90/311: 013_S_0502_aug1\n",
      "[CN] Processing subject 91/311: 013_S_0575\n",
      "[CN] Processing subject 92/311: 013_S_0575_aug0\n",
      "[CN] Processing subject 93/311: 013_S_0575_aug1\n",
      "[CN] Processing subject 94/311: 013_S_1035\n",
      "[CN] Processing subject 95/311: 013_S_1035_aug0\n",
      "[CN] Processing subject 96/311: 013_S_1035_aug1\n",
      "[CN] Processing subject 97/311: 014_S_0519\n",
      "[CN] Processing subject 98/311: 014_S_0519_aug0\n",
      "[CN] Processing subject 99/311: 014_S_0519_aug1\n",
      "[CN] Processing subject 100/311: 014_S_0520\n",
      "[CN] Processing subject 101/311: 014_S_0520_aug1\n",
      "[CN] Processing subject 102/311: 014_S_0548\n",
      "[CN] Processing subject 103/311: 014_S_0548_aug0\n",
      "[CN] Processing subject 104/311: 014_S_0548_aug1\n",
      "[CN] Processing subject 105/311: 014_S_0558\n",
      "[CN] Processing subject 106/311: 014_S_0558_aug0\n",
      "[CN] Processing subject 107/311: 014_S_0558_aug1\n",
      "[CN] Processing subject 108/311: 016_S_0359\n",
      "[CN] Processing subject 109/311: 016_S_0359_aug0\n",
      "[CN] Processing subject 110/311: 016_S_0359_aug1\n",
      "[CN] Processing subject 111/311: 016_S_0538\n",
      "[CN] Processing subject 112/311: 016_S_0538_aug0\n",
      "[CN] Processing subject 113/311: 016_S_0538_aug1\n",
      "[CN] Processing subject 114/311: 018_S_0043\n",
      "[CN] Processing subject 115/311: 018_S_0043_aug0\n",
      "[CN] Processing subject 116/311: 018_S_0043_aug1\n",
      "[CN] Processing subject 117/311: 018_S_0055\n",
      "[CN] Processing subject 118/311: 018_S_0055_aug1\n",
      "[CN] Processing subject 119/311: 018_S_0369\n",
      "[CN] Processing subject 120/311: 018_S_0369_aug0\n",
      "[CN] Processing subject 121/311: 018_S_0425\n",
      "[CN] Processing subject 122/311: 018_S_0425_aug0\n",
      "[CN] Processing subject 123/311: 018_S_0425_aug1\n",
      "[CN] Processing subject 124/311: 020_S_0097\n",
      "[CN] Processing subject 125/311: 020_S_0097_aug1\n",
      "[CN] Processing subject 126/311: 020_S_0883\n",
      "[CN] Processing subject 127/311: 020_S_0883_aug0\n",
      "[CN] Processing subject 128/311: 020_S_0883_aug1\n",
      "[CN] Processing subject 129/311: 020_S_0899\n",
      "[CN] Processing subject 130/311: 020_S_0899_aug1\n",
      "[CN] Processing subject 131/311: 020_S_1288\n",
      "[CN] Processing subject 132/311: 020_S_1288_aug0\n",
      "[CN] Processing subject 133/311: 020_S_1288_aug1\n",
      "[CN] Processing subject 134/311: 021_S_0159\n",
      "[CN] Processing subject 135/311: 021_S_0159_aug0\n",
      "[CN] Processing subject 136/311: 021_S_0159_aug1\n",
      "[CN] Processing subject 137/311: 021_S_0337\n",
      "[CN] Processing subject 138/311: 021_S_0337_aug0\n",
      "[CN] Processing subject 139/311: 021_S_0647\n",
      "[CN] Processing subject 140/311: 021_S_0647_aug0\n",
      "[CN] Processing subject 141/311: 021_S_0984\n",
      "[CN] Processing subject 142/311: 021_S_0984_aug0\n",
      "[CN] Processing subject 143/311: 021_S_0984_aug1\n",
      "[CN] Processing subject 144/311: 022_S_0014\n",
      "[CN] Processing subject 145/311: 022_S_0014_aug0\n",
      "[CN] Processing subject 146/311: 022_S_0014_aug1\n",
      "[CN] Processing subject 147/311: 022_S_0066\n",
      "[CN] Processing subject 148/311: 022_S_0066_aug0\n",
      "[CN] Processing subject 149/311: 022_S_0096\n",
      "[CN] Processing subject 150/311: 022_S_0096_aug0\n",
      "[CN] Processing subject 151/311: 022_S_0096_aug1\n",
      "[CN] Processing subject 152/311: 022_S_0130\n",
      "[CN] Processing subject 153/311: 022_S_0130_aug1\n",
      "[CN] Processing subject 154/311: 023_S_0031\n",
      "[CN] Processing subject 155/311: 023_S_0031_aug0\n",
      "[CN] Processing subject 156/311: 023_S_0031_aug1\n",
      "[CN] Processing subject 157/311: 023_S_0058\n",
      "[CN] Processing subject 158/311: 023_S_0058_aug0\n",
      "[CN] Processing subject 159/311: 023_S_0058_aug1\n",
      "[CN] Processing subject 160/311: 023_S_0061\n",
      "[CN] Processing subject 161/311: 023_S_0061_aug0\n",
      "[CN] Processing subject 162/311: 023_S_0061_aug1\n",
      "[CN] Processing subject 163/311: 023_S_0081\n",
      "[CN] Processing subject 164/311: 023_S_0081_aug0\n",
      "[CN] Processing subject 165/311: 023_S_0926\n",
      "[CN] Processing subject 166/311: 023_S_0926_aug1\n",
      "[CN] Processing subject 167/311: 023_S_0963\n",
      "[CN] Processing subject 168/311: 023_S_0963_aug1\n",
      "[CN] Processing subject 169/311: 023_S_1190\n",
      "[CN] Processing subject 170/311: 023_S_1190_aug0\n",
      "[CN] Processing subject 171/311: 023_S_1190_aug1\n",
      "[CN] Processing subject 172/311: 024_S_0985\n",
      "[CN] Processing subject 173/311: 024_S_0985_aug1\n",
      "[CN] Processing subject 174/311: 024_S_1063\n",
      "[CN] Processing subject 175/311: 024_S_1063_aug0\n",
      "[CN] Processing subject 176/311: 024_S_1063_aug1\n",
      "[CN] Processing subject 177/311: 027_S_0074\n",
      "[CN] Processing subject 178/311: 027_S_0074_aug0\n",
      "[CN] Processing subject 179/311: 027_S_0074_aug1\n",
      "[CN] Processing subject 180/311: 027_S_0118\n",
      "[CN] Processing subject 181/311: 027_S_0118_aug0\n",
      "[CN] Processing subject 182/311: 027_S_0118_aug1\n",
      "[CN] Processing subject 183/311: 027_S_0120\n",
      "[CN] Processing subject 184/311: 027_S_0120_aug0\n",
      "[CN] Processing subject 185/311: 027_S_0403\n",
      "[CN] Processing subject 186/311: 027_S_0403_aug0\n",
      "[CN] Processing subject 187/311: 029_S_0824\n",
      "[CN] Processing subject 188/311: 029_S_0843\n",
      "[CN] Processing subject 189/311: 029_S_0845\n",
      "[CN] Processing subject 190/311: 029_S_0866\n",
      "[CN] Processing subject 191/311: 031_S_0618\n",
      "[CN] Processing subject 192/311: 032_S_0095\n",
      "[CN] Processing subject 193/311: 032_S_0479\n",
      "[CN] Processing subject 194/311: 032_S_0677\n",
      "[CN] Processing subject 195/311: 032_S_1169\n",
      "[CN] Processing subject 196/311: 033_S_0516\n",
      "[CN] Processing subject 197/311: 033_S_0734\n",
      "[CN] Processing subject 198/311: 033_S_0741\n",
      "[CN] Processing subject 199/311: 033_S_0920\n",
      "[CN] Processing subject 200/311: 033_S_0923\n",
      "[CN] Processing subject 201/311: 033_S_1016\n",
      "[CN] Processing subject 202/311: 033_S_1086\n",
      "[CN] Processing subject 203/311: 033_S_1098\n",
      "[CN] Processing subject 204/311: 035_S_0048\n",
      "[CN] Processing subject 205/311: 035_S_0156\n",
      "[CN] Processing subject 206/311: 035_S_0555\n",
      "[CN] Processing subject 207/311: 036_S_0576\n",
      "[CN] Processing subject 208/311: 036_S_0672\n",
      "[CN] Processing subject 209/311: 036_S_0813\n",
      "[CN] Processing subject 210/311: 036_S_1023\n",
      "[CN] Processing subject 211/311: 037_S_0303\n",
      "[CN] Processing subject 212/311: 037_S_0327\n",
      "[CN] Processing subject 213/311: 037_S_0454\n",
      "[CN] Processing subject 214/311: 037_S_0467\n",
      "[CN] Processing subject 215/311: 041_S_0125\n",
      "[CN] Processing subject 216/311: 041_S_0262\n",
      "[CN] Processing subject 217/311: 041_S_0898\n",
      "[CN] Processing subject 218/311: 041_S_1002\n",
      "[CN] Processing subject 219/311: 051_S_1123\n",
      "[CN] Processing subject 220/311: 052_S_0951\n",
      "[CN] Processing subject 221/311: 052_S_1250\n",
      "[CN] Processing subject 222/311: 052_S_1251\n",
      "[CN] Processing subject 223/311: 057_S_0643\n",
      "[CN] Processing subject 224/311: 057_S_0779\n",
      "[CN] Processing subject 225/311: 057_S_0818\n",
      "[CN] Processing subject 226/311: 057_S_0934\n",
      "[CN] Processing subject 227/311: 062_S_0578\n",
      "[CN] Processing subject 228/311: 062_S_0768\n",
      "[CN] Processing subject 229/311: 062_S_1099\n",
      "[CN] Processing subject 230/311: 067_S_0019\n",
      "[CN] Processing subject 231/311: 067_S_0056\n",
      "[CN] Processing subject 232/311: 067_S_0059\n",
      "[CN] Processing subject 233/311: 067_S_0177\n",
      "[CN] Processing subject 234/311: 067_S_0257\n",
      "[CN] Processing subject 235/311: 068_S_0127\n",
      "[CN] Processing subject 236/311: 068_S_0210\n",
      "[CN] Processing subject 237/311: 068_S_0473\n",
      "[CN] Processing subject 238/311: 072_S_0315\n",
      "[CN] Processing subject 239/311: 073_S_0089\n",
      "[CN] Processing subject 240/311: 073_S_0311\n",
      "[CN] Processing subject 241/311: 073_S_0312\n",
      "[CN] Processing subject 242/311: 073_S_0386\n",
      "[CN] Processing subject 243/311: 082_S_1256\n",
      "[CN] Processing subject 244/311: 094_S_0526\n",
      "[CN] Processing subject 245/311: 094_S_0692\n",
      "[CN] Processing subject 246/311: 094_S_0711\n",
      "[CN] Processing subject 247/311: 094_S_1241\n",
      "[CN] Processing subject 248/311: 094_S_1267\n",
      "[CN] Processing subject 249/311: 098_S_0171\n",
      "[CN] Processing subject 250/311: 098_S_0172\n",
      "[CN] Processing subject 251/311: 098_S_0896\n",
      "[CN] Processing subject 252/311: 099_S_0040\n",
      "[CN] Processing subject 253/311: 099_S_0090\n",
      "[CN] Processing subject 254/311: 099_S_0352\n",
      "[CN] Processing subject 255/311: 099_S_0533\n",
      "[CN] Processing subject 256/311: 099_S_0534\n",
      "[CN] Processing subject 257/311: 100_S_0015\n",
      "[CN] Processing subject 258/311: 100_S_0035\n",
      "[CN] Processing subject 259/311: 100_S_0047\n",
      "[CN] Processing subject 260/311: 100_S_0069\n",
      "[CN] Processing subject 261/311: 109_S_0967\n",
      "[CN] Processing subject 262/311: 109_S_1014\n",
      "[CN] Processing subject 263/311: 114_S_0166\n",
      "[CN] Processing subject 264/311: 114_S_0173\n",
      "[CN] Processing subject 265/311: 114_S_0416\n",
      "[CN] Processing subject 266/311: 114_S_0601\n",
      "[CN] Processing subject 267/311: 116_S_0382\n",
      "[CN] Processing subject 268/311: 116_S_0648\n",
      "[CN] Processing subject 269/311: 116_S_0657\n",
      "[CN] Processing subject 270/311: 116_S_1232\n",
      "[CN] Processing subject 271/311: 116_S_1249\n",
      "[CN] Processing subject 272/311: 123_S_0072\n",
      "[CN] Processing subject 273/311: 123_S_0106\n",
      "[CN] Processing subject 274/311: 123_S_0113\n",
      "[CN] Processing subject 275/311: 123_S_0298\n",
      "[CN] Processing subject 276/311: 126_S_0605\n",
      "[CN] Processing subject 277/311: 126_S_0680\n",
      "[CN] Processing subject 278/311: 127_S_0259\n",
      "[CN] Processing subject 279/311: 127_S_0260\n",
      "[CN] Processing subject 280/311: 127_S_0622\n",
      "[CN] Processing subject 281/311: 127_S_0684\n",
      "[CN] Processing subject 282/311: 128_S_0863\n",
      "[CN] Processing subject 283/311: 128_S_1242\n",
      "[CN] Processing subject 284/311: 129_S_0778\n",
      "[CN] Processing subject 285/311: 130_S_0232\n",
      "[CN] Processing subject 286/311: 130_S_0886\n",
      "[CN] Processing subject 287/311: 130_S_0969\n",
      "[CN] Processing subject 288/311: 130_S_1200\n",
      "[CN] Processing subject 289/311: 131_S_0123\n",
      "[CN] Processing subject 290/311: 131_S_0441\n",
      "[CN] Processing subject 291/311: 131_S_1301\n",
      "[CN] Processing subject 292/311: 133_S_0433\n",
      "[CN] Processing subject 293/311: 133_S_0488\n",
      "[CN] Processing subject 294/311: 133_S_0525\n",
      "[CN] Processing subject 295/311: 136_S_0086\n",
      "[CN] Processing subject 296/311: 136_S_0184\n",
      "[CN] Processing subject 297/311: 136_S_0186\n",
      "[CN] Processing subject 298/311: 136_S_0196\n",
      "[CN] Processing subject 299/311: 137_S_0283\n",
      "[CN] Processing subject 300/311: 137_S_0301\n",
      "[CN] Processing subject 301/311: 137_S_0459\n",
      "[CN] Processing subject 302/311: 137_S_0686\n",
      "[CN] Processing subject 303/311: 137_S_0972\n",
      "[CN] Processing subject 304/311: 141_S_0717\n",
      "[CN] Processing subject 305/311: 141_S_0726\n",
      "[CN] Processing subject 306/311: 141_S_0767\n",
      "[CN] Processing subject 307/311: 141_S_0810\n",
      "[CN] Processing subject 308/311: 141_S_1094\n",
      "[CN] Processing subject 309/311: 941_S_1194\n",
      "[CN] Processing subject 310/311: 941_S_1197\n",
      "[CN] Processing subject 311/311: 941_S_1202\n",
      "[LMCI] Processing subject 1/311: 002_S_0729\n",
      "[LMCI] Processing subject 2/311: 002_S_0782\n",
      "[LMCI] Processing subject 3/311: 002_S_0954\n",
      "[LMCI] Processing subject 4/311: 002_S_1070\n",
      "[LMCI] Processing subject 5/311: 002_S_1155\n",
      "[LMCI] Processing subject 6/311: 002_S_1268\n",
      "[LMCI] Processing subject 7/311: 003_S_0908\n",
      "[LMCI] Processing subject 8/311: 003_S_1057\n",
      "[LMCI] Processing subject 9/311: 003_S_1122\n",
      "[LMCI] Processing subject 10/311: 005_S_0222\n",
      "[LMCI] Processing subject 11/311: 005_S_0324\n",
      "[LMCI] Processing subject 12/311: 005_S_0448\n",
      "[LMCI] Processing subject 13/311: 005_S_0546\n",
      "[LMCI] Processing subject 14/311: 005_S_1224\n",
      "[LMCI] Processing subject 15/311: 006_S_0675\n",
      "[LMCI] Processing subject 16/311: 006_S_1130\n",
      "[LMCI] Processing subject 17/311: 007_S_0041\n",
      "[LMCI] Processing subject 18/311: 007_S_0101\n",
      "[LMCI] Processing subject 19/311: 007_S_0128\n",
      "[LMCI] Processing subject 20/311: 007_S_0249\n",
      "[LMCI] Processing subject 21/311: 007_S_0293\n",
      "[LMCI] Processing subject 22/311: 007_S_0414\n",
      "[LMCI] Processing subject 23/311: 007_S_0698\n",
      "[LMCI] Processing subject 24/311: 009_S_1030\n",
      "[LMCI] Processing subject 25/311: 010_S_0422\n",
      "[LMCI] Processing subject 26/311: 010_S_0904\n",
      "[LMCI] Processing subject 27/311: 011_S_0168\n",
      "[LMCI] Processing subject 28/311: 011_S_0241\n",
      "[LMCI] Processing subject 29/311: 011_S_0326\n",
      "[LMCI] Processing subject 30/311: 011_S_0362\n",
      "[LMCI] Processing subject 31/311: 011_S_0856\n",
      "[LMCI] Processing subject 32/311: 011_S_0861\n",
      "[LMCI] Processing subject 33/311: 011_S_1080\n",
      "[LMCI] Processing subject 34/311: 011_S_1282\n",
      "[LMCI] Processing subject 35/311: 012_S_0634\n",
      "[LMCI] Processing subject 36/311: 012_S_0932\n",
      "[LMCI] Processing subject 37/311: 012_S_1033\n",
      "[LMCI] Processing subject 38/311: 012_S_1165\n",
      "[LMCI] Processing subject 39/311: 012_S_1292\n",
      "[LMCI] Processing subject 40/311: 012_S_1321\n",
      "[LMCI] Processing subject 41/311: 013_S_0240\n",
      "[LMCI] Processing subject 42/311: 013_S_0325\n",
      "[LMCI] Processing subject 43/311: 013_S_0860\n",
      "[LMCI] Processing subject 44/311: 013_S_1120\n",
      "[LMCI] Processing subject 45/311: 013_S_1186\n",
      "[LMCI] Processing subject 46/311: 013_S_1275\n",
      "[LMCI] Processing subject 47/311: 014_S_0169\n",
      "[LMCI] Processing subject 48/311: 014_S_0557\n",
      "[LMCI] Processing subject 49/311: 014_S_0563\n",
      "[LMCI] Processing subject 50/311: 014_S_0658\n",
      "[LMCI] Processing subject 51/311: 016_S_0354\n",
      "[LMCI] Processing subject 52/311: 016_S_0702\n",
      "[LMCI] Processing subject 53/311: 016_S_1028\n",
      "[LMCI] Processing subject 54/311: 016_S_1117\n",
      "[LMCI] Processing subject 55/311: 016_S_1121\n",
      "[LMCI] Processing subject 56/311: 016_S_1326\n",
      "[LMCI] Processing subject 57/311: 018_S_0057\n",
      "[LMCI] Processing subject 58/311: 018_S_0080\n",
      "[LMCI] Processing subject 59/311: 018_S_0087\n",
      "[LMCI] Processing subject 60/311: 018_S_0142\n",
      "[LMCI] Processing subject 61/311: 018_S_0155\n",
      "[LMCI] Processing subject 62/311: 018_S_0406\n",
      "[LMCI] Processing subject 63/311: 018_S_0450\n",
      "[LMCI] Processing subject 64/311: 021_S_0141\n",
      "[LMCI] Processing subject 65/311: 021_S_0231\n",
      "[LMCI] Processing subject 66/311: 021_S_0273\n",
      "[LMCI] Processing subject 67/311: 021_S_0276\n",
      "[LMCI] Processing subject 68/311: 021_S_0332\n",
      "[LMCI] Processing subject 69/311: 021_S_0424\n",
      "[LMCI] Processing subject 70/311: 021_S_0626\n",
      "[LMCI] Processing subject 71/311: 022_S_0004\n",
      "[LMCI] Processing subject 72/311: 022_S_0544\n",
      "[LMCI] Processing subject 73/311: 022_S_0750\n",
      "[LMCI] Processing subject 74/311: 022_S_0961\n",
      "[LMCI] Processing subject 75/311: 022_S_1351\n",
      "[LMCI] Processing subject 76/311: 022_S_1394\n",
      "[LMCI] Processing subject 77/311: 023_S_0030\n",
      "[LMCI] Processing subject 78/311: 023_S_0042\n",
      "[LMCI] Processing subject 79/311: 023_S_0078\n",
      "[LMCI] Processing subject 80/311: 023_S_0126\n",
      "[LMCI] Processing subject 81/311: 023_S_0217\n",
      "[LMCI] Processing subject 82/311: 023_S_0331\n",
      "[LMCI] Processing subject 83/311: 023_S_0376\n",
      "[LMCI] Processing subject 84/311: 023_S_0388\n",
      "[LMCI] Processing subject 85/311: 023_S_0604\n",
      "[LMCI] Processing subject 86/311: 023_S_0625\n",
      "[LMCI] Processing subject 87/311: 023_S_0855\n",
      "[LMCI] Processing subject 88/311: 023_S_0887\n",
      "[LMCI] Processing subject 89/311: 023_S_1046\n",
      "[LMCI] Processing subject 90/311: 023_S_1126\n",
      "[LMCI] Processing subject 91/311: 023_S_1247\n",
      "[LMCI] Processing subject 92/311: 024_S_1393\n",
      "[LMCI] Processing subject 93/311: 027_S_0116\n",
      "[LMCI] Processing subject 94/311: 027_S_0179\n",
      "[LMCI] Processing subject 95/311: 027_S_0256\n",
      "[LMCI] Processing subject 96/311: 027_S_0307\n",
      "[LMCI] Processing subject 97/311: 027_S_0408\n",
      "[LMCI] Processing subject 98/311: 027_S_0417\n",
      "[LMCI] Processing subject 99/311: 027_S_0461\n",
      "[LMCI] Processing subject 100/311: 027_S_0485\n",
      "[LMCI] Processing subject 101/311: 027_S_0644\n",
      "[LMCI] Processing subject 102/311: 027_S_0835\n",
      "[LMCI] Processing subject 103/311: 027_S_1045\n",
      "[LMCI] Processing subject 104/311: 027_S_1213\n",
      "[LMCI] Processing subject 105/311: 027_S_1277\n",
      "[LMCI] Processing subject 106/311: 027_S_1387\n",
      "[LMCI] Processing subject 107/311: 029_S_0878\n",
      "[LMCI] Processing subject 108/311: 029_S_0914\n",
      "[LMCI] Processing subject 109/311: 029_S_1073\n",
      "[LMCI] Processing subject 110/311: 029_S_1215\n",
      "[LMCI] Processing subject 111/311: 029_S_1318\n",
      "[LMCI] Processing subject 112/311: 029_S_1384\n",
      "[LMCI] Processing subject 113/311: 031_S_0294\n",
      "[LMCI] Processing subject 114/311: 031_S_0351\n",
      "[LMCI] Processing subject 115/311: 031_S_0568\n",
      "[LMCI] Processing subject 116/311: 031_S_0830\n",
      "[LMCI] Processing subject 117/311: 031_S_0867\n",
      "[LMCI] Processing subject 118/311: 031_S_1066\n",
      "[LMCI] Processing subject 119/311: 032_S_0187\n",
      "[LMCI] Processing subject 120/311: 032_S_0214\n",
      "[LMCI] Processing subject 121/311: 032_S_0718\n",
      "[LMCI] Processing subject 122/311: 032_S_0978\n",
      "[LMCI] Processing subject 123/311: 033_S_0511\n",
      "[LMCI] Processing subject 124/311: 033_S_0513\n",
      "[LMCI] Processing subject 125/311: 033_S_0514\n",
      "[LMCI] Processing subject 126/311: 033_S_0567\n",
      "[LMCI] Processing subject 127/311: 033_S_0723\n",
      "[LMCI] Processing subject 128/311: 033_S_0725\n",
      "[LMCI] Processing subject 129/311: 033_S_0906\n",
      "[LMCI] Processing subject 130/311: 033_S_0922\n",
      "[LMCI] Processing subject 131/311: 033_S_1116\n",
      "[LMCI] Processing subject 132/311: 033_S_1279\n",
      "[LMCI] Processing subject 133/311: 033_S_1284\n",
      "[LMCI] Processing subject 134/311: 033_S_1309\n",
      "[LMCI] Processing subject 135/311: 035_S_0033\n",
      "[LMCI] Processing subject 136/311: 035_S_0204\n",
      "[LMCI] Processing subject 137/311: 035_S_0292\n",
      "[LMCI] Processing subject 138/311: 035_S_0997\n",
      "[LMCI] Processing subject 139/311: 036_S_0656\n",
      "[LMCI] Processing subject 140/311: 036_S_0673\n",
      "[LMCI] Processing subject 141/311: 036_S_0748\n",
      "[LMCI] Processing subject 142/311: 036_S_0869\n",
      "[LMCI] Processing subject 143/311: 036_S_0945\n",
      "[LMCI] Processing subject 144/311: 036_S_0976\n",
      "[LMCI] Processing subject 145/311: 036_S_1135\n",
      "[LMCI] Processing subject 146/311: 036_S_1240\n",
      "[LMCI] Processing subject 147/311: 037_S_0150\n",
      "[LMCI] Processing subject 148/311: 037_S_0501\n",
      "[LMCI] Processing subject 149/311: 037_S_0539\n",
      "[LMCI] Processing subject 150/311: 037_S_0552\n",
      "[LMCI] Processing subject 151/311: 037_S_0566\n",
      "[LMCI] Processing subject 152/311: 037_S_0588\n",
      "[LMCI] Processing subject 153/311: 037_S_1078\n",
      "[LMCI] Processing subject 154/311: 037_S_1225\n",
      "[LMCI] Processing subject 155/311: 037_S_1421\n",
      "[LMCI] Processing subject 156/311: 041_S_0282\n",
      "[LMCI] Processing subject 157/311: 041_S_0314\n",
      "[LMCI] Processing subject 158/311: 041_S_0446\n",
      "[LMCI] Processing subject 159/311: 041_S_0549\n",
      "[LMCI] Processing subject 160/311: 041_S_0598\n",
      "[LMCI] Processing subject 161/311: 041_S_0679\n",
      "[LMCI] Processing subject 162/311: 041_S_1010\n",
      "[LMCI] Processing subject 163/311: 041_S_1260\n",
      "[LMCI] Processing subject 164/311: 041_S_1412\n",
      "[LMCI] Processing subject 165/311: 041_S_1418\n",
      "[LMCI] Processing subject 166/311: 041_S_1423\n",
      "[LMCI] Processing subject 167/311: 041_S_1425\n",
      "[LMCI] Processing subject 168/311: 051_S_1040\n",
      "[LMCI] Processing subject 169/311: 051_S_1072\n",
      "[LMCI] Processing subject 170/311: 051_S_1131\n",
      "[LMCI] Processing subject 171/311: 051_S_1331\n",
      "[LMCI] Processing subject 172/311: 052_S_0671\n",
      "[LMCI] Processing subject 173/311: 052_S_0952\n",
      "[LMCI] Processing subject 174/311: 052_S_1054\n",
      "[LMCI] Processing subject 175/311: 052_S_1168\n",
      "[LMCI] Processing subject 176/311: 052_S_1346\n",
      "[LMCI] Processing subject 177/311: 052_S_1352\n",
      "[LMCI] Processing subject 178/311: 053_S_0389\n",
      "[LMCI] Processing subject 179/311: 053_S_0507\n",
      "[LMCI] Processing subject 180/311: 053_S_0621\n",
      "[LMCI] Processing subject 181/311: 053_S_0919\n",
      "[LMCI] Processing subject 182/311: 057_S_0464\n",
      "[LMCI] Processing subject 183/311: 057_S_0839\n",
      "[LMCI] Processing subject 184/311: 057_S_0941\n",
      "[LMCI] Processing subject 185/311: 057_S_1007\n",
      "[LMCI] Processing subject 186/311: 057_S_1217\n",
      "[LMCI] Processing subject 187/311: 057_S_1265\n",
      "[LMCI] Processing subject 188/311: 057_S_1269\n",
      "[LMCI] Processing subject 189/311: 062_S_1182\n",
      "[LMCI] Processing subject 190/311: 062_S_1299\n",
      "[LMCI] Processing subject 191/311: 067_S_0038\n",
      "[LMCI] Processing subject 192/311: 067_S_0077\n",
      "[LMCI] Processing subject 193/311: 067_S_0098\n",
      "[LMCI] Processing subject 194/311: 067_S_0176\n",
      "[LMCI] Processing subject 195/311: 067_S_0284\n",
      "[LMCI] Processing subject 196/311: 067_S_0290\n",
      "[LMCI] Processing subject 197/311: 067_S_0336\n",
      "[LMCI] Processing subject 198/311: 067_S_0607\n",
      "[LMCI] Processing subject 199/311: 068_S_0442\n",
      "[LMCI] Processing subject 200/311: 068_S_0872\n",
      "[LMCI] Processing subject 201/311: 073_S_0518\n",
      "[LMCI] Processing subject 202/311: 073_S_0746\n",
      "[LMCI] Processing subject 203/311: 073_S_0909\n",
      "[LMCI] Processing subject 204/311: 082_S_0928\n",
      "[LMCI] Processing subject 205/311: 082_S_1119\n",
      "[LMCI] Processing subject 206/311: 094_S_0434\n",
      "[LMCI] Processing subject 207/311: 094_S_0531\n",
      "[LMCI] Processing subject 208/311: 094_S_0921\n",
      "[LMCI] Processing subject 209/311: 094_S_1188\n",
      "[LMCI] Processing subject 210/311: 094_S_1293\n",
      "[LMCI] Processing subject 211/311: 094_S_1398\n",
      "[LMCI] Processing subject 212/311: 094_S_1417\n",
      "[LMCI] Processing subject 213/311: 098_S_0160\n",
      "[LMCI] Processing subject 214/311: 098_S_0269\n",
      "[LMCI] Processing subject 215/311: 098_S_0667\n",
      "[LMCI] Processing subject 216/311: 099_S_0051\n",
      "[LMCI] Processing subject 217/311: 099_S_0054\n",
      "[LMCI] Processing subject 218/311: 099_S_0060\n",
      "[LMCI] Processing subject 219/311: 099_S_0111\n",
      "[LMCI] Processing subject 220/311: 099_S_0291\n",
      "[LMCI] Processing subject 221/311: 099_S_0551\n",
      "[LMCI] Processing subject 222/311: 099_S_0880\n",
      "[LMCI] Processing subject 223/311: 099_S_1034\n",
      "[LMCI] Processing subject 224/311: 100_S_0006\n",
      "[LMCI] Processing subject 225/311: 100_S_0190\n",
      "[LMCI] Processing subject 226/311: 100_S_0296\n",
      "[LMCI] Processing subject 227/311: 100_S_0995\n",
      "[LMCI] Processing subject 228/311: 109_S_0950\n",
      "[LMCI] Processing subject 229/311: 109_S_1114\n",
      "[LMCI] Processing subject 230/311: 109_S_1183\n",
      "[LMCI] Processing subject 231/311: 109_S_1343\n",
      "[LMCI] Processing subject 232/311: 114_S_0378\n",
      "[LMCI] Processing subject 233/311: 114_S_0410\n",
      "[LMCI] Processing subject 234/311: 114_S_0458\n",
      "[LMCI] Processing subject 235/311: 114_S_1103\n",
      "[LMCI] Processing subject 236/311: 114_S_1106\n",
      "[LMCI] Processing subject 237/311: 114_S_1118\n",
      "[LMCI] Processing subject 238/311: 116_S_0361\n",
      "[LMCI] Processing subject 239/311: 116_S_0649\n",
      "[LMCI] Processing subject 240/311: 116_S_0752\n",
      "[LMCI] Processing subject 241/311: 116_S_0834\n",
      "[LMCI] Processing subject 242/311: 116_S_1243\n",
      "[LMCI] Processing subject 243/311: 116_S_1271\n",
      "[LMCI] Processing subject 244/311: 116_S_1315\n",
      "[LMCI] Processing subject 245/311: 121_S_1322\n",
      "[LMCI] Processing subject 246/311: 123_S_0050\n",
      "[LMCI] Processing subject 247/311: 123_S_0108\n",
      "[LMCI] Processing subject 248/311: 123_S_0390\n",
      "[LMCI] Processing subject 249/311: 123_S_1300\n",
      "[LMCI] Processing subject 250/311: 126_S_0708\n",
      "[LMCI] Processing subject 251/311: 126_S_0709\n",
      "[LMCI] Processing subject 252/311: 126_S_0865\n",
      "[LMCI] Processing subject 253/311: 126_S_1187\n",
      "[LMCI] Processing subject 254/311: 127_S_0112\n",
      "[LMCI] Processing subject 255/311: 127_S_0393\n",
      "[LMCI] Processing subject 256/311: 127_S_0394\n",
      "[LMCI] Processing subject 257/311: 127_S_0925\n",
      "[LMCI] Processing subject 258/311: 127_S_1032\n",
      "[LMCI] Processing subject 259/311: 127_S_1140\n",
      "[LMCI] Processing subject 260/311: 127_S_1419\n",
      "[LMCI] Processing subject 261/311: 127_S_1427\n"
     ]
    }
   ],
   "source": [
    "feature_df_ad   = build_feature_matrix_single_class(r\"D:\\Alz\\FINAL_BALANCED_DATASET\\AD\",   \"AD\",   vgg16, resnet50, transform, device)\n",
    "feature_df_cn   = build_feature_matrix_single_class(r\"D:\\Alz\\FINAL_BALANCED_DATASET\\CN\",   \"CN\",   vgg16, resnet50, transform, device)\n",
    "feature_df_lmci = build_feature_matrix_single_class(r\"D:\\Alz\\FINAL_BALANCED_DATASET\\LMCI\", \"LMCI\", vgg16, resnet50, transform, device)\n",
    "\n",
    "feature_df = pd.concat([feature_df_ad, feature_df_cn, feature_df_lmci], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df_ad.to_pickle(\"features_ad.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa6b38",
   "metadata": {},
   "source": [
    "Loading Pretrained Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71135fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to C:\\Users\\ACSS/.cache\\torch\\hub\\checkpoints\\vit_b_16-c867db91.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 330M/330M [03:22<00:00, 1.71MB/s] \n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "# Load pretrained ViT model\n",
    "vit_weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "vit_model = vit_b_16(weights=vit_weights)\n",
    "vit_model.eval().to(device)\n",
    "\n",
    "# Remove classification head to get feature embeddings\n",
    "vit_model = torch.nn.Sequential(*list(vit_model.children())[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (1): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vit_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca819a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Defining preprocessing pipeline for VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda42226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import ViT_B_16_Weights\n",
    "\n",
    "# Load ViT preprocessing transforms\n",
    "vit_weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "vit_transform = vit_weights.transforms()\n",
    "\n",
    "# Optional: inspect the transform pipeline\n",
    "print(vit_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c05017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_path = r\"D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\\sagittal_slices\\sagittal_000.png\"\n",
    "img = Image.open(img_path).convert('RGB')  # ViT expects RGB\n",
    "tensor = vit_transform(img).unsqueeze(0).to(device)\n",
    "print(tensor.shape)  # Should be [1, 3, 224, 224]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0edc9c",
   "metadata": {},
   "source": [
    "Slice-Level Feature Extraction Using ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15873225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vit_features(img_path, vit_model, vit_transform, device):\n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        img = Image.open(img_path).convert('RGB')  # ViT expects RGB\n",
    "        img = vit_transform(img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            vit_feat = vit_model(img).view(-1).cpu().numpy()\n",
    "\n",
    "        return vit_feat\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ViT error on {img_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df9a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT error on D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\\sagittal_slices\\sagittal_000.png: Expected (batch_size, seq_length, hidden_dim) got torch.Size([1, 768, 14, 14])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m vit_feat = extract_vit_features(img_path, vit_model, vit_transform, device)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvit_feat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m)  \u001b[38;5;66;03m# Should be (768,) for ViT-B/16\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "vit_feat = extract_vit_features(img_path, vit_model, vit_transform, device)\n",
    "print(vit_feat.shape)  # Should be (768,) for ViT-B/16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70a230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(img_path))  # Should return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vit_features(img_path, vit_model, vit_transform, device):\n",
    "    try:\n",
    "        print(f\"Loading image: {img_path}\")\n",
    "        img = Image.open(img_path).convert('RGB')  # ViT expects RGB\n",
    "        img = vit_transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vit_feat = vit_model(img).view(-1).cpu().numpy()\n",
    "\n",
    "        return vit_feat\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ViT error on {img_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8344cca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image: D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\\sagittal_slices\\sagittal_000.png\n",
      "ViT error on D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\\sagittal_slices\\sagittal_000.png: Expected (batch_size, seq_length, hidden_dim) got torch.Size([1, 768, 14, 14])\n",
      "Feature extraction failed or returned None.\n"
     ]
    }
   ],
   "source": [
    "vit_feat = extract_vit_features(img_path, vit_model, vit_transform, device)\n",
    "\n",
    "if vit_feat is not None:\n",
    "    print(f\"Feature shape: {vit_feat.shape}\")\n",
    "else:\n",
    "    print(\"Feature extraction failed or returned None.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee56f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "# Load full model with weights\n",
    "vit_weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "vit_model = vit_b_16(weights=vit_weights)\n",
    "vit_model.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vit_features(img_path, vit_model, vit_transform, device):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = vit_transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vit_feat = vit_model.forward_features(img).view(-1).cpu().numpy()\n",
    "            return vit_feat\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ViT error on {img_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f466f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT error on D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\\sagittal_slices\\sagittal_000.png: 'VisionTransformer' object has no attribute 'forward_features'\n",
      "ViT feature extraction failed.\n"
     ]
    }
   ],
   "source": [
    "vit_feat = extract_vit_features(img_path, vit_model, vit_transform, device)\n",
    "if vit_feat is not None:\n",
    "    print(f\"ViT feature shape: {vit_feat.shape}\")  # Should be (768,)\n",
    "else:\n",
    "    print(\"ViT feature extraction failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vit_features(img_path, vit_model, vit_transform, device):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = vit_transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get patch embeddings\n",
    "            x = vit_model._process_input(img)  # [B, num_patches, hidden_dim]\n",
    "            batch_size = x.shape[0]\n",
    "\n",
    "            # Add CLS token\n",
    "            cls_token = vit_model.cls_token.expand(batch_size, -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "            # Add positional embeddings\n",
    "            x = x + vit_model.encoder.pos_embedding\n",
    "            x = vit_model.encoder.dropout(x)\n",
    "\n",
    "            # Pass through transformer encoder\n",
    "            x = vit_model.encoder.ln(vit_model.encoder.layers(x))\n",
    "\n",
    "            # Extract CLS token embedding\n",
    "            vit_feat = x[:, 0].view(-1).cpu().numpy()\n",
    "            return vit_feat\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ViT error on {img_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faadafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT error on D:\\Alz\\FINAL_BALANCED_DATASET\\AD\\002_S_0619\\sagittal_slices\\sagittal_000.png: 'VisionTransformer' object has no attribute 'cls_token'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m vit_feat = extract_vit_features(img_path, vit_model, vit_transform, device)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvit_feat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m)  \u001b[38;5;66;03m# Should be (768,)\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "vit_feat = extract_vit_features(img_path, vit_model, vit_transform, device)\n",
    "print(vit_feat.shape)  # Should be (768,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423f76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.19-py3-none-any.whl.metadata (60 kB)\n",
      "Requirement already satisfied: torch in c:\\program files\\python313\\lib\\site-packages (from timm) (2.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\program files\\python313\\lib\\site-packages (from timm) (0.23.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from timm) (0.34.4)\n",
      "Requirement already satisfied: safetensors in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python313\\lib\\site-packages (from huggingface_hub->timm) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python313\\lib\\site-packages (from huggingface_hub->timm) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface_hub->timm) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\program files\\python313\\lib\\site-packages (from torch->timm) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python313\\lib\\site-packages (from torch->timm) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python313\\lib\\site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python313\\lib\\site-packages (from torch->timm) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\program files\\python313\\lib\\site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python313\\lib\\site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from torchvision->timm) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\program files\\python313\\lib\\site-packages (from torchvision->timm) (11.3.0)\n",
      "Downloading timm-1.0.19-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.3/2.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.5 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 1.7 MB/s  0:00:01\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-1.0.19\n"
     ]
    }
   ],
   "source": [
    "!pip install timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACSS\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\ACSS\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ACSS\\.cache\\huggingface\\hub\\models--timm--vit_base_patch16_224.augreg2_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "# Load pretrained ViT base model\n",
    "vit_model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "vit_model.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vit_features(img_path, vit_model, vit_transform, device):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = vit_transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vit_feat = vit_model.forward_features(img).view(-1).cpu().numpy()\n",
    "            return vit_feat\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ViT error on {img_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadef61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT feature shape: (151296,)\n"
     ]
    }
   ],
   "source": [
    "vit_feat = extract_vit_features(img_path, vit_model, vit_transform, device)\n",
    "if vit_feat is not None:\n",
    "    print(f\"ViT feature shape: {vit_feat.shape}\")  # Should be (768,)\n",
    "else:\n",
    "    print(\"ViT feature extraction failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b2a74",
   "metadata": {},
   "source": [
    "Aggregating ViT Features Across Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject_vit(subject_path, vit_model, vit_transform, device):\n",
    "    # Check for nested 'sagittal_slices' folder\n",
    "    slice_dir = os.path.join(subject_path, 'sagittal_slices')\n",
    "    if os.path.exists(slice_dir):\n",
    "        slice_files = sorted(os.listdir(slice_dir))\n",
    "    else:\n",
    "        slice_dir = subject_path\n",
    "        slice_files = sorted([f for f in os.listdir(slice_dir) if f.endswith('.png')])\n",
    "\n",
    "    if len(slice_files) == 0:\n",
    "        print(f\"No slices found in: {slice_dir}\")\n",
    "        return None\n",
    "\n",
    "    features = []\n",
    "    for slice_file in slice_files:\n",
    "        slice_path = os.path.join(slice_dir, slice_file)\n",
    "        feat = extract_vit_features(slice_path, vit_model, vit_transform, device)\n",
    "        if feat is not None:\n",
    "            features.append(feat)\n",
    "\n",
    "    if features:\n",
    "        subject_vector = np.mean(features, axis=0)  # You can swap with max or median pooling\n",
    "        return subject_vector\n",
    "    else:\n",
    "        print(f\"All ViT slices failed for: {subject_path}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e9315",
   "metadata": {},
   "source": [
    "Building Full ViT Feature Matrix Across All Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_vit_feature_matrix(base_path, vit_model, vit_transform, device):\n",
    "    classes = ['AD', 'CN', 'LMCI']\n",
    "    data = []\n",
    "\n",
    "    for label in classes:\n",
    "        class_path = os.path.join(base_path, label)\n",
    "        subjects = sorted(os.listdir(class_path))\n",
    "\n",
    "        for i, subject in enumerate(subjects):\n",
    "            subject_path = os.path.join(class_path, subject)\n",
    "            print(f\"[ViT-{label}] Processing subject {i+1}/{len(subjects)}: {subject}\")\n",
    "            feat = process_subject_vit(subject_path, vit_model, vit_transform, device)\n",
    "\n",
    "            if feat is not None:\n",
    "                data.append({'features': feat, 'label': label, 'subject_id': subject})\n",
    "            else:\n",
    "                print(f\"Skipped subject due to missing or invalid ViT data: {subject_path}\")\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf2d5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ViT-AD] Processing subject 1/311: 002_S_0619\n",
      "[ViT-AD] Processing subject 2/311: 002_S_0619_aug1\n",
      "[ViT-AD] Processing subject 3/311: 002_S_0619_aug2\n",
      "[ViT-AD] Processing subject 4/311: 002_S_0816\n",
      "[ViT-AD] Processing subject 5/311: 002_S_0816_aug0\n",
      "[ViT-AD] Processing subject 6/311: 002_S_0816_aug1\n",
      "[ViT-AD] Processing subject 7/311: 002_S_0938\n",
      "[ViT-AD] Processing subject 8/311: 002_S_0938_aug0\n",
      "[ViT-AD] Processing subject 9/311: 002_S_0938_aug1\n",
      "[ViT-AD] Processing subject 10/311: 002_S_1018\n",
      "[ViT-AD] Processing subject 11/311: 002_S_1018_aug0\n",
      "[ViT-AD] Processing subject 12/311: 002_S_1018_aug1\n",
      "[ViT-AD] Processing subject 13/311: 005_S_0221\n",
      "[ViT-AD] Processing subject 14/311: 005_S_0221_aug0\n",
      "[ViT-AD] Processing subject 15/311: 005_S_0221_aug1\n",
      "[ViT-AD] Processing subject 16/311: 005_S_0221_aug2\n",
      "[ViT-AD] Processing subject 17/311: 005_S_0814\n",
      "[ViT-AD] Processing subject 18/311: 005_S_0814_aug0\n",
      "[ViT-AD] Processing subject 19/311: 005_S_0814_aug1\n",
      "[ViT-AD] Processing subject 20/311: 005_S_0814_aug2\n",
      "[ViT-AD] Processing subject 21/311: 005_S_1341\n",
      "[ViT-AD] Processing subject 22/311: 005_S_1341_aug0\n",
      "[ViT-AD] Processing subject 23/311: 005_S_1341_aug1\n",
      "[ViT-AD] Processing subject 24/311: 005_S_1341_aug2\n",
      "[ViT-AD] Processing subject 25/311: 006_S_0547\n",
      "[ViT-AD] Processing subject 26/311: 006_S_0547_aug0\n",
      "[ViT-AD] Processing subject 27/311: 006_S_0547_aug1\n",
      "[ViT-AD] Processing subject 28/311: 006_S_0547_aug2\n",
      "[ViT-AD] Processing subject 29/311: 007_S_0316\n",
      "[ViT-AD] Processing subject 30/311: 007_S_0316_aug0\n",
      "[ViT-AD] Processing subject 31/311: 007_S_0316_aug2\n",
      "[ViT-AD] Processing subject 32/311: 007_S_1339\n",
      "[ViT-AD] Processing subject 33/311: 007_S_1339_aug1\n",
      "[ViT-AD] Processing subject 34/311: 007_S_1339_aug2\n",
      "[ViT-AD] Processing subject 35/311: 010_S_0786\n",
      "[ViT-AD] Processing subject 36/311: 010_S_0786_aug0\n",
      "[ViT-AD] Processing subject 37/311: 010_S_0786_aug1\n",
      "[ViT-AD] Processing subject 38/311: 010_S_0786_aug2\n",
      "[ViT-AD] Processing subject 39/311: 010_S_0829\n",
      "[ViT-AD] Processing subject 40/311: 010_S_0829_aug1\n",
      "[ViT-AD] Processing subject 41/311: 011_S_0003\n",
      "[ViT-AD] Processing subject 42/311: 011_S_0003_aug1\n",
      "[ViT-AD] Processing subject 43/311: 011_S_0003_aug2\n",
      "[ViT-AD] Processing subject 44/311: 011_S_0010\n",
      "[ViT-AD] Processing subject 45/311: 011_S_0010_aug0\n",
      "[ViT-AD] Processing subject 46/311: 011_S_0010_aug1\n",
      "[ViT-AD] Processing subject 47/311: 011_S_0010_aug2\n",
      "[ViT-AD] Processing subject 48/311: 011_S_0053\n",
      "[ViT-AD] Processing subject 49/311: 011_S_0053_aug0\n",
      "[ViT-AD] Processing subject 50/311: 011_S_0053_aug1\n",
      "[ViT-AD] Processing subject 51/311: 011_S_0053_aug2\n",
      "[ViT-AD] Processing subject 52/311: 011_S_0183\n",
      "[ViT-AD] Processing subject 53/311: 011_S_0183_aug0\n",
      "[ViT-AD] Processing subject 54/311: 011_S_0183_aug1\n",
      "[ViT-AD] Processing subject 55/311: 011_S_0183_aug2\n",
      "[ViT-AD] Processing subject 56/311: 012_S_0689\n",
      "[ViT-AD] Processing subject 57/311: 012_S_0689_aug2\n",
      "[ViT-AD] Processing subject 58/311: 012_S_0712\n",
      "[ViT-AD] Processing subject 59/311: 012_S_0712_aug0\n",
      "[ViT-AD] Processing subject 60/311: 012_S_0720\n",
      "[ViT-AD] Processing subject 61/311: 012_S_0720_aug1\n",
      "[ViT-AD] Processing subject 62/311: 012_S_0803\n",
      "[ViT-AD] Processing subject 63/311: 012_S_0803_aug1\n",
      "[ViT-AD] Processing subject 64/311: 012_S_0803_aug2\n",
      "[ViT-AD] Processing subject 65/311: 013_S_1205\n",
      "[ViT-AD] Processing subject 66/311: 013_S_1205_aug1\n",
      "[ViT-AD] Processing subject 67/311: 014_S_0328\n",
      "[ViT-AD] Processing subject 68/311: 014_S_0328_aug0\n",
      "[ViT-AD] Processing subject 69/311: 014_S_0328_aug1\n",
      "[ViT-AD] Processing subject 70/311: 014_S_0328_aug2\n",
      "[ViT-AD] Processing subject 71/311: 014_S_1095\n",
      "[ViT-AD] Processing subject 72/311: 014_S_1095_aug0\n",
      "[ViT-AD] Processing subject 73/311: 014_S_1095_aug1\n",
      "[ViT-AD] Processing subject 74/311: 014_S_1095_aug2\n",
      "[ViT-AD] Processing subject 75/311: 016_S_0991\n",
      "[ViT-AD] Processing subject 76/311: 016_S_0991_aug0\n",
      "[ViT-AD] Processing subject 77/311: 016_S_0991_aug1\n",
      "[ViT-AD] Processing subject 78/311: 016_S_0991_aug2\n",
      "[ViT-AD] Processing subject 79/311: 018_S_0286\n",
      "[ViT-AD] Processing subject 80/311: 018_S_0286_aug1\n",
      "[ViT-AD] Processing subject 81/311: 018_S_0335\n",
      "[ViT-AD] Processing subject 82/311: 018_S_0335_aug0\n",
      "[ViT-AD] Processing subject 83/311: 018_S_0335_aug1\n",
      "[ViT-AD] Processing subject 84/311: 018_S_0335_aug2\n",
      "[ViT-AD] Processing subject 85/311: 018_S_0633\n",
      "[ViT-AD] Processing subject 86/311: 018_S_0633_aug0\n",
      "[ViT-AD] Processing subject 87/311: 018_S_0633_aug1\n",
      "[ViT-AD] Processing subject 88/311: 018_S_0682\n",
      "[ViT-AD] Processing subject 89/311: 018_S_0682_aug0\n",
      "[ViT-AD] Processing subject 90/311: 018_S_0682_aug1\n",
      "[ViT-AD] Processing subject 91/311: 018_S_0682_aug2\n",
      "[ViT-AD] Processing subject 92/311: 020_S_0213\n",
      "[ViT-AD] Processing subject 93/311: 020_S_0213_aug0\n",
      "[ViT-AD] Processing subject 94/311: 020_S_0213_aug2\n",
      "[ViT-AD] Processing subject 95/311: 021_S_0343\n",
      "[ViT-AD] Processing subject 96/311: 021_S_0343_aug1\n",
      "[ViT-AD] Processing subject 97/311: 021_S_0343_aug2\n",
      "[ViT-AD] Processing subject 98/311: 021_S_0642\n",
      "[ViT-AD] Processing subject 99/311: 021_S_0642_aug0\n",
      "[ViT-AD] Processing subject 100/311: 021_S_0642_aug1\n",
      "[ViT-AD] Processing subject 101/311: 021_S_0642_aug2\n",
      "[ViT-AD] Processing subject 102/311: 021_S_0753\n",
      "[ViT-AD] Processing subject 103/311: 021_S_0753_aug0\n",
      "[ViT-AD] Processing subject 104/311: 021_S_0753_aug1\n",
      "[ViT-AD] Processing subject 105/311: 021_S_0753_aug2\n",
      "[ViT-AD] Processing subject 106/311: 021_S_1109\n",
      "[ViT-AD] Processing subject 107/311: 021_S_1109_aug1\n",
      "[ViT-AD] Processing subject 108/311: 021_S_1109_aug2\n",
      "[ViT-AD] Processing subject 109/311: 022_S_0129\n",
      "[ViT-AD] Processing subject 110/311: 022_S_0129_aug1\n",
      "[ViT-AD] Processing subject 111/311: 022_S_0129_aug2\n",
      "[ViT-AD] Processing subject 112/311: 023_S_0083\n",
      "[ViT-AD] Processing subject 113/311: 023_S_0083_aug0\n",
      "[ViT-AD] Processing subject 114/311: 023_S_0083_aug2\n",
      "[ViT-AD] Processing subject 115/311: 023_S_0084\n",
      "[ViT-AD] Processing subject 116/311: 023_S_0084_aug0\n",
      "[ViT-AD] Processing subject 117/311: 023_S_0084_aug1\n",
      "[ViT-AD] Processing subject 118/311: 023_S_0139\n",
      "[ViT-AD] Processing subject 119/311: 023_S_0139_aug1\n",
      "[ViT-AD] Processing subject 120/311: 023_S_0916\n",
      "[ViT-AD] Processing subject 121/311: 023_S_0916_aug0\n",
      "[ViT-AD] Processing subject 122/311: 023_S_0916_aug1\n",
      "[ViT-AD] Processing subject 123/311: 023_S_0916_aug2\n",
      "[ViT-AD] Processing subject 124/311: 023_S_1262\n",
      "[ViT-AD] Processing subject 125/311: 024_S_1171\n",
      "[ViT-AD] Processing subject 126/311: 024_S_1171_aug0\n",
      "[ViT-AD] Processing subject 127/311: 024_S_1171_aug2\n",
      "[ViT-AD] Processing subject 128/311: 024_S_1307\n",
      "[ViT-AD] Processing subject 129/311: 027_S_0404\n",
      "[ViT-AD] Processing subject 130/311: 027_S_0404_aug1\n",
      "[ViT-AD] Processing subject 131/311: 027_S_0404_aug2\n",
      "[ViT-AD] Processing subject 132/311: 027_S_0850\n",
      "[ViT-AD] Processing subject 133/311: 027_S_0850_aug0\n",
      "[ViT-AD] Processing subject 134/311: 027_S_0850_aug1\n",
      "[ViT-AD] Processing subject 135/311: 027_S_0850_aug2\n",
      "[ViT-AD] Processing subject 136/311: 027_S_1081\n",
      "[ViT-AD] Processing subject 137/311: 027_S_1081_aug0\n",
      "[ViT-AD] Processing subject 138/311: 027_S_1082\n",
      "[ViT-AD] Processing subject 139/311: 027_S_1082_aug0\n",
      "[ViT-AD] Processing subject 140/311: 027_S_1082_aug2\n",
      "[ViT-AD] Processing subject 141/311: 027_S_1254\n",
      "[ViT-AD] Processing subject 142/311: 027_S_1254_aug0\n",
      "[ViT-AD] Processing subject 143/311: 027_S_1254_aug1\n",
      "[ViT-AD] Processing subject 144/311: 027_S_1254_aug2\n",
      "[ViT-AD] Processing subject 145/311: 027_S_1385\n",
      "[ViT-AD] Processing subject 146/311: 027_S_1385_aug0\n",
      "[ViT-AD] Processing subject 147/311: 027_S_1385_aug1\n",
      "[ViT-AD] Processing subject 148/311: 027_S_1385_aug2\n",
      "[ViT-AD] Processing subject 149/311: 029_S_0836\n",
      "[ViT-AD] Processing subject 150/311: 029_S_0836_aug0\n",
      "[ViT-AD] Processing subject 151/311: 029_S_0836_aug1\n",
      "[ViT-AD] Processing subject 152/311: 029_S_0836_aug2\n",
      "[ViT-AD] Processing subject 153/311: 029_S_0999\n",
      "[ViT-AD] Processing subject 154/311: 029_S_0999_aug0\n",
      "[ViT-AD] Processing subject 155/311: 029_S_0999_aug1\n",
      "[ViT-AD] Processing subject 156/311: 029_S_0999_aug2\n",
      "[ViT-AD] Processing subject 157/311: 029_S_1056\n",
      "[ViT-AD] Processing subject 158/311: 029_S_1056_aug0\n",
      "[ViT-AD] Processing subject 159/311: 031_S_0321\n",
      "[ViT-AD] Processing subject 160/311: 031_S_0321_aug0\n",
      "[ViT-AD] Processing subject 161/311: 031_S_0321_aug2\n",
      "[ViT-AD] Processing subject 162/311: 031_S_0554\n",
      "[ViT-AD] Processing subject 163/311: 031_S_0554_aug1\n",
      "[ViT-AD] Processing subject 164/311: 031_S_0554_aug2\n",
      "[ViT-AD] Processing subject 165/311: 031_S_1209\n",
      "[ViT-AD] Processing subject 166/311: 031_S_1209_aug0\n",
      "[ViT-AD] Processing subject 167/311: 031_S_1209_aug1\n",
      "[ViT-AD] Processing subject 168/311: 031_S_1209_aug2\n",
      "[ViT-AD] Processing subject 169/311: 032_S_0147\n",
      "[ViT-AD] Processing subject 170/311: 032_S_0147_aug0\n",
      "[ViT-AD] Processing subject 171/311: 032_S_0147_aug1\n",
      "[ViT-AD] Processing subject 172/311: 032_S_0147_aug2\n",
      "[ViT-AD] Processing subject 173/311: 032_S_0400\n",
      "[ViT-AD] Processing subject 174/311: 032_S_0400_aug0\n",
      "[ViT-AD] Processing subject 175/311: 032_S_0400_aug1\n",
      "[ViT-AD] Processing subject 176/311: 032_S_0400_aug2\n",
      "[ViT-AD] Processing subject 177/311: 032_S_1101\n",
      "[ViT-AD] Processing subject 178/311: 032_S_1101_aug0\n",
      "[ViT-AD] Processing subject 179/311: 032_S_1101_aug1\n",
      "[ViT-AD] Processing subject 180/311: 032_S_1101_aug2\n",
      "[ViT-AD] Processing subject 181/311: 033_S_0724\n",
      "[ViT-AD] Processing subject 182/311: 033_S_0724_aug0\n",
      "[ViT-AD] Processing subject 183/311: 033_S_0724_aug1\n",
      "[ViT-AD] Processing subject 184/311: 033_S_0724_aug2\n",
      "[ViT-AD] Processing subject 185/311: 033_S_0733\n",
      "[ViT-AD] Processing subject 186/311: 033_S_0733_aug0\n",
      "[ViT-AD] Processing subject 187/311: 033_S_0733_aug1\n",
      "[ViT-AD] Processing subject 188/311: 033_S_0739\n",
      "[ViT-AD] Processing subject 189/311: 033_S_0739_aug0\n",
      "[ViT-AD] Processing subject 190/311: 033_S_0739_aug1\n",
      "[ViT-AD] Processing subject 191/311: 033_S_0739_aug2\n",
      "[ViT-AD] Processing subject 192/311: 033_S_0889\n",
      "[ViT-AD] Processing subject 193/311: 033_S_0889_aug0\n",
      "[ViT-AD] Processing subject 194/311: 033_S_0889_aug2\n",
      "[ViT-AD] Processing subject 195/311: 033_S_1281\n",
      "[ViT-AD] Processing subject 196/311: 033_S_1281_aug1\n",
      "[ViT-AD] Processing subject 197/311: 033_S_1281_aug2\n",
      "[ViT-AD] Processing subject 198/311: 033_S_1283\n",
      "[ViT-AD] Processing subject 199/311: 033_S_1283_aug1\n",
      "[ViT-AD] Processing subject 200/311: 033_S_1283_aug2\n",
      "[ViT-AD] Processing subject 201/311: 033_S_1285\n",
      "[ViT-AD] Processing subject 202/311: 033_S_1285_aug0\n",
      "[ViT-AD] Processing subject 203/311: 033_S_1285_aug1\n",
      "[ViT-AD] Processing subject 204/311: 033_S_1285_aug2\n",
      "[ViT-AD] Processing subject 205/311: 033_S_1308\n",
      "[ViT-AD] Processing subject 206/311: 033_S_1308_aug1\n",
      "[ViT-AD] Processing subject 207/311: 033_S_1308_aug2\n",
      "[ViT-AD] Processing subject 208/311: 035_S_0341\n",
      "[ViT-AD] Processing subject 209/311: 035_S_0341_aug0\n",
      "[ViT-AD] Processing subject 210/311: 035_S_0341_aug1\n",
      "[ViT-AD] Processing subject 211/311: 035_S_0341_aug2\n",
      "[ViT-AD] Processing subject 212/311: 036_S_0577\n",
      "[ViT-AD] Processing subject 213/311: 036_S_0577_aug0\n",
      "[ViT-AD] Processing subject 214/311: 036_S_0577_aug2\n",
      "[ViT-AD] Processing subject 215/311: 036_S_0759\n",
      "[ViT-AD] Processing subject 216/311: 036_S_0759_aug0\n",
      "[ViT-AD] Processing subject 217/311: 036_S_0759_aug1\n",
      "[ViT-AD] Processing subject 218/311: 036_S_0760\n",
      "[ViT-AD] Processing subject 219/311: 036_S_0760_aug0\n",
      "[ViT-AD] Processing subject 220/311: 036_S_0760_aug2\n",
      "[ViT-AD] Processing subject 221/311: 036_S_1001\n",
      "[ViT-AD] Processing subject 222/311: 036_S_1001_aug2\n",
      "[ViT-AD] Processing subject 223/311: 037_S_0627\n",
      "[ViT-AD] Processing subject 224/311: 037_S_0627_aug0\n",
      "[ViT-AD] Processing subject 225/311: 037_S_0627_aug1\n",
      "[ViT-AD] Processing subject 226/311: 037_S_0627_aug2\n",
      "[ViT-AD] Processing subject 227/311: 041_S_1368\n",
      "[ViT-AD] Processing subject 228/311: 041_S_1368_aug0\n",
      "[ViT-AD] Processing subject 229/311: 041_S_1368_aug1\n",
      "[ViT-AD] Processing subject 230/311: 051_S_1296\n",
      "[ViT-AD] Processing subject 231/311: 051_S_1296_aug0\n",
      "[ViT-AD] Processing subject 232/311: 051_S_1296_aug1\n",
      "[ViT-AD] Processing subject 233/311: 051_S_1296_aug2\n",
      "[ViT-AD] Processing subject 234/311: 053_S_1044\n",
      "[ViT-AD] Processing subject 235/311: 053_S_1044_aug0\n",
      "[ViT-AD] Processing subject 236/311: 053_S_1044_aug1\n",
      "[ViT-AD] Processing subject 237/311: 053_S_1044_aug2\n",
      "[ViT-AD] Processing subject 238/311: 057_S_0474\n",
      "[ViT-AD] Processing subject 239/311: 057_S_0474_aug0\n",
      "[ViT-AD] Processing subject 240/311: 057_S_0474_aug1\n",
      "[ViT-AD] Processing subject 241/311: 057_S_1371\n",
      "[ViT-AD] Processing subject 242/311: 057_S_1371_aug0\n",
      "[ViT-AD] Processing subject 243/311: 057_S_1371_aug1\n",
      "[ViT-AD] Processing subject 244/311: 057_S_1371_aug2\n",
      "[ViT-AD] Processing subject 245/311: 057_S_1373\n",
      "[ViT-AD] Processing subject 246/311: 057_S_1373_aug0\n",
      "[ViT-AD] Processing subject 247/311: 057_S_1373_aug1\n",
      "[ViT-AD] Processing subject 248/311: 057_S_1373_aug2\n",
      "[ViT-AD] Processing subject 249/311: 057_S_1379\n",
      "[ViT-AD] Processing subject 250/311: 057_S_1379_aug0\n",
      "[ViT-AD] Processing subject 251/311: 057_S_1379_aug1\n",
      "[ViT-AD] Processing subject 252/311: 057_S_1379_aug2\n",
      "[ViT-AD] Processing subject 253/311: 062_S_0535\n",
      "[ViT-AD] Processing subject 254/311: 062_S_0535_aug0\n",
      "[ViT-AD] Processing subject 255/311: 062_S_0535_aug1\n",
      "[ViT-AD] Processing subject 256/311: 062_S_0535_aug2\n",
      "[ViT-AD] Processing subject 257/311: 062_S_0690\n",
      "[ViT-AD] Processing subject 258/311: 062_S_0730\n",
      "[ViT-AD] Processing subject 259/311: 062_S_0793\n",
      "[ViT-AD] Processing subject 260/311: 067_S_0029\n",
      "[ViT-AD] Processing subject 261/311: 067_S_0076\n",
      "[ViT-AD] Processing subject 262/311: 068_S_0109\n",
      "[ViT-AD] Processing subject 263/311: 073_S_0565\n",
      "[ViT-AD] Processing subject 264/311: 082_S_1377\n",
      "[ViT-AD] Processing subject 265/311: 094_S_1027\n",
      "[ViT-AD] Processing subject 266/311: 094_S_1090\n",
      "[ViT-AD] Processing subject 267/311: 094_S_1164\n",
      "[ViT-AD] Processing subject 268/311: 094_S_1397\n",
      "[ViT-AD] Processing subject 269/311: 094_S_1402\n",
      "[ViT-AD] Processing subject 270/311: 098_S_0149\n",
      "[ViT-AD] Processing subject 271/311: 099_S_0372\n",
      "[ViT-AD] Processing subject 272/311: 099_S_0470\n",
      "[ViT-AD] Processing subject 273/311: 099_S_1144\n",
      "[ViT-AD] Processing subject 274/311: 109_S_1157\n",
      "[ViT-AD] Processing subject 275/311: 114_S_0374\n",
      "[ViT-AD] Processing subject 276/311: 114_S_0979\n",
      "[ViT-AD] Processing subject 277/311: 116_S_0370\n",
      "[ViT-AD] Processing subject 278/311: 116_S_0392\n",
      "[ViT-AD] Processing subject 279/311: 116_S_0487\n",
      "[ViT-AD] Processing subject 280/311: 123_S_0088\n",
      "[ViT-AD] Processing subject 281/311: 123_S_0091\n",
      "[ViT-AD] Processing subject 282/311: 123_S_0094\n",
      "[ViT-AD] Processing subject 283/311: 123_S_0162\n",
      "[ViT-AD] Processing subject 284/311: 126_S_0606\n",
      "[ViT-AD] Processing subject 285/311: 126_S_0784\n",
      "[ViT-AD] Processing subject 286/311: 126_S_0891\n",
      "[ViT-AD] Processing subject 287/311: 126_S_1221\n",
      "[ViT-AD] Processing subject 288/311: 127_S_0431\n",
      "[ViT-AD] Processing subject 289/311: 127_S_0754\n",
      "[ViT-AD] Processing subject 290/311: 127_S_0844\n",
      "[ViT-AD] Processing subject 291/311: 127_S_1382\n",
      "[ViT-AD] Processing subject 292/311: 130_S_0956\n",
      "[ViT-AD] Processing subject 293/311: 130_S_1201\n",
      "[ViT-AD] Processing subject 294/311: 130_S_1290\n",
      "[ViT-AD] Processing subject 295/311: 130_S_1337\n",
      "[ViT-AD] Processing subject 296/311: 131_S_0457\n",
      "[ViT-AD] Processing subject 297/311: 131_S_0497\n",
      "[ViT-AD] Processing subject 298/311: 133_S_1170\n",
      "[ViT-AD] Processing subject 299/311: 136_S_0194\n",
      "[ViT-AD] Processing subject 300/311: 136_S_0299\n",
      "[ViT-AD] Processing subject 301/311: 136_S_0300\n",
      "[ViT-AD] Processing subject 302/311: 136_S_0426\n",
      "[ViT-AD] Processing subject 303/311: 137_S_0366\n",
      "[ViT-AD] Processing subject 304/311: 137_S_0796\n",
      "[ViT-AD] Processing subject 305/311: 137_S_1041\n",
      "[ViT-AD] Processing subject 306/311: 141_S_0696\n",
      "[ViT-AD] Processing subject 307/311: 141_S_0790\n",
      "[ViT-AD] Processing subject 308/311: 141_S_0852\n",
      "[ViT-AD] Processing subject 309/311: 141_S_0853\n",
      "[ViT-AD] Processing subject 310/311: 141_S_1137\n",
      "[ViT-AD] Processing subject 311/311: 141_S_1152\n",
      "[ViT-CN] Processing subject 1/311: 002_S_0295\n",
      "[ViT-CN] Processing subject 2/311: 002_S_0295_aug0\n",
      "[ViT-CN] Processing subject 3/311: 002_S_0295_aug1\n",
      "[ViT-CN] Processing subject 4/311: 002_S_0413\n",
      "[ViT-CN] Processing subject 5/311: 002_S_0413_aug1\n",
      "[ViT-CN] Processing subject 6/311: 002_S_0685\n",
      "[ViT-CN] Processing subject 7/311: 002_S_0685_aug0\n",
      "[ViT-CN] Processing subject 8/311: 002_S_0685_aug1\n",
      "[ViT-CN] Processing subject 9/311: 002_S_1261\n",
      "[ViT-CN] Processing subject 10/311: 002_S_1261_aug0\n",
      "[ViT-CN] Processing subject 11/311: 002_S_1261_aug1\n",
      "[ViT-CN] Processing subject 12/311: 002_S_1280\n",
      "[ViT-CN] Processing subject 13/311: 002_S_1280_aug0\n",
      "[ViT-CN] Processing subject 14/311: 002_S_1280_aug1\n",
      "[ViT-CN] Processing subject 15/311: 003_S_0907\n",
      "[ViT-CN] Processing subject 16/311: 003_S_0907_aug0\n",
      "[ViT-CN] Processing subject 17/311: 003_S_0907_aug1\n",
      "[ViT-CN] Processing subject 18/311: 003_S_0981\n",
      "[ViT-CN] Processing subject 19/311: 003_S_0981_aug1\n",
      "[ViT-CN] Processing subject 20/311: 005_S_0223\n",
      "[ViT-CN] Processing subject 21/311: 005_S_0223_aug0\n",
      "[ViT-CN] Processing subject 22/311: 005_S_0223_aug1\n",
      "[ViT-CN] Processing subject 23/311: 005_S_0553\n",
      "[ViT-CN] Processing subject 24/311: 005_S_0553_aug0\n",
      "[ViT-CN] Processing subject 25/311: 005_S_0602\n",
      "[ViT-CN] Processing subject 26/311: 005_S_0602_aug1\n",
      "[ViT-CN] Processing subject 27/311: 005_S_0610\n",
      "[ViT-CN] Processing subject 28/311: 005_S_0610_aug0\n",
      "[ViT-CN] Processing subject 29/311: 005_S_0610_aug1\n",
      "[ViT-CN] Processing subject 30/311: 006_S_0498\n",
      "[ViT-CN] Processing subject 31/311: 006_S_0498_aug0\n",
      "[ViT-CN] Processing subject 32/311: 006_S_0498_aug1\n",
      "[ViT-CN] Processing subject 33/311: 006_S_0681\n",
      "[ViT-CN] Processing subject 34/311: 006_S_0681_aug0\n",
      "[ViT-CN] Processing subject 35/311: 006_S_0681_aug1\n",
      "[ViT-CN] Processing subject 36/311: 006_S_0731\n",
      "[ViT-CN] Processing subject 37/311: 006_S_0731_aug0\n",
      "[ViT-CN] Processing subject 38/311: 006_S_0731_aug1\n",
      "[ViT-CN] Processing subject 39/311: 007_S_0068\n",
      "[ViT-CN] Processing subject 40/311: 007_S_0068_aug0\n",
      "[ViT-CN] Processing subject 41/311: 007_S_0068_aug1\n",
      "[ViT-CN] Processing subject 42/311: 007_S_0070\n",
      "[ViT-CN] Processing subject 43/311: 007_S_0070_aug0\n",
      "[ViT-CN] Processing subject 44/311: 007_S_0070_aug1\n",
      "[ViT-CN] Processing subject 45/311: 007_S_1206\n",
      "[ViT-CN] Processing subject 46/311: 007_S_1206_aug0\n",
      "[ViT-CN] Processing subject 47/311: 007_S_1206_aug1\n",
      "[ViT-CN] Processing subject 48/311: 007_S_1222\n",
      "[ViT-CN] Processing subject 49/311: 007_S_1222_aug0\n",
      "[ViT-CN] Processing subject 50/311: 007_S_1222_aug1\n",
      "[ViT-CN] Processing subject 51/311: 009_S_0751\n",
      "[ViT-CN] Processing subject 52/311: 009_S_0751_aug0\n",
      "[ViT-CN] Processing subject 53/311: 009_S_0842\n",
      "[ViT-CN] Processing subject 54/311: 009_S_0842_aug0\n",
      "[ViT-CN] Processing subject 55/311: 009_S_0862\n",
      "[ViT-CN] Processing subject 56/311: 009_S_0862_aug0\n",
      "[ViT-CN] Processing subject 57/311: 009_S_0862_aug1\n",
      "[ViT-CN] Processing subject 58/311: 010_S_0067\n",
      "[ViT-CN] Processing subject 59/311: 010_S_0067_aug0\n",
      "[ViT-CN] Processing subject 60/311: 010_S_0067_aug1\n",
      "[ViT-CN] Processing subject 61/311: 010_S_0419\n",
      "[ViT-CN] Processing subject 62/311: 010_S_0419_aug0\n",
      "[ViT-CN] Processing subject 63/311: 010_S_0419_aug1\n",
      "[ViT-CN] Processing subject 64/311: 010_S_0472\n",
      "[ViT-CN] Processing subject 65/311: 010_S_0472_aug0\n",
      "[ViT-CN] Processing subject 66/311: 010_S_0472_aug1\n",
      "[ViT-CN] Processing subject 67/311: 011_S_0005\n",
      "[ViT-CN] Processing subject 68/311: 011_S_0005_aug0\n",
      "[ViT-CN] Processing subject 69/311: 011_S_0005_aug1\n",
      "[ViT-CN] Processing subject 70/311: 011_S_0016\n",
      "[ViT-CN] Processing subject 71/311: 011_S_0016_aug0\n",
      "[ViT-CN] Processing subject 72/311: 011_S_0021\n",
      "[ViT-CN] Processing subject 73/311: 011_S_0021_aug0\n",
      "[ViT-CN] Processing subject 74/311: 011_S_0022\n",
      "[ViT-CN] Processing subject 75/311: 011_S_0022_aug0\n",
      "[ViT-CN] Processing subject 76/311: 011_S_0023\n",
      "[ViT-CN] Processing subject 77/311: 011_S_0023_aug0\n",
      "[ViT-CN] Processing subject 78/311: 011_S_0023_aug1\n",
      "[ViT-CN] Processing subject 79/311: 012_S_0637\n",
      "[ViT-CN] Processing subject 80/311: 012_S_0637_aug0\n",
      "[ViT-CN] Processing subject 81/311: 012_S_0637_aug1\n",
      "[ViT-CN] Processing subject 82/311: 012_S_1009\n",
      "[ViT-CN] Processing subject 83/311: 012_S_1009_aug0\n",
      "[ViT-CN] Processing subject 84/311: 012_S_1009_aug1\n",
      "[ViT-CN] Processing subject 85/311: 012_S_1133\n",
      "[ViT-CN] Processing subject 86/311: 012_S_1133_aug0\n",
      "[ViT-CN] Processing subject 87/311: 012_S_1133_aug1\n",
      "[ViT-CN] Processing subject 88/311: 013_S_0502\n",
      "[ViT-CN] Processing subject 89/311: 013_S_0502_aug0\n",
      "[ViT-CN] Processing subject 90/311: 013_S_0502_aug1\n",
      "[ViT-CN] Processing subject 91/311: 013_S_0575\n",
      "[ViT-CN] Processing subject 92/311: 013_S_0575_aug0\n",
      "[ViT-CN] Processing subject 93/311: 013_S_0575_aug1\n",
      "[ViT-CN] Processing subject 94/311: 013_S_1035\n",
      "[ViT-CN] Processing subject 95/311: 013_S_1035_aug0\n",
      "[ViT-CN] Processing subject 96/311: 013_S_1035_aug1\n",
      "[ViT-CN] Processing subject 97/311: 014_S_0519\n",
      "[ViT-CN] Processing subject 98/311: 014_S_0519_aug0\n",
      "[ViT-CN] Processing subject 99/311: 014_S_0519_aug1\n",
      "[ViT-CN] Processing subject 100/311: 014_S_0520\n",
      "[ViT-CN] Processing subject 101/311: 014_S_0520_aug1\n",
      "[ViT-CN] Processing subject 102/311: 014_S_0548\n",
      "[ViT-CN] Processing subject 103/311: 014_S_0548_aug0\n",
      "[ViT-CN] Processing subject 104/311: 014_S_0548_aug1\n",
      "[ViT-CN] Processing subject 105/311: 014_S_0558\n",
      "[ViT-CN] Processing subject 106/311: 014_S_0558_aug0\n",
      "[ViT-CN] Processing subject 107/311: 014_S_0558_aug1\n",
      "[ViT-CN] Processing subject 108/311: 016_S_0359\n",
      "[ViT-CN] Processing subject 109/311: 016_S_0359_aug0\n",
      "[ViT-CN] Processing subject 110/311: 016_S_0359_aug1\n",
      "[ViT-CN] Processing subject 111/311: 016_S_0538\n",
      "[ViT-CN] Processing subject 112/311: 016_S_0538_aug0\n",
      "[ViT-CN] Processing subject 113/311: 016_S_0538_aug1\n",
      "[ViT-CN] Processing subject 114/311: 018_S_0043\n",
      "[ViT-CN] Processing subject 115/311: 018_S_0043_aug0\n",
      "[ViT-CN] Processing subject 116/311: 018_S_0043_aug1\n",
      "[ViT-CN] Processing subject 117/311: 018_S_0055\n",
      "[ViT-CN] Processing subject 118/311: 018_S_0055_aug1\n",
      "[ViT-CN] Processing subject 119/311: 018_S_0369\n",
      "[ViT-CN] Processing subject 120/311: 018_S_0369_aug0\n",
      "[ViT-CN] Processing subject 121/311: 018_S_0425\n",
      "[ViT-CN] Processing subject 122/311: 018_S_0425_aug0\n",
      "[ViT-CN] Processing subject 123/311: 018_S_0425_aug1\n",
      "[ViT-CN] Processing subject 124/311: 020_S_0097\n",
      "[ViT-CN] Processing subject 125/311: 020_S_0097_aug1\n",
      "[ViT-CN] Processing subject 126/311: 020_S_0883\n",
      "[ViT-CN] Processing subject 127/311: 020_S_0883_aug0\n",
      "[ViT-CN] Processing subject 128/311: 020_S_0883_aug1\n",
      "[ViT-CN] Processing subject 129/311: 020_S_0899\n",
      "[ViT-CN] Processing subject 130/311: 020_S_0899_aug1\n",
      "[ViT-CN] Processing subject 131/311: 020_S_1288\n",
      "[ViT-CN] Processing subject 132/311: 020_S_1288_aug0\n",
      "[ViT-CN] Processing subject 133/311: 020_S_1288_aug1\n",
      "[ViT-CN] Processing subject 134/311: 021_S_0159\n",
      "[ViT-CN] Processing subject 135/311: 021_S_0159_aug0\n",
      "[ViT-CN] Processing subject 136/311: 021_S_0159_aug1\n",
      "[ViT-CN] Processing subject 137/311: 021_S_0337\n",
      "[ViT-CN] Processing subject 138/311: 021_S_0337_aug0\n",
      "[ViT-CN] Processing subject 139/311: 021_S_0647\n",
      "[ViT-CN] Processing subject 140/311: 021_S_0647_aug0\n",
      "[ViT-CN] Processing subject 141/311: 021_S_0984\n",
      "[ViT-CN] Processing subject 142/311: 021_S_0984_aug0\n",
      "[ViT-CN] Processing subject 143/311: 021_S_0984_aug1\n",
      "[ViT-CN] Processing subject 144/311: 022_S_0014\n",
      "[ViT-CN] Processing subject 145/311: 022_S_0014_aug0\n",
      "[ViT-CN] Processing subject 146/311: 022_S_0014_aug1\n",
      "[ViT-CN] Processing subject 147/311: 022_S_0066\n",
      "[ViT-CN] Processing subject 148/311: 022_S_0066_aug0\n",
      "[ViT-CN] Processing subject 149/311: 022_S_0096\n",
      "[ViT-CN] Processing subject 150/311: 022_S_0096_aug0\n",
      "[ViT-CN] Processing subject 151/311: 022_S_0096_aug1\n",
      "[ViT-CN] Processing subject 152/311: 022_S_0130\n",
      "[ViT-CN] Processing subject 153/311: 022_S_0130_aug1\n",
      "[ViT-CN] Processing subject 154/311: 023_S_0031\n",
      "[ViT-CN] Processing subject 155/311: 023_S_0031_aug0\n",
      "[ViT-CN] Processing subject 156/311: 023_S_0031_aug1\n",
      "[ViT-CN] Processing subject 157/311: 023_S_0058\n",
      "[ViT-CN] Processing subject 158/311: 023_S_0058_aug0\n",
      "[ViT-CN] Processing subject 159/311: 023_S_0058_aug1\n",
      "[ViT-CN] Processing subject 160/311: 023_S_0061\n",
      "[ViT-CN] Processing subject 161/311: 023_S_0061_aug0\n",
      "[ViT-CN] Processing subject 162/311: 023_S_0061_aug1\n",
      "[ViT-CN] Processing subject 163/311: 023_S_0081\n",
      "[ViT-CN] Processing subject 164/311: 023_S_0081_aug0\n",
      "[ViT-CN] Processing subject 165/311: 023_S_0926\n",
      "[ViT-CN] Processing subject 166/311: 023_S_0926_aug1\n",
      "[ViT-CN] Processing subject 167/311: 023_S_0963\n",
      "[ViT-CN] Processing subject 168/311: 023_S_0963_aug1\n",
      "[ViT-CN] Processing subject 169/311: 023_S_1190\n",
      "[ViT-CN] Processing subject 170/311: 023_S_1190_aug0\n",
      "[ViT-CN] Processing subject 171/311: 023_S_1190_aug1\n",
      "[ViT-CN] Processing subject 172/311: 024_S_0985\n",
      "[ViT-CN] Processing subject 173/311: 024_S_0985_aug1\n",
      "[ViT-CN] Processing subject 174/311: 024_S_1063\n",
      "[ViT-CN] Processing subject 175/311: 024_S_1063_aug0\n",
      "[ViT-CN] Processing subject 176/311: 024_S_1063_aug1\n",
      "[ViT-CN] Processing subject 177/311: 027_S_0074\n",
      "[ViT-CN] Processing subject 178/311: 027_S_0074_aug0\n",
      "[ViT-CN] Processing subject 179/311: 027_S_0074_aug1\n",
      "[ViT-CN] Processing subject 180/311: 027_S_0118\n",
      "[ViT-CN] Processing subject 181/311: 027_S_0118_aug0\n",
      "[ViT-CN] Processing subject 182/311: 027_S_0118_aug1\n",
      "[ViT-CN] Processing subject 183/311: 027_S_0120\n",
      "[ViT-CN] Processing subject 184/311: 027_S_0120_aug0\n",
      "[ViT-CN] Processing subject 185/311: 027_S_0403\n",
      "[ViT-CN] Processing subject 186/311: 027_S_0403_aug0\n",
      "[ViT-CN] Processing subject 187/311: 029_S_0824\n",
      "[ViT-CN] Processing subject 188/311: 029_S_0843\n",
      "[ViT-CN] Processing subject 189/311: 029_S_0845\n",
      "[ViT-CN] Processing subject 190/311: 029_S_0866\n",
      "[ViT-CN] Processing subject 191/311: 031_S_0618\n",
      "[ViT-CN] Processing subject 192/311: 032_S_0095\n",
      "[ViT-CN] Processing subject 193/311: 032_S_0479\n",
      "[ViT-CN] Processing subject 194/311: 032_S_0677\n",
      "[ViT-CN] Processing subject 195/311: 032_S_1169\n",
      "[ViT-CN] Processing subject 196/311: 033_S_0516\n",
      "[ViT-CN] Processing subject 197/311: 033_S_0734\n",
      "[ViT-CN] Processing subject 198/311: 033_S_0741\n",
      "[ViT-CN] Processing subject 199/311: 033_S_0920\n",
      "[ViT-CN] Processing subject 200/311: 033_S_0923\n",
      "[ViT-CN] Processing subject 201/311: 033_S_1016\n",
      "[ViT-CN] Processing subject 202/311: 033_S_1086\n",
      "[ViT-CN] Processing subject 203/311: 033_S_1098\n",
      "[ViT-CN] Processing subject 204/311: 035_S_0048\n",
      "[ViT-CN] Processing subject 205/311: 035_S_0156\n",
      "[ViT-CN] Processing subject 206/311: 035_S_0555\n",
      "[ViT-CN] Processing subject 207/311: 036_S_0576\n",
      "[ViT-CN] Processing subject 208/311: 036_S_0672\n",
      "[ViT-CN] Processing subject 209/311: 036_S_0813\n",
      "[ViT-CN] Processing subject 210/311: 036_S_1023\n",
      "[ViT-CN] Processing subject 211/311: 037_S_0303\n",
      "[ViT-CN] Processing subject 212/311: 037_S_0327\n",
      "[ViT-CN] Processing subject 213/311: 037_S_0454\n",
      "[ViT-CN] Processing subject 214/311: 037_S_0467\n",
      "[ViT-CN] Processing subject 215/311: 041_S_0125\n",
      "[ViT-CN] Processing subject 216/311: 041_S_0262\n",
      "[ViT-CN] Processing subject 217/311: 041_S_0898\n",
      "[ViT-CN] Processing subject 218/311: 041_S_1002\n",
      "[ViT-CN] Processing subject 219/311: 051_S_1123\n",
      "[ViT-CN] Processing subject 220/311: 052_S_0951\n",
      "[ViT-CN] Processing subject 221/311: 052_S_1250\n",
      "[ViT-CN] Processing subject 222/311: 052_S_1251\n",
      "[ViT-CN] Processing subject 223/311: 057_S_0643\n",
      "[ViT-CN] Processing subject 224/311: 057_S_0779\n",
      "[ViT-CN] Processing subject 225/311: 057_S_0818\n",
      "[ViT-CN] Processing subject 226/311: 057_S_0934\n",
      "[ViT-CN] Processing subject 227/311: 062_S_0578\n",
      "[ViT-CN] Processing subject 228/311: 062_S_0768\n",
      "[ViT-CN] Processing subject 229/311: 062_S_1099\n",
      "[ViT-CN] Processing subject 230/311: 067_S_0019\n",
      "[ViT-CN] Processing subject 231/311: 067_S_0056\n",
      "[ViT-CN] Processing subject 232/311: 067_S_0059\n",
      "[ViT-CN] Processing subject 233/311: 067_S_0177\n",
      "[ViT-CN] Processing subject 234/311: 067_S_0257\n",
      "[ViT-CN] Processing subject 235/311: 068_S_0127\n",
      "[ViT-CN] Processing subject 236/311: 068_S_0210\n",
      "[ViT-CN] Processing subject 237/311: 068_S_0473\n",
      "[ViT-CN] Processing subject 238/311: 072_S_0315\n",
      "[ViT-CN] Processing subject 239/311: 073_S_0089\n",
      "[ViT-CN] Processing subject 240/311: 073_S_0311\n",
      "[ViT-CN] Processing subject 241/311: 073_S_0312\n",
      "[ViT-CN] Processing subject 242/311: 073_S_0386\n",
      "[ViT-CN] Processing subject 243/311: 082_S_1256\n",
      "[ViT-CN] Processing subject 244/311: 094_S_0526\n",
      "[ViT-CN] Processing subject 245/311: 094_S_0692\n",
      "[ViT-CN] Processing subject 246/311: 094_S_0711\n",
      "[ViT-CN] Processing subject 247/311: 094_S_1241\n",
      "[ViT-CN] Processing subject 248/311: 094_S_1267\n",
      "[ViT-CN] Processing subject 249/311: 098_S_0171\n",
      "[ViT-CN] Processing subject 250/311: 098_S_0172\n",
      "[ViT-CN] Processing subject 251/311: 098_S_0896\n",
      "[ViT-CN] Processing subject 252/311: 099_S_0040\n",
      "[ViT-CN] Processing subject 253/311: 099_S_0090\n",
      "[ViT-CN] Processing subject 254/311: 099_S_0352\n",
      "[ViT-CN] Processing subject 255/311: 099_S_0533\n",
      "[ViT-CN] Processing subject 256/311: 099_S_0534\n",
      "[ViT-CN] Processing subject 257/311: 100_S_0015\n",
      "[ViT-CN] Processing subject 258/311: 100_S_0035\n",
      "[ViT-CN] Processing subject 259/311: 100_S_0047\n",
      "[ViT-CN] Processing subject 260/311: 100_S_0069\n",
      "[ViT-CN] Processing subject 261/311: 109_S_0967\n",
      "[ViT-CN] Processing subject 262/311: 109_S_1014\n",
      "[ViT-CN] Processing subject 263/311: 114_S_0166\n",
      "[ViT-CN] Processing subject 264/311: 114_S_0173\n",
      "[ViT-CN] Processing subject 265/311: 114_S_0416\n",
      "[ViT-CN] Processing subject 266/311: 114_S_0601\n",
      "[ViT-CN] Processing subject 267/311: 116_S_0382\n",
      "[ViT-CN] Processing subject 268/311: 116_S_0648\n",
      "[ViT-CN] Processing subject 269/311: 116_S_0657\n",
      "[ViT-CN] Processing subject 270/311: 116_S_1232\n",
      "[ViT-CN] Processing subject 271/311: 116_S_1249\n",
      "[ViT-CN] Processing subject 272/311: 123_S_0072\n",
      "[ViT-CN] Processing subject 273/311: 123_S_0106\n",
      "[ViT-CN] Processing subject 274/311: 123_S_0113\n",
      "[ViT-CN] Processing subject 275/311: 123_S_0298\n",
      "[ViT-CN] Processing subject 276/311: 126_S_0605\n",
      "[ViT-CN] Processing subject 277/311: 126_S_0680\n",
      "[ViT-CN] Processing subject 278/311: 127_S_0259\n",
      "[ViT-CN] Processing subject 279/311: 127_S_0260\n",
      "[ViT-CN] Processing subject 280/311: 127_S_0622\n",
      "[ViT-CN] Processing subject 281/311: 127_S_0684\n",
      "[ViT-CN] Processing subject 282/311: 128_S_0863\n",
      "[ViT-CN] Processing subject 283/311: 128_S_1242\n",
      "[ViT-CN] Processing subject 284/311: 129_S_0778\n",
      "[ViT-CN] Processing subject 285/311: 130_S_0232\n",
      "[ViT-CN] Processing subject 286/311: 130_S_0886\n",
      "[ViT-CN] Processing subject 287/311: 130_S_0969\n",
      "[ViT-CN] Processing subject 288/311: 130_S_1200\n",
      "[ViT-CN] Processing subject 289/311: 131_S_0123\n",
      "[ViT-CN] Processing subject 290/311: 131_S_0441\n",
      "[ViT-CN] Processing subject 291/311: 131_S_1301\n",
      "[ViT-CN] Processing subject 292/311: 133_S_0433\n",
      "[ViT-CN] Processing subject 293/311: 133_S_0488\n",
      "[ViT-CN] Processing subject 294/311: 133_S_0525\n",
      "[ViT-CN] Processing subject 295/311: 136_S_0086\n",
      "[ViT-CN] Processing subject 296/311: 136_S_0184\n",
      "[ViT-CN] Processing subject 297/311: 136_S_0186\n",
      "[ViT-CN] Processing subject 298/311: 136_S_0196\n",
      "[ViT-CN] Processing subject 299/311: 137_S_0283\n",
      "[ViT-CN] Processing subject 300/311: 137_S_0301\n",
      "[ViT-CN] Processing subject 301/311: 137_S_0459\n",
      "[ViT-CN] Processing subject 302/311: 137_S_0686\n",
      "[ViT-CN] Processing subject 303/311: 137_S_0972\n",
      "[ViT-CN] Processing subject 304/311: 141_S_0717\n",
      "[ViT-CN] Processing subject 305/311: 141_S_0726\n",
      "[ViT-CN] Processing subject 306/311: 141_S_0767\n",
      "[ViT-CN] Processing subject 307/311: 141_S_0810\n",
      "[ViT-CN] Processing subject 308/311: 141_S_1094\n",
      "[ViT-CN] Processing subject 309/311: 941_S_1194\n",
      "[ViT-CN] Processing subject 310/311: 941_S_1197\n",
      "[ViT-CN] Processing subject 311/311: 941_S_1202\n",
      "[ViT-LMCI] Processing subject 1/311: 002_S_0729\n",
      "[ViT-LMCI] Processing subject 2/311: 002_S_0782\n",
      "[ViT-LMCI] Processing subject 3/311: 002_S_0954\n",
      "[ViT-LMCI] Processing subject 4/311: 002_S_1070\n",
      "[ViT-LMCI] Processing subject 5/311: 002_S_1155\n",
      "[ViT-LMCI] Processing subject 6/311: 002_S_1268\n",
      "[ViT-LMCI] Processing subject 7/311: 003_S_0908\n",
      "[ViT-LMCI] Processing subject 8/311: 003_S_1057\n",
      "[ViT-LMCI] Processing subject 9/311: 003_S_1122\n",
      "[ViT-LMCI] Processing subject 10/311: 005_S_0222\n",
      "[ViT-LMCI] Processing subject 11/311: 005_S_0324\n",
      "[ViT-LMCI] Processing subject 12/311: 005_S_0448\n",
      "[ViT-LMCI] Processing subject 13/311: 005_S_0546\n",
      "[ViT-LMCI] Processing subject 14/311: 005_S_1224\n",
      "[ViT-LMCI] Processing subject 15/311: 006_S_0675\n",
      "[ViT-LMCI] Processing subject 16/311: 006_S_1130\n",
      "[ViT-LMCI] Processing subject 17/311: 007_S_0041\n",
      "[ViT-LMCI] Processing subject 18/311: 007_S_0101\n",
      "[ViT-LMCI] Processing subject 19/311: 007_S_0128\n",
      "[ViT-LMCI] Processing subject 20/311: 007_S_0249\n",
      "[ViT-LMCI] Processing subject 21/311: 007_S_0293\n",
      "[ViT-LMCI] Processing subject 22/311: 007_S_0414\n",
      "[ViT-LMCI] Processing subject 23/311: 007_S_0698\n",
      "[ViT-LMCI] Processing subject 24/311: 009_S_1030\n",
      "[ViT-LMCI] Processing subject 25/311: 010_S_0422\n",
      "[ViT-LMCI] Processing subject 26/311: 010_S_0904\n",
      "[ViT-LMCI] Processing subject 27/311: 011_S_0168\n",
      "[ViT-LMCI] Processing subject 28/311: 011_S_0241\n",
      "[ViT-LMCI] Processing subject 29/311: 011_S_0326\n",
      "[ViT-LMCI] Processing subject 30/311: 011_S_0362\n",
      "[ViT-LMCI] Processing subject 31/311: 011_S_0856\n",
      "[ViT-LMCI] Processing subject 32/311: 011_S_0861\n",
      "[ViT-LMCI] Processing subject 33/311: 011_S_1080\n",
      "[ViT-LMCI] Processing subject 34/311: 011_S_1282\n",
      "[ViT-LMCI] Processing subject 35/311: 012_S_0634\n",
      "[ViT-LMCI] Processing subject 36/311: 012_S_0932\n",
      "[ViT-LMCI] Processing subject 37/311: 012_S_1033\n",
      "[ViT-LMCI] Processing subject 38/311: 012_S_1165\n",
      "[ViT-LMCI] Processing subject 39/311: 012_S_1292\n",
      "[ViT-LMCI] Processing subject 40/311: 012_S_1321\n",
      "[ViT-LMCI] Processing subject 41/311: 013_S_0240\n",
      "[ViT-LMCI] Processing subject 42/311: 013_S_0325\n",
      "[ViT-LMCI] Processing subject 43/311: 013_S_0860\n",
      "[ViT-LMCI] Processing subject 44/311: 013_S_1120\n",
      "[ViT-LMCI] Processing subject 45/311: 013_S_1186\n",
      "[ViT-LMCI] Processing subject 46/311: 013_S_1275\n",
      "[ViT-LMCI] Processing subject 47/311: 014_S_0169\n",
      "[ViT-LMCI] Processing subject 48/311: 014_S_0557\n",
      "[ViT-LMCI] Processing subject 49/311: 014_S_0563\n",
      "[ViT-LMCI] Processing subject 50/311: 014_S_0658\n",
      "[ViT-LMCI] Processing subject 51/311: 016_S_0354\n",
      "[ViT-LMCI] Processing subject 52/311: 016_S_0702\n",
      "[ViT-LMCI] Processing subject 53/311: 016_S_1028\n",
      "[ViT-LMCI] Processing subject 54/311: 016_S_1117\n",
      "[ViT-LMCI] Processing subject 55/311: 016_S_1121\n",
      "[ViT-LMCI] Processing subject 56/311: 016_S_1326\n",
      "[ViT-LMCI] Processing subject 57/311: 018_S_0057\n",
      "[ViT-LMCI] Processing subject 58/311: 018_S_0080\n",
      "[ViT-LMCI] Processing subject 59/311: 018_S_0087\n",
      "[ViT-LMCI] Processing subject 60/311: 018_S_0142\n",
      "[ViT-LMCI] Processing subject 61/311: 018_S_0155\n",
      "[ViT-LMCI] Processing subject 62/311: 018_S_0406\n",
      "[ViT-LMCI] Processing subject 63/311: 018_S_0450\n",
      "[ViT-LMCI] Processing subject 64/311: 021_S_0141\n",
      "[ViT-LMCI] Processing subject 65/311: 021_S_0231\n",
      "[ViT-LMCI] Processing subject 66/311: 021_S_0273\n",
      "[ViT-LMCI] Processing subject 67/311: 021_S_0276\n",
      "[ViT-LMCI] Processing subject 68/311: 021_S_0332\n",
      "[ViT-LMCI] Processing subject 69/311: 021_S_0424\n",
      "[ViT-LMCI] Processing subject 70/311: 021_S_0626\n",
      "[ViT-LMCI] Processing subject 71/311: 022_S_0004\n",
      "[ViT-LMCI] Processing subject 72/311: 022_S_0544\n",
      "[ViT-LMCI] Processing subject 73/311: 022_S_0750\n",
      "[ViT-LMCI] Processing subject 74/311: 022_S_0961\n",
      "[ViT-LMCI] Processing subject 75/311: 022_S_1351\n",
      "[ViT-LMCI] Processing subject 76/311: 022_S_1394\n",
      "[ViT-LMCI] Processing subject 77/311: 023_S_0030\n",
      "[ViT-LMCI] Processing subject 78/311: 023_S_0042\n",
      "[ViT-LMCI] Processing subject 79/311: 023_S_0078\n",
      "[ViT-LMCI] Processing subject 80/311: 023_S_0126\n",
      "[ViT-LMCI] Processing subject 81/311: 023_S_0217\n",
      "[ViT-LMCI] Processing subject 82/311: 023_S_0331\n",
      "[ViT-LMCI] Processing subject 83/311: 023_S_0376\n",
      "[ViT-LMCI] Processing subject 84/311: 023_S_0388\n",
      "[ViT-LMCI] Processing subject 85/311: 023_S_0604\n",
      "[ViT-LMCI] Processing subject 86/311: 023_S_0625\n",
      "[ViT-LMCI] Processing subject 87/311: 023_S_0855\n",
      "[ViT-LMCI] Processing subject 88/311: 023_S_0887\n",
      "[ViT-LMCI] Processing subject 89/311: 023_S_1046\n",
      "[ViT-LMCI] Processing subject 90/311: 023_S_1126\n",
      "[ViT-LMCI] Processing subject 91/311: 023_S_1247\n",
      "[ViT-LMCI] Processing subject 92/311: 024_S_1393\n",
      "[ViT-LMCI] Processing subject 93/311: 027_S_0116\n",
      "[ViT-LMCI] Processing subject 94/311: 027_S_0179\n",
      "[ViT-LMCI] Processing subject 95/311: 027_S_0256\n",
      "[ViT-LMCI] Processing subject 96/311: 027_S_0307\n",
      "[ViT-LMCI] Processing subject 97/311: 027_S_0408\n",
      "[ViT-LMCI] Processing subject 98/311: 027_S_0417\n",
      "[ViT-LMCI] Processing subject 99/311: 027_S_0461\n",
      "[ViT-LMCI] Processing subject 100/311: 027_S_0485\n",
      "[ViT-LMCI] Processing subject 101/311: 027_S_0644\n",
      "[ViT-LMCI] Processing subject 102/311: 027_S_0835\n",
      "[ViT-LMCI] Processing subject 103/311: 027_S_1045\n",
      "[ViT-LMCI] Processing subject 104/311: 027_S_1213\n",
      "[ViT-LMCI] Processing subject 105/311: 027_S_1277\n",
      "[ViT-LMCI] Processing subject 106/311: 027_S_1387\n",
      "[ViT-LMCI] Processing subject 107/311: 029_S_0878\n",
      "[ViT-LMCI] Processing subject 108/311: 029_S_0914\n",
      "[ViT-LMCI] Processing subject 109/311: 029_S_1073\n",
      "[ViT-LMCI] Processing subject 110/311: 029_S_1215\n",
      "[ViT-LMCI] Processing subject 111/311: 029_S_1318\n",
      "[ViT-LMCI] Processing subject 112/311: 029_S_1384\n",
      "[ViT-LMCI] Processing subject 113/311: 031_S_0294\n",
      "[ViT-LMCI] Processing subject 114/311: 031_S_0351\n",
      "[ViT-LMCI] Processing subject 115/311: 031_S_0568\n",
      "[ViT-LMCI] Processing subject 116/311: 031_S_0830\n",
      "[ViT-LMCI] Processing subject 117/311: 031_S_0867\n",
      "[ViT-LMCI] Processing subject 118/311: 031_S_1066\n",
      "[ViT-LMCI] Processing subject 119/311: 032_S_0187\n",
      "[ViT-LMCI] Processing subject 120/311: 032_S_0214\n",
      "[ViT-LMCI] Processing subject 121/311: 032_S_0718\n",
      "[ViT-LMCI] Processing subject 122/311: 032_S_0978\n",
      "[ViT-LMCI] Processing subject 123/311: 033_S_0511\n",
      "[ViT-LMCI] Processing subject 124/311: 033_S_0513\n",
      "[ViT-LMCI] Processing subject 125/311: 033_S_0514\n",
      "[ViT-LMCI] Processing subject 126/311: 033_S_0567\n",
      "[ViT-LMCI] Processing subject 127/311: 033_S_0723\n",
      "[ViT-LMCI] Processing subject 128/311: 033_S_0725\n",
      "[ViT-LMCI] Processing subject 129/311: 033_S_0906\n",
      "[ViT-LMCI] Processing subject 130/311: 033_S_0922\n",
      "[ViT-LMCI] Processing subject 131/311: 033_S_1116\n",
      "[ViT-LMCI] Processing subject 132/311: 033_S_1279\n",
      "[ViT-LMCI] Processing subject 133/311: 033_S_1284\n",
      "[ViT-LMCI] Processing subject 134/311: 033_S_1309\n",
      "[ViT-LMCI] Processing subject 135/311: 035_S_0033\n",
      "[ViT-LMCI] Processing subject 136/311: 035_S_0204\n",
      "[ViT-LMCI] Processing subject 137/311: 035_S_0292\n",
      "[ViT-LMCI] Processing subject 138/311: 035_S_0997\n",
      "[ViT-LMCI] Processing subject 139/311: 036_S_0656\n",
      "[ViT-LMCI] Processing subject 140/311: 036_S_0673\n",
      "[ViT-LMCI] Processing subject 141/311: 036_S_0748\n",
      "[ViT-LMCI] Processing subject 142/311: 036_S_0869\n",
      "[ViT-LMCI] Processing subject 143/311: 036_S_0945\n",
      "[ViT-LMCI] Processing subject 144/311: 036_S_0976\n",
      "[ViT-LMCI] Processing subject 145/311: 036_S_1135\n",
      "[ViT-LMCI] Processing subject 146/311: 036_S_1240\n",
      "[ViT-LMCI] Processing subject 147/311: 037_S_0150\n",
      "[ViT-LMCI] Processing subject 148/311: 037_S_0501\n",
      "[ViT-LMCI] Processing subject 149/311: 037_S_0539\n",
      "[ViT-LMCI] Processing subject 150/311: 037_S_0552\n",
      "[ViT-LMCI] Processing subject 151/311: 037_S_0566\n",
      "[ViT-LMCI] Processing subject 152/311: 037_S_0588\n",
      "[ViT-LMCI] Processing subject 153/311: 037_S_1078\n",
      "[ViT-LMCI] Processing subject 154/311: 037_S_1225\n",
      "[ViT-LMCI] Processing subject 155/311: 037_S_1421\n",
      "[ViT-LMCI] Processing subject 156/311: 041_S_0282\n",
      "[ViT-LMCI] Processing subject 157/311: 041_S_0314\n",
      "[ViT-LMCI] Processing subject 158/311: 041_S_0446\n",
      "[ViT-LMCI] Processing subject 159/311: 041_S_0549\n",
      "[ViT-LMCI] Processing subject 160/311: 041_S_0598\n",
      "[ViT-LMCI] Processing subject 161/311: 041_S_0679\n",
      "[ViT-LMCI] Processing subject 162/311: 041_S_1010\n",
      "[ViT-LMCI] Processing subject 163/311: 041_S_1260\n",
      "[ViT-LMCI] Processing subject 164/311: 041_S_1412\n",
      "[ViT-LMCI] Processing subject 165/311: 041_S_1418\n",
      "[ViT-LMCI] Processing subject 166/311: 041_S_1423\n",
      "[ViT-LMCI] Processing subject 167/311: 041_S_1425\n",
      "[ViT-LMCI] Processing subject 168/311: 051_S_1040\n",
      "[ViT-LMCI] Processing subject 169/311: 051_S_1072\n",
      "[ViT-LMCI] Processing subject 170/311: 051_S_1131\n",
      "[ViT-LMCI] Processing subject 171/311: 051_S_1331\n",
      "[ViT-LMCI] Processing subject 172/311: 052_S_0671\n",
      "[ViT-LMCI] Processing subject 173/311: 052_S_0952\n",
      "[ViT-LMCI] Processing subject 174/311: 052_S_1054\n",
      "[ViT-LMCI] Processing subject 175/311: 052_S_1168\n",
      "[ViT-LMCI] Processing subject 176/311: 052_S_1346\n",
      "[ViT-LMCI] Processing subject 177/311: 052_S_1352\n",
      "[ViT-LMCI] Processing subject 178/311: 053_S_0389\n",
      "[ViT-LMCI] Processing subject 179/311: 053_S_0507\n",
      "[ViT-LMCI] Processing subject 180/311: 053_S_0621\n",
      "[ViT-LMCI] Processing subject 181/311: 053_S_0919\n",
      "[ViT-LMCI] Processing subject 182/311: 057_S_0464\n",
      "[ViT-LMCI] Processing subject 183/311: 057_S_0839\n",
      "[ViT-LMCI] Processing subject 184/311: 057_S_0941\n",
      "[ViT-LMCI] Processing subject 185/311: 057_S_1007\n",
      "[ViT-LMCI] Processing subject 186/311: 057_S_1217\n",
      "[ViT-LMCI] Processing subject 187/311: 057_S_1265\n",
      "[ViT-LMCI] Processing subject 188/311: 057_S_1269\n",
      "[ViT-LMCI] Processing subject 189/311: 062_S_1182\n",
      "[ViT-LMCI] Processing subject 190/311: 062_S_1299\n",
      "[ViT-LMCI] Processing subject 191/311: 067_S_0038\n",
      "[ViT-LMCI] Processing subject 192/311: 067_S_0077\n",
      "[ViT-LMCI] Processing subject 193/311: 067_S_0098\n",
      "[ViT-LMCI] Processing subject 194/311: 067_S_0176\n",
      "[ViT-LMCI] Processing subject 195/311: 067_S_0284\n",
      "[ViT-LMCI] Processing subject 196/311: 067_S_0290\n",
      "[ViT-LMCI] Processing subject 197/311: 067_S_0336\n",
      "[ViT-LMCI] Processing subject 198/311: 067_S_0607\n",
      "[ViT-LMCI] Processing subject 199/311: 068_S_0442\n",
      "[ViT-LMCI] Processing subject 200/311: 068_S_0872\n",
      "[ViT-LMCI] Processing subject 201/311: 073_S_0518\n",
      "[ViT-LMCI] Processing subject 202/311: 073_S_0746\n",
      "[ViT-LMCI] Processing subject 203/311: 073_S_0909\n",
      "[ViT-LMCI] Processing subject 204/311: 082_S_0928\n",
      "[ViT-LMCI] Processing subject 205/311: 082_S_1119\n",
      "[ViT-LMCI] Processing subject 206/311: 094_S_0434\n",
      "[ViT-LMCI] Processing subject 207/311: 094_S_0531\n",
      "[ViT-LMCI] Processing subject 208/311: 094_S_0921\n",
      "[ViT-LMCI] Processing subject 209/311: 094_S_1188\n",
      "[ViT-LMCI] Processing subject 210/311: 094_S_1293\n",
      "[ViT-LMCI] Processing subject 211/311: 094_S_1398\n",
      "[ViT-LMCI] Processing subject 212/311: 094_S_1417\n",
      "[ViT-LMCI] Processing subject 213/311: 098_S_0160\n",
      "[ViT-LMCI] Processing subject 214/311: 098_S_0269\n",
      "[ViT-LMCI] Processing subject 215/311: 098_S_0667\n",
      "[ViT-LMCI] Processing subject 216/311: 099_S_0051\n",
      "[ViT-LMCI] Processing subject 217/311: 099_S_0054\n",
      "[ViT-LMCI] Processing subject 218/311: 099_S_0060\n",
      "[ViT-LMCI] Processing subject 219/311: 099_S_0111\n",
      "[ViT-LMCI] Processing subject 220/311: 099_S_0291\n",
      "[ViT-LMCI] Processing subject 221/311: 099_S_0551\n",
      "[ViT-LMCI] Processing subject 222/311: 099_S_0880\n",
      "[ViT-LMCI] Processing subject 223/311: 099_S_1034\n",
      "[ViT-LMCI] Processing subject 224/311: 100_S_0006\n",
      "[ViT-LMCI] Processing subject 225/311: 100_S_0190\n",
      "[ViT-LMCI] Processing subject 226/311: 100_S_0296\n",
      "[ViT-LMCI] Processing subject 227/311: 100_S_0995\n",
      "[ViT-LMCI] Processing subject 228/311: 109_S_0950\n",
      "[ViT-LMCI] Processing subject 229/311: 109_S_1114\n",
      "[ViT-LMCI] Processing subject 230/311: 109_S_1183\n",
      "[ViT-LMCI] Processing subject 231/311: 109_S_1343\n",
      "[ViT-LMCI] Processing subject 232/311: 114_S_0378\n",
      "[ViT-LMCI] Processing subject 233/311: 114_S_0410\n",
      "[ViT-LMCI] Processing subject 234/311: 114_S_0458\n",
      "[ViT-LMCI] Processing subject 235/311: 114_S_1103\n",
      "[ViT-LMCI] Processing subject 236/311: 114_S_1106\n",
      "[ViT-LMCI] Processing subject 237/311: 114_S_1118\n",
      "[ViT-LMCI] Processing subject 238/311: 116_S_0361\n",
      "[ViT-LMCI] Processing subject 239/311: 116_S_0649\n",
      "[ViT-LMCI] Processing subject 240/311: 116_S_0752\n",
      "[ViT-LMCI] Processing subject 241/311: 116_S_0834\n",
      "[ViT-LMCI] Processing subject 242/311: 116_S_1243\n",
      "[ViT-LMCI] Processing subject 243/311: 116_S_1271\n",
      "[ViT-LMCI] Processing subject 244/311: 116_S_1315\n",
      "[ViT-LMCI] Processing subject 245/311: 121_S_1322\n",
      "[ViT-LMCI] Processing subject 246/311: 123_S_0050\n",
      "[ViT-LMCI] Processing subject 247/311: 123_S_0108\n",
      "[ViT-LMCI] Processing subject 248/311: 123_S_0390\n",
      "[ViT-LMCI] Processing subject 249/311: 123_S_1300\n",
      "[ViT-LMCI] Processing subject 250/311: 126_S_0708\n",
      "[ViT-LMCI] Processing subject 251/311: 126_S_0709\n",
      "[ViT-LMCI] Processing subject 252/311: 126_S_0865\n",
      "[ViT-LMCI] Processing subject 253/311: 126_S_1187\n",
      "[ViT-LMCI] Processing subject 254/311: 127_S_0112\n",
      "[ViT-LMCI] Processing subject 255/311: 127_S_0393\n",
      "[ViT-LMCI] Processing subject 256/311: 127_S_0394\n",
      "[ViT-LMCI] Processing subject 257/311: 127_S_0925\n",
      "[ViT-LMCI] Processing subject 258/311: 127_S_1032\n",
      "[ViT-LMCI] Processing subject 259/311: 127_S_1140\n",
      "[ViT-LMCI] Processing subject 260/311: 127_S_1419\n",
      "[ViT-LMCI] Processing subject 261/311: 127_S_1427\n",
      "[ViT-LMCI] Processing subject 262/311: 128_S_0947\n",
      "[ViT-LMCI] Processing subject 263/311: 128_S_1043\n",
      "[ViT-LMCI] Processing subject 264/311: 128_S_1088\n",
      "[ViT-LMCI] Processing subject 265/311: 128_S_1148\n",
      "[ViT-LMCI] Processing subject 266/311: 128_S_1407\n",
      "[ViT-LMCI] Processing subject 267/311: 128_S_1408\n",
      "[ViT-LMCI] Processing subject 268/311: 129_S_1246\n",
      "[ViT-LMCI] Processing subject 269/311: 130_S_0102\n",
      "[ViT-LMCI] Processing subject 270/311: 130_S_0423\n",
      "[ViT-LMCI] Processing subject 271/311: 130_S_0505\n",
      "[ViT-LMCI] Processing subject 272/311: 130_S_0783\n",
      "[ViT-LMCI] Processing subject 273/311: 131_S_0384\n",
      "[ViT-LMCI] Processing subject 274/311: 131_S_1389\n",
      "[ViT-LMCI] Processing subject 275/311: 132_S_0987\n",
      "[ViT-LMCI] Processing subject 276/311: 133_S_0629\n",
      "[ViT-LMCI] Processing subject 277/311: 133_S_0638\n",
      "[ViT-LMCI] Processing subject 278/311: 133_S_0727\n",
      "[ViT-LMCI] Processing subject 279/311: 133_S_0771\n",
      "[ViT-LMCI] Processing subject 280/311: 133_S_0792\n",
      "[ViT-LMCI] Processing subject 281/311: 133_S_0912\n",
      "[ViT-LMCI] Processing subject 282/311: 133_S_0913\n",
      "[ViT-LMCI] Processing subject 283/311: 136_S_0107\n",
      "[ViT-LMCI] Processing subject 284/311: 136_S_0195\n",
      "[ViT-LMCI] Processing subject 285/311: 136_S_0429\n",
      "[ViT-LMCI] Processing subject 286/311: 136_S_0579\n",
      "[ViT-LMCI] Processing subject 287/311: 136_S_0695\n",
      "[ViT-LMCI] Processing subject 288/311: 136_S_0873\n",
      "[ViT-LMCI] Processing subject 289/311: 136_S_0874\n",
      "[ViT-LMCI] Processing subject 290/311: 136_S_1227\n",
      "[ViT-LMCI] Processing subject 291/311: 137_S_0158\n",
      "[ViT-LMCI] Processing subject 292/311: 137_S_0443\n",
      "[ViT-LMCI] Processing subject 293/311: 137_S_0481\n",
      "[ViT-LMCI] Processing subject 294/311: 137_S_0631\n",
      "[ViT-LMCI] Processing subject 295/311: 137_S_0668\n",
      "[ViT-LMCI] Processing subject 296/311: 137_S_0669\n",
      "[ViT-LMCI] Processing subject 297/311: 137_S_0722\n",
      "[ViT-LMCI] Processing subject 298/311: 137_S_0800\n",
      "[ViT-LMCI] Processing subject 299/311: 137_S_0825\n",
      "[ViT-LMCI] Processing subject 300/311: 137_S_0973\n",
      "[ViT-LMCI] Processing subject 301/311: 137_S_0994\n",
      "[ViT-LMCI] Processing subject 302/311: 137_S_1414\n",
      "[ViT-LMCI] Processing subject 303/311: 137_S_1426\n",
      "[ViT-LMCI] Processing subject 304/311: 141_S_0851\n",
      "[ViT-LMCI] Processing subject 305/311: 141_S_0915\n",
      "[ViT-LMCI] Processing subject 306/311: 141_S_0982\n",
      "[ViT-LMCI] Processing subject 307/311: 141_S_1004\n",
      "[ViT-LMCI] Processing subject 308/311: 141_S_1052\n",
      "[ViT-LMCI] Processing subject 309/311: 141_S_1245\n",
      "[ViT-LMCI] Processing subject 310/311: 141_S_1255\n",
      "[ViT-LMCI] Processing subject 311/311: 941_S_1311\n",
      "Total subjects processed with ViT: 933\n",
      "                                            features label       subject_id\n",
      "0  [1.2259233, -1.594083, -0.8396726, 1.570506, -...    AD       002_S_0619\n",
      "1  [0.9756239, -1.0976694, -0.80415076, 1.4985536...    AD  002_S_0619_aug1\n",
      "2  [1.4952722, -0.34109765, -0.3436658, 1.9679965...    AD  002_S_0619_aug2\n",
      "3  [1.6166587, -1.4291171, -1.5476516, 1.4495391,...    AD       002_S_0816\n",
      "4  [2.0281672, -1.4437877, -1.1708579, 1.706738, ...    AD  002_S_0816_aug0\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"D:\\Alz\\FINAL_BALANCED_DATASET\"\n",
    "vit_feature_df = build_vit_feature_matrix(base_path, vit_model, vit_transform, device)\n",
    "print(f\"Total subjects processed with ViT: {len(vit_feature_df)}\")\n",
    "print(vit_feature_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5682adc",
   "metadata": {},
   "source": [
    "Feature Confusion and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc280632",
   "metadata": {},
   "outputs": [],
   "source": [
    "Concatenate CNN + ViT Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8445aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_features(cnn_df, vit_df):\n",
    "    # Inner join on subject_id to ensure alignment\n",
    "    merged_df = pd.merge(cnn_df, vit_df, on='subject_id', suffixes=('_cnn', '_vit'))\n",
    "\n",
    "    fused_data = []\n",
    "    for _, row in merged_df.iterrows():\n",
    "        cnn_feat = row['features_cnn']\n",
    "        vit_feat = row['features_vit']\n",
    "        fused_feat = np.concatenate([cnn_feat, vit_feat])\n",
    "        fused_data.append({\n",
    "            'features': fused_feat,\n",
    "            'label': row['label_cnn'],  # same label in both\n",
    "            'subject_id': row['subject_id']\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(fused_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f14af08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused feature shape: (178432,)\n",
      "Total subjects fused: 933\n"
     ]
    }
   ],
   "source": [
    "fused_df = fuse_features(feature_df, vit_feature_df)\n",
    "print(f\"Fused feature shape: {fused_df.iloc[0]['features'].shape}\")\n",
    "print(f\"Total subjects fused: {len(fused_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f12b9",
   "metadata": {},
   "source": [
    "Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2355a3",
   "metadata": {},
   "source": [
    "Applying PCA to fused features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_pca(fused_df, n_components=512):\n",
    "    # Extract feature matrix\n",
    "    X = np.stack(fused_df['features'].values)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Build reduced DataFrame\n",
    "    reduced_df = pd.DataFrame({\n",
    "        'features': list(X_pca),\n",
    "        'label': fused_df['label'].values,\n",
    "        'subject_id': fused_df['subject_id'].values\n",
    "    })\n",
    "\n",
    "    print(f\"PCA explained variance ratio (top {n_components}): {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "    return reduced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA explained variance ratio (top 512): 0.9649\n",
      "Reduced feature shape: (512,)\n"
     ]
    }
   ],
   "source": [
    "reduced_df = apply_pca(fused_df, n_components=512)\n",
    "print(f\"Reduced feature shape: {reduced_df.iloc[0]['features'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d5b60",
   "metadata": {},
   "source": [
    "Train-Test Split + Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af2514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def stratified_split(df, test_size=0.2, random_state=42):\n",
    "    X = np.stack(df['features'].values)\n",
    "    y = df['label'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465294eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 746, Test size: 187\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = stratified_split(reduced_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154736e",
   "metadata": {},
   "source": [
    "Training on SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def train_evaluate_svm(X_train, X_test, y_train, y_test):\n",
    "    # Initialize SVM with RBF kernel\n",
    "    svm = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = svm.predict(X_test)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc6eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[35  1 26]\n",
      " [16 27 19]\n",
      " [ 0 11 52]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.69      0.56      0.62        62\n",
      "          CN       0.69      0.44      0.53        62\n",
      "        LMCI       0.54      0.83      0.65        63\n",
      "\n",
      "    accuracy                           0.61       187\n",
      "   macro avg       0.64      0.61      0.60       187\n",
      "weighted avg       0.64      0.61      0.60       187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_model = train_evaluate_svm(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f068142a",
   "metadata": {},
   "source": [
    "Permutation Feature Importance (SVM-Compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf246586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def interpret_svm_features(model, X_test, y_test, top_k=20, save_path=None):\n",
    "    try:\n",
    "        result = permutation_importance(\n",
    "            model, X_test, y_test, n_repeats=10, random_state=42, scoring='accuracy'\n",
    "        )\n",
    "        importances = result.importances_mean\n",
    "\n",
    "        if importances is None or len(importances) == 0:\n",
    "            print(\"No importances returned. Check model or input data.\")\n",
    "            return None, None\n",
    "\n",
    "        indices = np.argsort(importances)[::-1][:top_k]\n",
    "\n",
    "        # Plot top-k important features\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(top_k), importances[indices], align='center', color='skyblue')\n",
    "        plt.xticks(range(top_k), [f'F{i}' for i in indices], rotation=45)\n",
    "        plt.title(\"Top Feature Importances (Permutation)\")\n",
    "        plt.xlabel(\"Feature Index\")\n",
    "        plt.ylabel(\"Importance\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            print(f\"Plot saved to: {save_path}\")\n",
    "        else:\n",
    "            plt.show(block=True)\n",
    "\n",
    "        return indices, importances[indices]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature interpretation: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e743ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to: svm_feature_importance.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_indices, top_importances = interpret_svm_features(\n",
    "    svm_model, X_test, y_test, top_k=20, save_path=\"svm_feature_importance.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d80abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-Only Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.62      0.55      0.58        62\n",
      "          CN       0.61      0.18      0.28        62\n",
      "        LMCI       0.50      0.90      0.64        63\n",
      "\n",
      "    accuracy                           0.55       187\n",
      "   macro avg       0.58      0.54      0.50       187\n",
      "weighted avg       0.58      0.55      0.50       187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Extract CNN features and labels\n",
    "X_cnn = np.stack(feature_df['features'].values)\n",
    "y_cnn = feature_df['label'].values\n",
    "\n",
    "# Step 2: Train-test split\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
    "    X_cnn, y_cnn, test_size=0.2, stratify=y_cnn, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Train SVM\n",
    "svm_cnn = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_cnn.fit(X_train_cnn, y_train_cnn)\n",
    "\n",
    "# Step 4: Predict and evaluate\n",
    "y_pred_cnn = svm_cnn.predict(X_test_cnn)\n",
    "print(\"CNN-Only Classification Report:\")\n",
    "print(classification_report(y_test_cnn, y_pred_cnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0549130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-Only Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.62      0.55      0.58        62\n",
      "          CN       0.61      0.18      0.28        62\n",
      "        LMCI       0.50      0.90      0.64        63\n",
      "\n",
      "    accuracy                           0.55       187\n",
      "   macro avg       0.58      0.54      0.50       187\n",
      "weighted avg       0.58      0.55      0.50       187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN-Only Classification Report:\")\n",
    "print(classification_report(y_test_cnn, y_pred_cnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07418a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-Only Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.59      0.53      0.56        62\n",
      "          CN       0.80      0.13      0.22        62\n",
      "        LMCI       0.51      0.98      0.67        63\n",
      "\n",
      "    accuracy                           0.55       187\n",
      "   macro avg       0.63      0.55      0.49       187\n",
      "weighted avg       0.63      0.55      0.49       187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Extract ViT features and labels\n",
    "X_vit = np.stack(vit_feature_df['features'].values)\n",
    "y_vit = vit_feature_df['label'].values\n",
    "\n",
    "# Step 2: Train-test split\n",
    "X_train_vit, X_test_vit, y_train_vit, y_test_vit = train_test_split(\n",
    "    X_vit, y_vit, test_size=0.2, stratify=y_vit, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Train SVM\n",
    "svm_vit = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_vit.fit(X_train_vit, y_train_vit)\n",
    "\n",
    "# Step 4: Predict and evaluate\n",
    "y_pred_vit = svm_vit.predict(X_test_vit)\n",
    "print(\"ViT-Only Classification Report:\")\n",
    "print(classification_report(y_test_vit, y_pred_vit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8a02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-Only Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.59      0.53      0.56        62\n",
      "          CN       0.80      0.13      0.22        62\n",
      "        LMCI       0.51      0.98      0.67        63\n",
      "\n",
      "    accuracy                           0.55       187\n",
      "   macro avg       0.63      0.55      0.49       187\n",
      "weighted avg       0.63      0.55      0.49       187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ViT-Only Classification Report:\")\n",
    "print(classification_report(y_test_vit, y_pred_vit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba15191",
   "metadata": {},
   "source": [
    "Weighted Fusion Based on Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_importance(importances, total_dims=512):\n",
    "    weights = np.zeros(total_dims)\n",
    "    for idx, score in zip(top_indices, top_importances):\n",
    "        weights[idx] = score\n",
    "    weights /= np.sum(weights)  # Normalize to sum = 1\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f5a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weighted_fusion(fused_df, weights):\n",
    "    X = np.stack(fused_df['features'].values)\n",
    "    X_weighted = X * weights  # Element-wise weighting\n",
    "\n",
    "    weighted_df = pd.DataFrame({\n",
    "        'features': list(X_weighted),\n",
    "        'label': fused_df['label'].values,\n",
    "        'subject_id': fused_df['subject_id'].values\n",
    "    })\n",
    "\n",
    "    return weighted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a075c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA explained variance ratio (top 512): 0.9649\n"
     ]
    }
   ],
   "source": [
    "reduced_df = apply_pca(fused_df, n_components=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weighted_fusion(reduced_df, weights):\n",
    "    X = np.stack(reduced_df['features'].values)\n",
    "    X_weighted = X * weights  # Now shapes match: (933, 512)  (512,)\n",
    "\n",
    "    weighted_df = pd.DataFrame({\n",
    "        'features': list(X_weighted),\n",
    "        'label': reduced_df['label'].values,\n",
    "        'subject_id': reduced_df['subject_id'].values\n",
    "    })\n",
    "\n",
    "    return weighted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9170dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Fusion Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.65      0.52      0.58        62\n",
      "          CN       0.59      0.32      0.42        62\n",
      "        LMCI       0.50      0.83      0.62        63\n",
      "\n",
      "    accuracy                           0.56       187\n",
      "   macro avg       0.58      0.55      0.54       187\n",
      "weighted avg       0.58      0.56      0.54       187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = normalize_importance(top_importances, total_dims=512)\n",
    "weighted_df = apply_weighted_fusion(reduced_df, weights)\n",
    "\n",
    "\n",
    "X = np.stack(weighted_df['features'].values)\n",
    "y = weighted_df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "svm_weighted = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_weighted.fit(X_train, y_train)\n",
    "y_pred = svm_weighted.predict(X_test)\n",
    "\n",
    "print(\"Weighted Fusion Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a5c09",
   "metadata": {},
   "source": [
    "Contrastive Alignment of CNN and ViT Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: Split fused features back into CNN and ViT\n",
    "cnn_features = np.stack(fused_df['features'].apply(lambda x: x[:len(x)//2]))\n",
    "vit_features = np.stack(fused_df['features'].apply(lambda x: x[len(x)//2:]))\n",
    "labels = fused_df['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2: Infer dimensions from a sample\n",
    "sample_vector = fused_df['features'].iloc[0]\n",
    "D_total = len(sample_vector)\n",
    "\n",
    "# If CNN and ViT were equal-length:\n",
    "cnn_dim = vit_dim = D_total // 2\n",
    "\n",
    "# Split features\n",
    "cnn_features = np.stack(fused_df['features'].apply(lambda x: x[:cnn_dim]))\n",
    "vit_features = np.stack(fused_df['features'].apply(lambda x: x[cnn_dim:]))\n",
    "labels = fused_df['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d800acdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN shape: (933, 89216)\n",
      "ViT shape: (933, 89216)\n",
      "Labels shape: (933,)\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN shape:\", cnn_features.shape)\n",
    "print(\"ViT shape:\", vit_features.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f71c0",
   "metadata": {},
   "source": [
    "Defining Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a12269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(cnn_embed, vit_embed, labels, margin=0.5):\n",
    "    batch_size = cnn_embed.size(0)\n",
    "    loss = 0.0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(batch_size):\n",
    "            if i == j:\n",
    "                continue\n",
    "            sim = F.cosine_similarity(cnn_embed[i], vit_embed[j], dim=0)\n",
    "            if labels[i] == labels[j]:\n",
    "                loss += (1 - sim)  # pull together\n",
    "            else:\n",
    "                loss += F.relu(sim - margin)  # push apart\n",
    "\n",
    "    return loss / (batch_size * (batch_size - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1: Create label mapping\n",
    "label_map = {'AD': 0, 'CN': 1, 'LMCI': 2}\n",
    "labels_encoded = np.array([label_map[label] for label in labels])\n",
    "\n",
    "# Step 2.2: Convert to PyTorch tensor\n",
    "label_tensor = torch.tensor(labels_encoded, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ba6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [0 1 2]\n",
      "Tensor shape: torch.Size([933])\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded labels:\", np.unique(labels_encoded))\n",
    "print(\"Tensor shape:\", label_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44145c0b",
   "metadata": {},
   "source": [
    "Building Alignment Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b44a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AlignmentNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.project(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_net = AlignmentNet(input_dim=89216, output_dim=128)\n",
    "vit_net = AlignmentNet(input_dim=89216, output_dim=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693a9d7",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4efe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\program files\\python313\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python313\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\program files\\python313\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\program files\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\acss\\appdata\\roaming\\python\\python313\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\program files\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597270c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Step 4.1: Move networks to device\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m device = \u001b[43mtorch\u001b[49m.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m cnn_net = cnn_net.to(device)\n\u001b[32m      6\u001b[39m vit_net = vit_net.to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Step 4.1: Move networks to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn_net = cnn_net.to(device)\n",
    "vit_net = vit_net.to(device)\n",
    "\n",
    "# Step 4.2: Prepare data\n",
    "cnn_tensor = cnn_tensor.to(device)\n",
    "vit_tensor = vit_tensor.to(device)\n",
    "label_tensor = label_tensor.to(device)\n",
    "\n",
    "# Step 4.3: Optimizer\n",
    "optimizer = optim.Adam(list(cnn_net.parameters()) + list(vit_net.parameters()), lr=1e-3)\n",
    "\n",
    "# Step 4.4: Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_embed = cnn_net(cnn_tensor)\n",
    "    vit_embed = vit_net(vit_tensor)\n",
    "\n",
    "    loss = contrastive_loss(cnn_embed, vit_embed, label_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Contrastive Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
