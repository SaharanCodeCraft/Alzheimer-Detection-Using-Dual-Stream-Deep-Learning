{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45d419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Project Configuration\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "PROJECT_ROOT = Path(\"..\")\n",
    "DATA_ROOT = PROJECT_ROOT / \"DATASET\" / \"final_balanced_dataset\" / \"FINAL_BALANCED_DATASET\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Device\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb39e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset root found: ..\\DATASET\\final_balanced_dataset\\FINAL_BALANCED_DATASET\n",
      "Class folders: ['AD', 'CN', 'LMCI', 'results']\n"
     ]
    }
   ],
   "source": [
    "assert DATA_ROOT.exists(), \"❌ final_dataset path does not exist\"\n",
    "print(\"✅ Dataset root found:\", DATA_ROOT)\n",
    "\n",
    "print(\"Class folders:\", [p.name for p in DATA_ROOT.iterdir() if p.is_dir()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a935a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\ADMIN\\Documents\\Alz_work\\Notebooks\n",
      "\n",
      "Parent directory contents:\n",
      "['DATASET', 'Notebooks', 'Results']\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nParent directory contents:\")\n",
    "print(os.listdir(\"..\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc32e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of data folder (if exists):\n",
      "❌ No data folder found at ../data\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nContents of data folder (if exists):\")\n",
    "if os.path.exists(\"../data\"):\n",
    "    print(os.listdir(\"../data\"))\n",
    "else:\n",
    "    print(\"❌ No data folder found at ../data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d720fcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:\n",
      "c:\\Users\\ADMIN\\Documents\\Alz_work\\Notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\")\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b531dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of parent directory (..):\n",
      "['DATASET', 'Notebooks', 'Results']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nContents of parent directory (..):\")\n",
    "print(os.listdir(\"..\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12bf7b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of grandparent directory (../..):\n",
      "['Alz_work', 'AM Project', 'desktop.ini', 'Discovery Studio', 'Document.rtf', 'Isro_work', 'My Music', 'My Pictures', 'My Videos', 'Nasa_work']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nContents of grandparent directory (../..):\")\n",
    "print(os.listdir(\"../..\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a37d9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found matches:\n",
      "C:\\Users\\ADMIN\\Documents\\Alz_work\\DATASET\\final_balanced_dataset\n",
      "C:\\Users\\ADMIN\\Documents\\Alz_work\\DATASET\\FINAL_BALANCED_DATASET\\final_balanced_dataset\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"..\").resolve()\n",
    "matches = list(root.rglob(\"final_balanced_dataset\"))\n",
    "\n",
    "print(\"Found matches:\")\n",
    "for m in matches:\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769713f",
   "metadata": {},
   "source": [
    "##### PATIENT-WISE DATASET INDEXING & LEAKAGE-PROOF SPLITS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eacfb03",
   "metadata": {},
   "source": [
    "building a subject-level index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95eaf329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subjects: 933\n",
      "class\n",
      "CN      311\n",
      "LMCI    311\n",
      "AD      311\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>class</th>\n",
       "      <th>subject_path</th>\n",
       "      <th>num_slices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CN_002_S_0295</td>\n",
       "      <td>CN</td>\n",
       "      <td>..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN_002_S_0295_aug0</td>\n",
       "      <td>CN</td>\n",
       "      <td>..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CN_002_S_0295_aug1</td>\n",
       "      <td>CN</td>\n",
       "      <td>..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN_002_S_0413</td>\n",
       "      <td>CN</td>\n",
       "      <td>..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CN_002_S_0413_aug1</td>\n",
       "      <td>CN</td>\n",
       "      <td>..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subject_id class  \\\n",
       "0       CN_002_S_0295    CN   \n",
       "1  CN_002_S_0295_aug0    CN   \n",
       "2  CN_002_S_0295_aug1    CN   \n",
       "3       CN_002_S_0413    CN   \n",
       "4  CN_002_S_0413_aug1    CN   \n",
       "\n",
       "                                        subject_path  num_slices  \n",
       "0  ..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...           2  \n",
       "1  ..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...          50  \n",
       "2  ..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...          50  \n",
       "3  ..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...           1  \n",
       "4  ..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...          50  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 2.1: Subject-wise Indexing\n",
    "# =========================\n",
    "\n",
    "data = []\n",
    "\n",
    "for class_name in [\"CN\", \"LMCI\", \"AD\"]:\n",
    "    class_dir = DATA_ROOT / class_name\n",
    "    assert class_dir.exists(), f\"Missing class folder: {class_name}\"\n",
    "    \n",
    "    for subject_dir in class_dir.iterdir():\n",
    "        if subject_dir.is_dir():\n",
    "            subject_id = f\"{class_name}_{subject_dir.name}\"\n",
    "            \n",
    "            slices = list(subject_dir.glob(\"*\"))\n",
    "            if len(slices) == 0:\n",
    "                continue\n",
    "            \n",
    "            data.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"class\": class_name,\n",
    "                \"subject_path\": subject_dir,\n",
    "                \"num_slices\": len(slices)\n",
    "            })\n",
    "\n",
    "subjects_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Total subjects:\", len(subjects_df))\n",
    "print(subjects_df[\"class\"].value_counts())\n",
    "subjects_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebaad926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT: ..\\DATASET\\final_balanced_dataset\\FINAL_BALANCED_DATASET\n",
      "\n",
      "Top-level contents:\n",
      "- AD\n",
      "- CN\n",
      "- final_folder_splits.json\n",
      "- LMCI\n",
      "- results\n",
      "- subject_splits.json\n",
      "- unique_subjects.json\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"\\nTop-level contents:\")\n",
    "for item in DATA_ROOT.iterdir():\n",
    "    print(\"-\", item.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c309a832",
   "metadata": {},
   "source": [
    "CANONICAL SUBJECT ID\n",
    "\n",
    "We will extract the base subject ID by removing augmentation suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e92a5433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total folders (including augmentations): 933\n",
      "\n",
      "Unique subjects after grouping:\n",
      "639\n",
      "\n",
      "Class-wise unique subject counts:\n",
      "class\n",
      "LMCI    311\n",
      "CN      195\n",
      "AD      133\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>class</th>\n",
       "      <th>subject_path</th>\n",
       "      <th>num_slices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CN_002_S_0295</td>\n",
       "      <td>CN</td>\n",
       "      <td>..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN_002_S_0295</td>\n",
       "      <td>CN</td>\n",
       "      <td>..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CN_002_S_0295</td>\n",
       "      <td>CN</td>\n",
       "      <td>..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN_002_S_0413</td>\n",
       "      <td>CN</td>\n",
       "      <td>..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CN_002_S_0413</td>\n",
       "      <td>CN</td>\n",
       "      <td>..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject_id class                                       subject_path  \\\n",
       "0  CN_002_S_0295    CN  ..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...   \n",
       "1  CN_002_S_0295    CN  ..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...   \n",
       "2  CN_002_S_0295    CN  ..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...   \n",
       "3  CN_002_S_0413    CN  ..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...   \n",
       "4  CN_002_S_0413    CN  ..\\DATASET\\final_balanced_dataset\\FINAL_BALANC...   \n",
       "\n",
       "   num_slices  \n",
       "0           2  \n",
       "1          50  \n",
       "2          50  \n",
       "3           1  \n",
       "4          50  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 2.1 (FIXED): Subject-wise Indexing with Augmentation Handling\n",
    "# =========================\n",
    "\n",
    "data = []\n",
    "\n",
    "for class_name in [\"CN\", \"LMCI\", \"AD\"]:\n",
    "    class_dir = DATA_ROOT / class_name\n",
    "    assert class_dir.exists(), f\"Missing class folder: {class_name}\"\n",
    "    \n",
    "    for subject_dir in class_dir.iterdir():\n",
    "        if subject_dir.is_dir():\n",
    "            \n",
    "            # Remove augmentation suffixes (_aug0, _aug1, etc.)\n",
    "            base_subject_id = subject_dir.name.split(\"_aug\")[0]\n",
    "            subject_id = f\"{class_name}_{base_subject_id}\"\n",
    "            \n",
    "            slices = list(subject_dir.glob(\"*\"))\n",
    "            if len(slices) == 0:\n",
    "                continue\n",
    "            \n",
    "            data.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"class\": class_name,\n",
    "                \"subject_path\": subject_dir,\n",
    "                \"num_slices\": len(slices)\n",
    "            })\n",
    "\n",
    "subjects_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Total folders (including augmentations):\", len(subjects_df))\n",
    "print(\"\\nUnique subjects after grouping:\")\n",
    "print(subjects_df[\"subject_id\"].nunique())\n",
    "\n",
    "print(\"\\nClass-wise unique subject counts:\")\n",
    "print(subjects_df.drop_duplicates(\"subject_id\")[\"class\"].value_counts())\n",
    "\n",
    "subjects_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978712eb",
   "metadata": {},
   "source": [
    "FINAL SUBJECT TABLE (CANONICAL)\n",
    "\n",
    "We will now create one row per REAL subject,\n",
    "and attach all augmented folders to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d211bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total REAL subjects: 639\n",
      "\n",
      "Class-wise REAL subject counts:\n",
      "class\n",
      "LMCI    311\n",
      "CN      195\n",
      "AD      133\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>class</th>\n",
       "      <th>subject_dirs</th>\n",
       "      <th>total_slices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CN_002_S_0295</td>\n",
       "      <td>CN</td>\n",
       "      <td>[..\\DATASET\\final_balanced_dataset\\FINAL_BALAN...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN_002_S_0413</td>\n",
       "      <td>CN</td>\n",
       "      <td>[..\\DATASET\\final_balanced_dataset\\FINAL_BALAN...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CN_002_S_0685</td>\n",
       "      <td>CN</td>\n",
       "      <td>[..\\DATASET\\final_balanced_dataset\\FINAL_BALAN...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN_002_S_1261</td>\n",
       "      <td>CN</td>\n",
       "      <td>[..\\DATASET\\final_balanced_dataset\\FINAL_BALAN...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CN_002_S_1280</td>\n",
       "      <td>CN</td>\n",
       "      <td>[..\\DATASET\\final_balanced_dataset\\FINAL_BALAN...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject_id class                                       subject_dirs  \\\n",
       "0  CN_002_S_0295    CN  [..\\DATASET\\final_balanced_dataset\\FINAL_BALAN...   \n",
       "1  CN_002_S_0413    CN  [..\\DATASET\\final_balanced_dataset\\FINAL_BALAN...   \n",
       "2  CN_002_S_0685    CN  [..\\DATASET\\final_balanced_dataset\\FINAL_BALAN...   \n",
       "3  CN_002_S_1261    CN  [..\\DATASET\\final_balanced_dataset\\FINAL_BALAN...   \n",
       "4  CN_002_S_1280    CN  [..\\DATASET\\final_balanced_dataset\\FINAL_BALAN...   \n",
       "\n",
       "   total_slices  \n",
       "0           102  \n",
       "1            51  \n",
       "2           101  \n",
       "3           102  \n",
       "4           101  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 2.1 FINAL: Canonical subject indexing\n",
    "# =========================\n",
    "\n",
    "records = {}\n",
    "\n",
    "for class_name in [\"CN\", \"LMCI\", \"AD\"]:\n",
    "    class_dir = DATA_ROOT / class_name\n",
    "    \n",
    "    for subject_dir in class_dir.iterdir():\n",
    "        if subject_dir.is_dir():\n",
    "            base_id = subject_dir.name.split(\"_aug\")[0]\n",
    "            subject_id = f\"{class_name}_{base_id}\"\n",
    "            \n",
    "            if subject_id not in records:\n",
    "                records[subject_id] = {\n",
    "                    \"subject_id\": subject_id,\n",
    "                    \"class\": class_name,\n",
    "                    \"subject_dirs\": [],\n",
    "                    \"total_slices\": 0\n",
    "                }\n",
    "            \n",
    "            slices = list(subject_dir.glob(\"*\"))\n",
    "            records[subject_id][\"subject_dirs\"].append(subject_dir)\n",
    "            records[subject_id][\"total_slices\"] += len(slices)\n",
    "\n",
    "subjects_df = pd.DataFrame(records.values())\n",
    "\n",
    "print(\"Total REAL subjects:\", len(subjects_df))\n",
    "print(\"\\nClass-wise REAL subject counts:\")\n",
    "print(subjects_df[\"class\"].value_counts())\n",
    "\n",
    "subjects_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87685d4",
   "metadata": {},
   "source": [
    "PATIENT-WISE TRAIN / VAL / TEST SPLIT (REAL SUBJECTS ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ef844a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subjects: 447\n",
      "Validation subjects: 96\n",
      "Test subjects: 96\n",
      "\n",
      "Class distribution:\n",
      "Train:\n",
      " class\n",
      "LMCI    218\n",
      "CN      136\n",
      "AD       93\n",
      "Name: count, dtype: int64\n",
      "Val:\n",
      " class\n",
      "LMCI    47\n",
      "CN      29\n",
      "AD      20\n",
      "Name: count, dtype: int64\n",
      "Test:\n",
      " class\n",
      "LMCI    46\n",
      "CN      30\n",
      "AD      20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 2.2: Patient-wise Train / Val / Test Split (REAL subjects)\n",
    "# =========================\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    subjects_df,\n",
    "    test_size=0.30,\n",
    "    stratify=subjects_df[\"class\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,\n",
    "    stratify=temp_df[\"class\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Train subjects:\", len(train_df))\n",
    "print(\"Validation subjects:\", len(val_df))\n",
    "print(\"Test subjects:\", len(test_df))\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(\"Train:\\n\", train_df[\"class\"].value_counts())\n",
    "print(\"Val:\\n\", val_df[\"class\"].value_counts())\n",
    "print(\"Test:\\n\", test_df[\"class\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687c684",
   "metadata": {},
   "source": [
    "HARD LEAKAGE CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56cc0626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No subject leakage across splits\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Leakage Verification\n",
    "# =========================\n",
    "\n",
    "assert set(train_df[\"subject_id\"]).isdisjoint(val_df[\"subject_id\"])\n",
    "assert set(train_df[\"subject_id\"]).isdisjoint(test_df[\"subject_id\"])\n",
    "assert set(val_df[\"subject_id\"]).isdisjoint(test_df[\"subject_id\"])\n",
    "\n",
    "print(\"✅ No subject leakage across splits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd93d3",
   "metadata": {},
   "source": [
    "SAVE & FREEZE SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a5c7b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Subject-wise splits saved at: ..\\results\\splits\n"
     ]
    }
   ],
   "source": [
    "splits_dir = RESULTS_DIR / \"splits\"\n",
    "splits_dir.mkdir(exist_ok=True)\n",
    "\n",
    "train_df.to_csv(splits_dir / \"train_subjects.csv\", index=False)\n",
    "val_df.to_csv(splits_dir / \"val_subjects.csv\", index=False)\n",
    "test_df.to_csv(splits_dir / \"test_subjects.csv\", index=False)\n",
    "\n",
    "print(\"✅ Subject-wise splits saved at:\", splits_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792731fe",
   "metadata": {},
   "source": [
    "FROZEN PREPROCESSING + SPLIT-AWARE AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1cd46",
   "metadata": {},
   "source": [
    "Create a single, frozen preprocessing + augmentation pipeline such that:\n",
    "\n",
    "✅ Same preprocessing everywhere (train / val / test)\n",
    "\n",
    "✅ Augmentation applied only to TRAIN\n",
    "\n",
    "❌ No augmentation leakage into val/test\n",
    "\n",
    "✅ All augmentations of a subject stay in the same split\n",
    "\n",
    "✅ Ready for CNN + ViT simultaneously\n",
    "\n",
    "\n",
    "What we freeze now\n",
    "\n",
    "Image size: 224 × 224\n",
    "\n",
    "Normalization: ImageNet mean/std\n",
    "(because you use ImageNet-pretrained CNN + ViT)\n",
    "\n",
    "Augmentations: medically safe only\n",
    "\n",
    "Dataset is subject-driven, not slice-driven"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8a7ea",
   "metadata": {},
   "source": [
    "defining transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0743c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transforms defined and frozen\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 3.1: Preprocessing & Augmentation Transforms\n",
    "# =========================\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# -------- TRAIN transforms (with augmentation) --------\n",
    "train_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomApply([\n",
    "        T.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "    ], p=0.5),\n",
    "    T.RandomApply([\n",
    "        T.GaussianBlur(kernel_size=3)\n",
    "    ], p=0.3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# -------- VAL / TEST transforms (NO augmentation) --------\n",
    "eval_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "print(\"✅ Transforms defined and frozen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf46a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99debf",
   "metadata": {},
   "source": [
    "BUILD SUBJECT-AWARE DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d35e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlzheimerSubjectDataset(Dataset):\n",
    "    def __init__(self, subjects_df, transform=None, cache=True):\n",
    "        self.subjects_df = subjects_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "        self._cache = {}\n",
    "\n",
    "        self.class_to_label = {\n",
    "            \"CN\": 0,\n",
    "            \"LMCI\": 1,\n",
    "            \"AD\": 2\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subjects_df)\n",
    "\n",
    "    def _load_subject(self, idx):\n",
    "        row = self.subjects_df.iloc[idx]\n",
    "        images = []\n",
    "\n",
    "        for subject_dir in row[\"subject_dirs\"]:\n",
    "            for img_path in subject_dir.rglob(\"*.png\"):\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                images.append(img)\n",
    "\n",
    "        if len(images) == 0:\n",
    "            raise RuntimeError(f\"No images found for subject {row['subject_id']}\")\n",
    "\n",
    "        return torch.stack(images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cache and idx in self._cache:\n",
    "            images = self._cache[idx]\n",
    "        else:\n",
    "            images = self._load_subject(idx)\n",
    "            if self.cache:\n",
    "                self._cache[idx] = images\n",
    "\n",
    "        label = self.class_to_label[self.subjects_df.iloc[idx][\"class\"]]\n",
    "        return images, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a226a",
   "metadata": {},
   "source": [
    "CREATE DATASETS (SPLIT-AWARE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6135f8",
   "metadata": {},
   "source": [
    "connect splits-> datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb983fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subjects: 447\n",
      "Val subjects: 96\n",
      "Test subjects: 96\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 3.3: Create Datasets\n",
    "# =========================\n",
    "\n",
    "train_dataset = AlzheimerSubjectDataset(train_df, transform=train_transforms)\n",
    "val_dataset   = AlzheimerSubjectDataset(val_df, transform=eval_transforms)\n",
    "test_dataset  = AlzheimerSubjectDataset(test_df, transform=eval_transforms)\n",
    "\n",
    "print(\"Train subjects:\", len(train_dataset))\n",
    "print(\"Val subjects:\", len(val_dataset))\n",
    "print(\"Test subjects:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aabfdb",
   "metadata": {},
   "source": [
    "sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "311684dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample label: 2\n",
      "Sample tensor shape: torch.Size([50, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 3.4: Dataset Sanity Check\n",
    "# =========================\n",
    "\n",
    "sample_images, sample_label = train_dataset[0]\n",
    "\n",
    "print(\"Sample label:\", sample_label)\n",
    "print(\"Sample tensor shape:\", sample_images.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af750d",
   "metadata": {},
   "source": [
    "CNN FEATURE EXTRACTOR (ResNet50, SUBJECT-LEVEL)\n",
    "For each subject:\n",
    "\n",
    "MRI slices → ResNet50 → slice features → subject embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24be27f",
   "metadata": {},
   "source": [
    "Load ResNet50 (feature extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c7d21ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ResNet50 loaded as feature extractor\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 4.1: Load CNN Backbone (ResNet50)\n",
    "# =========================\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "cnn_backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Remove classification head\n",
    "cnn_backbone.fc = nn.Identity()\n",
    "\n",
    "cnn_backbone = cnn_backbone.to(DEVICE)\n",
    "cnn_backbone.eval()\n",
    "\n",
    "print(\"✅ ResNet50 loaded as feature extractor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4a4a2",
   "metadata": {},
   "source": [
    "Subject-level CNN feature extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8db41882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Step 4.2: CNN Subject-level Feature Extraction\n",
    "# =========================\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_cnn_subject_features(model, dataset, batch_size=16):\n",
    "    model.eval()\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        images, label = dataset[idx]      # images: (num_slices, 3, 224, 224)\n",
    "        images = images.to(DEVICE)\n",
    "\n",
    "        slice_features = []\n",
    "\n",
    "        # Process slices in mini-batches\n",
    "        for i in range(0, images.size(0), batch_size):\n",
    "            batch = images[i:i+batch_size]\n",
    "            feats = model(batch)          # (batch, 2048)\n",
    "            slice_features.append(feats.cpu())\n",
    "\n",
    "        slice_features = torch.cat(slice_features, dim=0)   # (num_slices, 2048)\n",
    "\n",
    "        # Subject-level pooling\n",
    "        subject_feature = slice_features.mean(dim=0)        # (2048,)\n",
    "\n",
    "        all_features.append(subject_feature)\n",
    "        all_labels.append(label)\n",
    "\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"Processed {idx+1}/{len(dataset)} subjects\")\n",
    "\n",
    "    X = torch.stack(all_features)\n",
    "    y = torch.tensor(all_labels)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3071e4",
   "metadata": {},
   "source": [
    "Test on SMALL subset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2c4ef62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: torch.Size([5, 2048])\n",
      "Labels: [2, 1, 2, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 4.3: Small-scale Test Extraction\n",
    "# =========================\n",
    "\n",
    "small_subset = torch.utils.data.Subset(train_dataset, range(5))\n",
    "\n",
    "X_test, y_test = extract_cnn_subject_features(cnn_backbone, small_subset)\n",
    "\n",
    "print(\"Feature shape:\", X_test.shape)\n",
    "print(\"Labels:\", y_test.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4f7d5",
   "metadata": {},
   "source": [
    "Full extraction (train / val / test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7d67c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/447 subjects\n",
      "Processed 100/447 subjects\n",
      "Processed 150/447 subjects\n",
      "Processed 200/447 subjects\n",
      "Processed 250/447 subjects\n",
      "Processed 300/447 subjects\n",
      "Processed 350/447 subjects\n",
      "Processed 400/447 subjects\n",
      "Processed 50/96 subjects\n",
      "Processed 50/96 subjects\n",
      "Train features: torch.Size([447, 2048])\n",
      "Val features: torch.Size([96, 2048])\n",
      "Test features: torch.Size([96, 2048])\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 4.4: Full CNN Feature Extraction\n",
    "# =========================\n",
    "\n",
    "X_train_cnn, y_train = extract_cnn_subject_features(cnn_backbone, train_dataset)\n",
    "X_val_cnn, y_val     = extract_cnn_subject_features(cnn_backbone, val_dataset)\n",
    "X_test_cnn, y_test   = extract_cnn_subject_features(cnn_backbone, test_dataset)\n",
    "\n",
    "print(\"Train features:\", X_train_cnn.shape)\n",
    "print(\"Val features:\", X_val_cnn.shape)\n",
    "print(\"Test features:\", X_test_cnn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74759775",
   "metadata": {},
   "source": [
    "ViT-B/16 FEATURE EXTRACTION (SUBJECT-LEVEL)\n",
    "For each subject:\n",
    "\n",
    "MRI slices → ViT-B/16 → slice embeddings → subject embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299bf0a",
   "metadata": {},
   "source": [
    "loading vit-B/16 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c36aed19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ViT-B/16 loaded as feature extractor\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 5.1: Load ViT-B/16 Backbone\n",
    "# =========================\n",
    "\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "vit_model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Replace classification head with identity\n",
    "vit_model.heads = nn.Identity()\n",
    "\n",
    "vit_model = vit_model.to(DEVICE)\n",
    "vit_model.eval()\n",
    "\n",
    "print(\"✅ ViT-B/16 loaded as feature extractor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7180fa",
   "metadata": {},
   "source": [
    "ViT Subject-level Feature Extraction Function\n",
    "This mirrors the CNN extraction logic (important for fairness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "881feb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Step 5.2: ViT Subject-level Feature Extraction\n",
    "# =========================\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_vit_subject_features(model, dataset, batch_size=16):\n",
    "    model.eval()\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        images, label = dataset[idx]        # (num_slices, 3, 224, 224)\n",
    "        images = images.to(DEVICE)\n",
    "\n",
    "        slice_features = []\n",
    "\n",
    "        for i in range(0, images.size(0), batch_size):\n",
    "            batch = images[i:i+batch_size]\n",
    "            feats = model(batch)            # (batch, 768)\n",
    "            slice_features.append(feats.cpu())\n",
    "\n",
    "        slice_features = torch.cat(slice_features, dim=0)  # (num_slices, 768)\n",
    "\n",
    "        # Subject-level pooling\n",
    "        subject_feature = slice_features.mean(dim=0)       # (768,)\n",
    "\n",
    "        all_features.append(subject_feature)\n",
    "        all_labels.append(label)\n",
    "\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"Processed {idx+1}/{len(dataset)} subjects\")\n",
    "\n",
    "    X = torch.stack(all_features)\n",
    "    y = torch.tensor(all_labels)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05376b2b",
   "metadata": {},
   "source": [
    "SMALL-SCALE SAFETY TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abe5f6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT feature shape: torch.Size([5, 768])\n",
      "Labels: [2, 1, 2, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 5.3: Small-scale ViT Feature Test\n",
    "# =========================\n",
    "\n",
    "small_subset = torch.utils.data.Subset(train_dataset, range(5))\n",
    "\n",
    "X_vit_test, y_vit_test = extract_vit_subject_features(vit_model, small_subset)\n",
    "\n",
    "print(\"ViT feature shape:\", X_vit_test.shape)\n",
    "print(\"Labels:\", y_vit_test.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b9d4c0",
   "metadata": {},
   "source": [
    "FULL ViT FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a06cba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/447 subjects\n",
      "Processed 100/447 subjects\n",
      "Processed 150/447 subjects\n",
      "Processed 200/447 subjects\n",
      "Processed 250/447 subjects\n",
      "Processed 300/447 subjects\n",
      "Processed 350/447 subjects\n",
      "Processed 400/447 subjects\n",
      "Processed 50/96 subjects\n",
      "Processed 50/96 subjects\n",
      "Train ViT features: torch.Size([447, 768])\n",
      "Val ViT features: torch.Size([96, 768])\n",
      "Test ViT features: torch.Size([96, 768])\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 5.4: Full ViT Feature Extraction\n",
    "# =========================\n",
    "\n",
    "X_train_vit, y_train_vit = extract_vit_subject_features(vit_model, train_dataset)\n",
    "X_val_vit, y_val_vit     = extract_vit_subject_features(vit_model, val_dataset)\n",
    "X_test_vit, y_test_vit   = extract_vit_subject_features(vit_model, test_dataset)\n",
    "\n",
    "print(\"Train ViT features:\", X_train_vit.shape)\n",
    "print(\"Val ViT features:\", X_val_vit.shape)\n",
    "print(\"Test ViT features:\", X_test_vit.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9e7184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 2.3.3\n",
      "torch: 2.7.1+cu118\n",
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c0a71",
   "metadata": {},
   "source": [
    "BASELINE EXPERIMENTS (CNN, ViT, SIMPLE FUSION)\n",
    "\n",
    "Train and evaluate three baseline models:\n",
    "\n",
    "CNN-only (ResNet50 features)\n",
    "\n",
    "ViT-only (ViT-B/16 features)\n",
    "\n",
    "Simple CNN + ViT fusion (concatenation)\n",
    "\n",
    "Using:\n",
    "\n",
    "the same train/val/test splits\n",
    "\n",
    "no fancy tricks\n",
    "\n",
    "clear metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f62c133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Step 6.1: Baseline Utilities\n",
    "# =========================\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b3d91",
   "metadata": {},
   "source": [
    "CNN-ONLY BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d3a2c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CNN-only (Validation) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.43      0.41      0.42        29\n",
      "        LMCI       0.58      0.66      0.62        47\n",
      "          AD       0.47      0.35      0.40        20\n",
      "\n",
      "    accuracy                           0.52        96\n",
      "   macro avg       0.49      0.47      0.48        96\n",
      "weighted avg       0.51      0.52      0.51        96\n",
      "\n",
      "=== CNN-only (Test) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.41      0.53      0.46        30\n",
      "        LMCI       0.65      0.65      0.65        46\n",
      "          AD       0.64      0.35      0.45        20\n",
      "\n",
      "    accuracy                           0.55        96\n",
      "   macro avg       0.57      0.51      0.52        96\n",
      "weighted avg       0.57      0.55      0.55        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 6.2: CNN-only Baseline (SVM)\n",
    "# =========================\n",
    "\n",
    "svm_cnn = SVC(\n",
    "    kernel=\"linear\",\n",
    "    class_weight=\"balanced\",\n",
    "    probability=False,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "svm_cnn.fit(X_train_cnn.numpy(), y_train.numpy())\n",
    "\n",
    "y_val_pred_cnn = svm_cnn.predict(X_val_cnn.numpy())\n",
    "y_test_pred_cnn = svm_cnn.predict(X_test_cnn.numpy())\n",
    "\n",
    "print(\"=== CNN-only (Validation) ===\")\n",
    "print(classification_report(y_val.numpy(), y_val_pred_cnn, target_names=[\"CN\", \"LMCI\", \"AD\"]))\n",
    "\n",
    "print(\"=== CNN-only (Test) ===\")\n",
    "print(classification_report(y_test.numpy(), y_test_pred_cnn, target_names=[\"CN\", \"LMCI\", \"AD\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4846ca36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ViT-only (Validation) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.50      0.45      0.47        29\n",
      "        LMCI       0.67      0.83      0.74        47\n",
      "          AD       0.83      0.50      0.62        20\n",
      "\n",
      "    accuracy                           0.65        96\n",
      "   macro avg       0.67      0.59      0.61        96\n",
      "weighted avg       0.65      0.65      0.64        96\n",
      "\n",
      "=== ViT-only (Test) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.48      0.47      0.47        30\n",
      "        LMCI       0.59      0.72      0.65        46\n",
      "          AD       0.64      0.35      0.45        20\n",
      "\n",
      "    accuracy                           0.56        96\n",
      "   macro avg       0.57      0.51      0.52        96\n",
      "weighted avg       0.57      0.56      0.55        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 6.3: ViT-only Baseline (SVM)\n",
    "# =========================\n",
    "\n",
    "svm_vit = SVC(\n",
    "    kernel=\"linear\",\n",
    "    class_weight=\"balanced\",\n",
    "    probability=False,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "svm_vit.fit(X_train_vit.numpy(), y_train_vit.numpy())\n",
    "\n",
    "y_val_pred_vit = svm_vit.predict(X_val_vit.numpy())\n",
    "y_test_pred_vit = svm_vit.predict(X_test_vit.numpy())\n",
    "\n",
    "print(\"=== ViT-only (Validation) ===\")\n",
    "print(classification_report(y_val_vit.numpy(), y_val_pred_vit, target_names=[\"CN\", \"LMCI\", \"AD\"]))\n",
    "\n",
    "print(\"=== ViT-only (Test) ===\")\n",
    "print(classification_report(y_test_vit.numpy(), y_test_pred_vit, target_names=[\"CN\", \"LMCI\", \"AD\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "100e099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused feature dimension: 2816\n",
      "=== CNN + ViT Fusion (Validation) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.45      0.45      0.45        29\n",
      "        LMCI       0.67      0.81      0.73        47\n",
      "          AD       0.70      0.35      0.47        20\n",
      "\n",
      "    accuracy                           0.60        96\n",
      "   macro avg       0.60      0.54      0.55        96\n",
      "weighted avg       0.61      0.60      0.59        96\n",
      "\n",
      "=== CNN + ViT Fusion (Test) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.43      0.43      0.43        30\n",
      "        LMCI       0.59      0.74      0.65        46\n",
      "          AD       0.62      0.25      0.36        20\n",
      "\n",
      "    accuracy                           0.54        96\n",
      "   macro avg       0.55      0.47      0.48        96\n",
      "weighted avg       0.55      0.54      0.52        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 6.4: CNN + ViT Simple Fusion Baseline\n",
    "# =========================\n",
    "\n",
    "X_train_fused = torch.cat([X_train_cnn, X_train_vit], dim=1)\n",
    "X_val_fused   = torch.cat([X_val_cnn, X_val_vit], dim=1)\n",
    "X_test_fused  = torch.cat([X_test_cnn, X_test_vit], dim=1)\n",
    "\n",
    "print(\"Fused feature dimension:\", X_train_fused.shape[1])\n",
    "\n",
    "svm_fused = SVC(\n",
    "    kernel=\"linear\",\n",
    "    class_weight=\"balanced\",\n",
    "    probability=False,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "svm_fused.fit(X_train_fused.numpy(), y_train.numpy())\n",
    "\n",
    "y_val_pred_fused = svm_fused.predict(X_val_fused.numpy())\n",
    "y_test_pred_fused = svm_fused.predict(X_test_fused.numpy())\n",
    "\n",
    "print(\"=== CNN + ViT Fusion (Validation) ===\")\n",
    "print(classification_report(y_val.numpy(), y_val_pred_fused, target_names=[\"CN\", \"LMCI\", \"AD\"]))\n",
    "\n",
    "print(\"=== CNN + ViT Fusion (Test) ===\")\n",
    "print(classification_report(y_test.numpy(), y_test_pred_fused, target_names=[\"CN\", \"LMCI\", \"AD\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20b5f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Baseline predictions saved\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 6.5: Save Baseline Predictions\n",
    "# =========================\n",
    "\n",
    "np.save(RESULTS_DIR / \"y_test_true.npy\", y_test.numpy())\n",
    "np.save(RESULTS_DIR / \"y_test_pred_cnn.npy\", y_test_pred_cnn)\n",
    "np.save(RESULTS_DIR / \"y_test_pred_vit.npy\", y_test_pred_vit)\n",
    "np.save(RESULTS_DIR / \"y_test_pred_fused.npy\", y_test_pred_fused)\n",
    "\n",
    "print(\"✅ Baseline predictions saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea9a91",
   "metadata": {},
   "source": [
    "END-TO-END DUAL-STREAM MODEL\n",
    "\n",
    "(CNN + ViT + Attention Fusion + Classifier)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b21ffb",
   "metadata": {},
   "source": [
    "MRI slices (per subject)\n",
    "   ├── CNN Stream (Med3D ResNet50 / ResNet50 for now)\n",
    "   ├── Transformer Stream (ViT-B/16)\n",
    "   └── Attention Fusion (learns importance)\n",
    "           ↓\n",
    "      Subject-level embedding\n",
    "           ↓\n",
    "        Classifier\n",
    "           ↓\n",
    "      CN / LMCI / AD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7414eb",
   "metadata": {},
   "source": [
    "Right now, to avoid breaking the environment, we will:\n",
    "\n",
    "Implement Step 7 using ResNet50 (ImageNet) as a placeholder\n",
    "\n",
    "The architecture will be identical\n",
    "\n",
    "In the next step, we will swap the CNN backbone to Med3D\n",
    "with zero changes to training code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fe797",
   "metadata": {},
   "source": [
    "##### dual - stream model defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00c6da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Step 7.1: End-to-End Dual-Stream Model with Attention Fusion\n",
    "# =========================\n",
    "\n",
    "class AttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated attention fusion for CNN & ViT features\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_dim, vit_dim, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_cnn = nn.Linear(cnn_dim, hidden_dim)\n",
    "        self.fc_vit = nn.Linear(vit_dim, hidden_dim)\n",
    "        \n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cnn_feat, vit_feat):\n",
    "        cnn_h = self.fc_cnn(cnn_feat)\n",
    "        vit_h = self.fc_vit(vit_feat)\n",
    "\n",
    "        combined = torch.cat([cnn_h, vit_h], dim=1)\n",
    "        weights = self.attn(combined)   # (B, 2)\n",
    "\n",
    "        fused = (\n",
    "            weights[:, 0:1] * cnn_h +\n",
    "            weights[:, 1:2] * vit_h\n",
    "        )\n",
    "\n",
    "        return fused\n",
    "\n",
    "\n",
    "class DualStreamAlzheimerModel(nn.Module):\n",
    "    def __init__(self, cnn_backbone, vit_backbone, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = cnn_backbone\n",
    "        self.vit = vit_backbone\n",
    "\n",
    "        self.fusion = AttentionFusion(\n",
    "            cnn_dim=2048,\n",
    "            vit_dim=768,\n",
    "            hidden_dim=512\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        images: (B, S, 3, 224, 224)\n",
    "        \"\"\"\n",
    "        B, S, C, H, W = images.shape\n",
    "        images = images.view(B * S, C, H, W)\n",
    "\n",
    "        # CNN stream\n",
    "        cnn_feats = self.cnn(images)          # (B*S, 2048)\n",
    "        cnn_feats = cnn_feats.view(B, S, -1).mean(dim=1)\n",
    "\n",
    "        # ViT stream\n",
    "        vit_feats = self.vit(images)          # (B*S, 768)\n",
    "        vit_feats = vit_feats.view(B, S, -1).mean(dim=1)\n",
    "\n",
    "        fused = self.fusion(cnn_feats, vit_feats)\n",
    "        logits = self.classifier(fused)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab993ed",
   "metadata": {},
   "source": [
    "initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8b07d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dual-stream model initialized\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 7.2: Model Initialization\n",
    "# =========================\n",
    "\n",
    "model = DualStreamAlzheimerModel(\n",
    "    cnn_backbone=cnn_backbone,\n",
    "    vit_backbone=vit_model\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"✅ Dual-stream model initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a22e8a",
   "metadata": {},
   "source": [
    "Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e88cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loss, optimizer, scheduler ready\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 7.3: Loss, Optimizer, Scheduler\n",
    "# =========================\n",
    "\n",
    "class_counts = train_df[\"class\"].value_counts().sort_index()\n",
    "class_weights = 1.0 / torch.tensor(class_counts.values, dtype=torch.float)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = class_weights.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "\n",
    "print(\"✅ Loss, optimizer, scheduler ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed7715",
   "metadata": {},
   "source": [
    "Subject-Batch DataLoader (CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69a52e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 7.4.1: Slice Sampling Utility\n",
    "# =========================\n",
    "\n",
    "import random\n",
    "\n",
    "def sample_slices(images, num_slices=50, training=True):\n",
    "    \"\"\"\n",
    "    images: Tensor of shape (S, 3, 224, 224)\n",
    "    returns: Tensor of shape (num_slices, 3, 224, 224)\n",
    "    \"\"\"\n",
    "    S = images.shape[0]\n",
    "\n",
    "    # Case 1: exact match\n",
    "    if S == num_slices:\n",
    "        return images\n",
    "\n",
    "    # Case 2: more slices than needed\n",
    "    if S > num_slices:\n",
    "        if training:\n",
    "            indices = random.sample(range(S), num_slices)\n",
    "        else:\n",
    "            indices = torch.linspace(0, S - 1, steps=num_slices).long()\n",
    "        return images[indices]\n",
    "\n",
    "    # Case 3: fewer slices → repeat\n",
    "    repeat_factor = (num_slices // S) + 1\n",
    "    images = images.repeat(repeat_factor, 1, 1, 1)\n",
    "    return images[:num_slices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c3a1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Windows-safe collate wrappers\n",
    "# =========================\n",
    "\n",
    "def train_collate_fn(batch):\n",
    "    return subject_collate(\n",
    "        batch,\n",
    "        num_slices=16,\n",
    "        training=True\n",
    "    )\n",
    "\n",
    "def eval_collate_fn(batch):\n",
    "    return subject_collate(\n",
    "        batch,\n",
    "        num_slices=16,\n",
    "        training=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e666063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 7.4.2: Fixed-size Subject Collate Function\n",
    "# =========================\n",
    "\n",
    "def subject_collate(batch, num_slices=50, training=True):\n",
    "    images_list = []\n",
    "    labels = []\n",
    "\n",
    "    for images, label in batch:\n",
    "        # images: (S, 3, 224, 224)\n",
    "        images = sample_slices(\n",
    "            images,\n",
    "            num_slices=num_slices,\n",
    "            training=training\n",
    "        )\n",
    "        images_list.append(images)\n",
    "        labels.append(label)\n",
    "\n",
    "    images = torch.stack(images_list)   # (B, 50, 3, 224, 224)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7fd6e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stage 2 DataLoaders initialized (num_workers=2)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STAGE 2: Safe multiprocessing DataLoaders (Windows)\n",
    "# =========================\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=2,          # 🔑 safe on Windows\n",
    "    pin_memory=True,        # 🔑 faster CPU -> GPU\n",
    "    persistent_workers=False,\n",
    "    collate_fn=train_collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=eval_collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=eval_collate_fn\n",
    ")\n",
    "\n",
    "print(\"✅ Stage 2 DataLoaders initialized (num_workers=2)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69ab06",
   "metadata": {},
   "source": [
    "end-to-end training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da628c",
   "metadata": {},
   "source": [
    "metrix utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "811ea39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Step 7.5.1: Metrics utilities\n",
    "# =========================\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43d3faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Step 7.5.2: Early Stopping\n",
    "# =========================\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = np.inf\n",
    "        self.counter = 0\n",
    "        self.should_stop = False\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7eea7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Step 7.5.3: Train / Validate functions\n",
    "# =========================\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += compute_accuracy(logits, labels)\n",
    "\n",
    "    return total_loss / len(loader), total_acc / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_one_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += compute_accuracy(logits, labels)\n",
    "\n",
    "    return total_loss / len(loader), total_acc / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3046789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CNN and ViT frozen (Phase 1)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# PHASE 1: Freeze heavy backbones\n",
    "# =========================\n",
    "\n",
    "for param in model.cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.vit.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"✅ CNN and ViT frozen (Phase 1)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98976e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e3cd7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Step 7.5.4: Full training loop\n",
    "# =========================\n",
    "\n",
    "EPOCHS = 100\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "checkpoint_path = RESULTS_DIR / \"best_dualstream_model.pt\"\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc = validate_one_epoch(model, val_loader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(\"✅ Best model updated\")\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping.step(val_loss)\n",
    "    if early_stopping.should_stop:\n",
    "        print(\"🛑 Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(\"Best validation accuracy:\", best_val_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
